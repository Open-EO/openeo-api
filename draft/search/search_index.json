{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"openEO - Concepts and API Reference \u00b6 Note: The specification is currently still an early version, with the potential for some major things to change. The core is now fleshed out, so everybody is encouraged to try it out and give feedback (for example by adding issues ). But the goal is to actually be able to act on that feedback, which will mean changes are quite possible. openEO develops an open application programming interface (API) that connects clients like R, Python and JavaScript to big Earth observation cloud back-ends in a simple and unified way. The acronym openEO contracts two concepts: open : used here in the context of open source software; open source software is available in source code form, and can be freely modified and redistributed; the openEO project will create open source software, reusable under a liberal open source license (Apache 2.0) EO : Earth observation Jointly, the openEO targets the processing and analysis of Earth observation data. The main objectives of the project are the following concepts: Simplicity : nowadays, many end-users use Python or R to analyse data and JavaScript to develop web applications; analysing large amounts of EO imagery should be equally simple, and seamlessly integrate with existing workflows Unification : current EO cloud back-ends all have a different API , making EO data analysis hard to validate and reproduce and back-ends difficult to compare in terms of capability and costs, or to combine them in a joint analysis across back-ends. A unified API can resolve many of these problems. The following pages introduce the core concepts of the project. Make sure to introduce yourself to the major technical terms used in the openEO project by reading the glossary . The openEO API defines a HTTP API that lets cloud back-ends with large Earth observation datasets communicate with front end analysis applications in an interoperable way. This documentation describes important API concepts and design decisions and gives a complete API reference documentation . As an overview, the openEO API specifies how to discover which Earth observation data and processes are available at cloud back-ends, execute (chained) processes on back-ends, run user-defined functions (UDFs) on back-ends where UDFs can be exposed to the data in different ways, download (intermediate) results, and manage user content including billing. The API is defined as an OpenAPI 3.0 JSON file. openEO , A Common, Open Source Interface between Earth Observation Data Infrastructures and Front-End Applications is a H2020 project funded under call EO-2-2017: EO Big Data Shift, under proposal number 776242. It will run from Oct 2017 to Sept 2020. This project has received funding from the European Union\u2019s Horizon 2020 research and innovation programme under grant agreement No 776242. The contents of this website reflects only the authors\u2019 view; the European Commission is not responsible for any use that may be made of the information it provides.","title":"Introduction"},{"location":"#openeo-concepts-and-api-reference","text":"Note: The specification is currently still an early version, with the potential for some major things to change. The core is now fleshed out, so everybody is encouraged to try it out and give feedback (for example by adding issues ). But the goal is to actually be able to act on that feedback, which will mean changes are quite possible. openEO develops an open application programming interface (API) that connects clients like R, Python and JavaScript to big Earth observation cloud back-ends in a simple and unified way. The acronym openEO contracts two concepts: open : used here in the context of open source software; open source software is available in source code form, and can be freely modified and redistributed; the openEO project will create open source software, reusable under a liberal open source license (Apache 2.0) EO : Earth observation Jointly, the openEO targets the processing and analysis of Earth observation data. The main objectives of the project are the following concepts: Simplicity : nowadays, many end-users use Python or R to analyse data and JavaScript to develop web applications; analysing large amounts of EO imagery should be equally simple, and seamlessly integrate with existing workflows Unification : current EO cloud back-ends all have a different API , making EO data analysis hard to validate and reproduce and back-ends difficult to compare in terms of capability and costs, or to combine them in a joint analysis across back-ends. A unified API can resolve many of these problems. The following pages introduce the core concepts of the project. Make sure to introduce yourself to the major technical terms used in the openEO project by reading the glossary . The openEO API defines a HTTP API that lets cloud back-ends with large Earth observation datasets communicate with front end analysis applications in an interoperable way. This documentation describes important API concepts and design decisions and gives a complete API reference documentation . As an overview, the openEO API specifies how to discover which Earth observation data and processes are available at cloud back-ends, execute (chained) processes on back-ends, run user-defined functions (UDFs) on back-ends where UDFs can be exposed to the data in different ways, download (intermediate) results, and manage user content including billing. The API is defined as an OpenAPI 3.0 JSON file. openEO , A Common, Open Source Interface between Earth Observation Data Infrastructures and Front-End Applications is a H2020 project funded under call EO-2-2017: EO Big Data Shift, under proposal number 776242. It will run from Oct 2017 to Sept 2020. This project has received funding from the European Union\u2019s Horizon 2020 research and innovation programme under grant agreement No 776242. The contents of this website reflects only the authors\u2019 view; the European Commission is not responsible for any use that may be made of the information it provides.","title":"openEO - Concepts and API Reference"},{"location":"apireference/","text":"Placeholder for generated API specification.","title":"Core API Reference"},{"location":"arch/","text":"Architecture \u00b6 The openEO API defines a language how clients communicate to back-ends in order to analyze large Earth observation datasets. The API will be implemented by drivers for specific back-ends. Some first architecture considerations are listed below. The openEO API is a contract between clients and back-ends that describes the communication only Each back-end runs its own API instance including the specific back-end driver. There is no API instance that runs more than one driver. Clients in R, Python, and JavaScript connect directly to the back-ends and communicate with the back-ends over HTTPS according to the openEO API specification. API instances can run on back-end servers or additional intermediate layers, which then communicate to back-ends in a back-end specific way. Back-ends may add functionality and extend the API wherever there is need. There will be a central back-end registry service (openEO Hub), to allow users to search for back-ends with specific functionality and or data. The openEO API may define profiles in order to group specific functionality. Figure: Architecture Microservices \u00b6 To simplify and structure the development, the API is divided into a few microservices. Microservice Description Capabilities This microservice reports on the capabilities of the back-end, i.e. which API endpoints are implemented, which authentication methods are supported, and whether and how UDFs can be executed at the back-end. EO Data Discovery Describes which collections are available at the back-end. Process Discovery Provides services to find out which processes a back-end provides, i.e., what users can do with the available data. UDF Discovery and execution of user-defined functions. Batch Job Management Organizes and manages batch jobs that run processes on back-ends. File Management Organizes and manages user-uploaded files. Process Graph Management Organizes and manages user-defined process graphs. Secondary Services Management External web services to access data and job results such as a OGC WMTS service. Account Management User management, accounting and authentication.","title":"Architecture"},{"location":"arch/#architecture","text":"The openEO API defines a language how clients communicate to back-ends in order to analyze large Earth observation datasets. The API will be implemented by drivers for specific back-ends. Some first architecture considerations are listed below. The openEO API is a contract between clients and back-ends that describes the communication only Each back-end runs its own API instance including the specific back-end driver. There is no API instance that runs more than one driver. Clients in R, Python, and JavaScript connect directly to the back-ends and communicate with the back-ends over HTTPS according to the openEO API specification. API instances can run on back-end servers or additional intermediate layers, which then communicate to back-ends in a back-end specific way. Back-ends may add functionality and extend the API wherever there is need. There will be a central back-end registry service (openEO Hub), to allow users to search for back-ends with specific functionality and or data. The openEO API may define profiles in order to group specific functionality. Figure: Architecture","title":"Architecture"},{"location":"arch/#microservices","text":"To simplify and structure the development, the API is divided into a few microservices. Microservice Description Capabilities This microservice reports on the capabilities of the back-end, i.e. which API endpoints are implemented, which authentication methods are supported, and whether and how UDFs can be executed at the back-end. EO Data Discovery Describes which collections are available at the back-end. Process Discovery Provides services to find out which processes a back-end provides, i.e., what users can do with the available data. UDF Discovery and execution of user-defined functions. Batch Job Management Organizes and manages batch jobs that run processes on back-ends. File Management Organizes and manages user-uploaded files. Process Graph Management Organizes and manages user-defined process graphs. Secondary Services Management External web services to access data and job results such as a OGC WMTS service. Account Management User management, accounting and authentication.","title":"Microservices"},{"location":"changelog/","text":"Changelog \u00b6 All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning . Unreleased / Draft \u00b6 Added \u00b6 GET / : Field production added to response. #184 Links with relation types terms-of-service and privacy-policy explicitly documented. Clients must handle them properly if present. #212 GET /collections and GET /collections/{collectionId} : New field deprecated can be used to indicate outdated collections. Links with relation type latest-version can point to the latest version. #226 Added a Data Cube Dimension of type bands to the cube:dimensions property. #208 POST /result : May add a link to a log file in the header. #214 GET /jobs/{job_id}/logs and GET /services/{service_id}/logs : Endpoints that publish logging information. #214 Changed \u00b6 The concept of callbacks has been renamed. Schema format/subtype callback has been renamed to process-graph . #216 Unsupported endpoints are not forced to return a FeatureUnsupported (501) error and can return a simple NotFound (404) instead. If currency returned by GET / is null , costs and budget are unsupported. costs and budget fields in various endpoints can be set to null (default). The default type for Process Graph Variables is not string , but no specific (any) data type. Default values can be of any type. Official support for CommonMark 0.29 instead of CommonMark 0.28 . #203 The parameter user_id has been removed from the endpoints to manage user files ( /files/{user_id} ). #218 GET / : Property links is required. GET /collections and GET /collections/{collectionId} : Updated STAC to version 0.8.1. #185 , #204 . GET /credentials/oidc : Changed response to support multiple OpenID Connect identity providers ( #201 ) and clarified workflow overall. Bearer token are built from the authentication method, an optional provider id and the token itself. #219 GET /udf_runtimes : description fields don't allow null values any longer. GET /output_formats renamed to GET /file_formats to allow listing input file formats. #215 The structure of the response has changed. The former response body for the output formats is now available in the property output . The input file formats are now available in the property input with the same schema as for output formats. Additionally, each format can have a title . GET /jobs/{job_id}/results : Response body for status code 200 has changed to be a valid STAC Item, allows content type application/geo+json . Response body for status code 424 has been extended. Deprecated \u00b6 The processes should not use the JSON Schema keyword format any longer. Instead use the custom keyword subtype . #233 PROJ definitions are deprecated in favor of EPSG codes, WKT2 and PROJJSON. #58 Removed \u00b6 GET /job/{job_id} : Removed property error . Request information from GET /job/{job_id}/logs instead. GET /job/{job_id}/results : Metalink XML encoding has been removed. #205 Expires header has been removed, use expires property in the response body instead. GET /credentials/basic doesn't return a user_id . Instead request it from GET /me . GET /collections/{collectionId} : Removed optional STAC extensions from the API specification. Inform yourself about useful STAC extensions instead. #176 Fixed \u00b6 Service parameters and attributes in GET /service_types and output format parameters in GET /file_formats (previously GET /output_formats ) now have a type , which was previously only mentioned in examples. Clarified how clients and back-ends should implement well-known discovery for GET ./well-known/openeo . #202 [0.4.2] - 2019-06-11 \u00b6 Added \u00b6 Basic JSON Schema for process graph validation. Changed \u00b6 Updated the process catalog, see the separate changelog. Removed \u00b6 Disallowed CommonMark in descriptions of process graph variables and process graph nodes. Fixed \u00b6 Improved documentation with several clarifications, better examples and more. SAR Bands had a required but undefined property. #187 Clarified how file paths in the URL must be encoded for file handling. OpenAPI nullable issues: Removed null from SAR Bands enum for OpenAPI code generator, is handled by nullable . OpenAPI-Specification#1900 nullable doesn't combine well with anyOf , allOf and oneOf , therefore placed nullable also in one of the sub-schemas. OpenAPI-Specification#1368 [0.4.1] - 2019-05-29 \u00b6 Changed \u00b6 Updated the process catalog, see the separate changelog. Removed \u00b6 The property sar:absolute_orbit in GET /collections/{collection_id} has been removed. Sending a Bearer token to GET /credentials/oidc is not allowed any longer. Fixed \u00b6 Improved and clarified the documentation and descriptions. GET /collections/{collection_id} : properties in GET /collections/{collection_id} doesn't require any of the integrated STAC extensions any longer. The property sci:publications in GET /collections/{collection_id} was ported over incorrectly from STAC. The data type has been changed from object to array. GET /jobs/{job_id}/results was expected to return HTTP status code 424 with an error message, but it was specified in /jobs/{job_id}/estimate instead. The definition was moved. #177 path in GET and PUT /files/{user_id} is required again. Fixed several issues in the client development guidelines. [0.4.0] - 2019-03-07 \u00b6 Added \u00b6 GET /jobs/{job_id}/estimate can return the estimated required storage capacity. #122 GET /jobs/{job_id} has two new properties: progress indicates the batch job progress when running. #82 error states the error message when a job errored out. GET /jobs/{job_id}/result mirrors this error message in a response with HTTP status code 424. #165 GET /.well-known/openeo allows clients to choose between versions. #148 GET / (Capabilities): Requires to return a title ( title ), a description ( description ) and the back-end version ( backend_version ). #154 Billing plans have an additional required property paid . #157 Should provide a link to the Well-Known URI ( /.well-known/openeo ) in the new links property. GET /processes (Process discovery): Processes can be categorizes with the category property. Parameters can be ordered with the parameter_order property instead of having a random order. Support for references to other processes in descriptions. Processes and parameters can be declared to be experimental . GET /output_formats and GET /service_types can now provide links per entry. GET /udf_runtimes provide a list of UDF runtime environments. #87 GET /service_types allows to specify variables that can be used in process graphs. #172 Changed \u00b6 Completely new version of the processes. Changed process graph to a flexible graph-like structure, which also allows callbacks. #160 Updated GET /collections and GET /collections/{collection_id} to follow STAC v0.6.2. #158 , #173 The process_graph_id of stored process graphs, the service_id of services and the job_id of jobs has changed to id in responses. #130 The status property for jobs is now required. POST /preview renamed to POST /result . #162 GET / (Capabilities): version in the response was renamed to api_version . Endpoint paths must follow the openAPI specification. #128 Billing plan descriptions allow CommonMark. #164 /files/{user_id}/{path} File management: Clarified handling of folders. #146 GET method: The name property was renamed to path . #133 PUT method: Returns file meta data with a different response code. #163 GET /processes (Process discovery): The name property of processes has changed to id . #130 mime_type replaced with media_type in the input parameters and return values. The schema for exceptions follows the general schema for openEO errors. #139 Changed the structure of examples . POST /validation (Process graph validation): Returns HTTP status code 200 for valid and invalid process graphs and responds with a list of errors. #144 Allowed to call the endpoint without authentication. #151 Behavior for DELETE /jobs/{job_id}/results and POST /jobs/{job_id}/results specified depending on the job status. Clarified status changes in general. #142 Improved client development guidelines. #124 , #138 Removed \u00b6 Numeric openEO error codes. Replaced in responses with textual error codes. #139 Query parameters to replace process graph variables in GET /process_graphs/{process_graph_id} . #147 min_parameters and dependencies for parameters in process descriptions returned by GET /processes . Replaced output format properties in favor of a save_result process, which has resulted in in the removal of: The default output format in GET /output_formats . #153 The output format properties in POST /result (fka POST /preview ), POST /jobs , PATCH /jobs and GET /jobs/{job_id} requests. #153 gis_data_type (not to be confused with gis_data_types ) in the parameters of output formats in GET /output_formats Fixed \u00b6 Added missing Access-Control-Expose-Headers header to required CORS headers. Some endpoints didn't include authentication information. GET /jobs/{job_id}/estimate : Property downloads_included had a wrong default value. [0.3.1] - 2018-11-06 \u00b6 Added \u00b6 createProcessGraph method to client development guidelines. JSON file with all specified errors. Textual error codes for each specified error. Allow setting a plan for POST /preview Default billing plan in GET / . #141 Job ID in JSON response for GET /jobs/{job_id}/results . Changed \u00b6 Several optional fields such as output , title and description are now nullable instead of requiring to omit them. The output format is not required in POST /preview any more and thus allows falling back to the default. The output_format parameter in createJob and execute in client development guidelines. The extent parameters in filter_bbox and filter_daterange are formally required now. Deprecated \u00b6 Numeric openEO error codes are soon to be replaced with textual error codes. eo:resolution in collection bands is a duplicate of eo:gsd . Use eo:gsd instead. Fixed \u00b6 Fixed a wrong definition of the header OpenEO-Costs in POST /preview . Fixed typo in method authenticateOIDC in client development guidelines. Fixed the definition of spatial extents by swapping north and south. Replaced the outdated occurrences of srs with crs in spatial extents. Added missing required descriptions to process definitions. Added missing error messages. Fixed unclear specification for arrays used as process graph arguments. Fixed inconsist schema of openEO error responses: Field is now consistently named message instead of description . [0.3.0] - 2018-09-21 \u00b6 First version after proof of concept tackling many major issues. No changelog available. [0.0.2] - 2018-03-22 \u00b6 Version for proof of concept. No changelog available. [0.0.1] - 2018-02-07 \u00b6 Initial version.","title":"Changelog"},{"location":"changelog/#changelog","text":"All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning .","title":"Changelog"},{"location":"changelog/#unreleased-draft","text":"","title":"Unreleased / Draft"},{"location":"changelog/#added","text":"GET / : Field production added to response. #184 Links with relation types terms-of-service and privacy-policy explicitly documented. Clients must handle them properly if present. #212 GET /collections and GET /collections/{collectionId} : New field deprecated can be used to indicate outdated collections. Links with relation type latest-version can point to the latest version. #226 Added a Data Cube Dimension of type bands to the cube:dimensions property. #208 POST /result : May add a link to a log file in the header. #214 GET /jobs/{job_id}/logs and GET /services/{service_id}/logs : Endpoints that publish logging information. #214","title":"Added"},{"location":"changelog/#changed","text":"The concept of callbacks has been renamed. Schema format/subtype callback has been renamed to process-graph . #216 Unsupported endpoints are not forced to return a FeatureUnsupported (501) error and can return a simple NotFound (404) instead. If currency returned by GET / is null , costs and budget are unsupported. costs and budget fields in various endpoints can be set to null (default). The default type for Process Graph Variables is not string , but no specific (any) data type. Default values can be of any type. Official support for CommonMark 0.29 instead of CommonMark 0.28 . #203 The parameter user_id has been removed from the endpoints to manage user files ( /files/{user_id} ). #218 GET / : Property links is required. GET /collections and GET /collections/{collectionId} : Updated STAC to version 0.8.1. #185 , #204 . GET /credentials/oidc : Changed response to support multiple OpenID Connect identity providers ( #201 ) and clarified workflow overall. Bearer token are built from the authentication method, an optional provider id and the token itself. #219 GET /udf_runtimes : description fields don't allow null values any longer. GET /output_formats renamed to GET /file_formats to allow listing input file formats. #215 The structure of the response has changed. The former response body for the output formats is now available in the property output . The input file formats are now available in the property input with the same schema as for output formats. Additionally, each format can have a title . GET /jobs/{job_id}/results : Response body for status code 200 has changed to be a valid STAC Item, allows content type application/geo+json . Response body for status code 424 has been extended.","title":"Changed"},{"location":"changelog/#deprecated","text":"The processes should not use the JSON Schema keyword format any longer. Instead use the custom keyword subtype . #233 PROJ definitions are deprecated in favor of EPSG codes, WKT2 and PROJJSON. #58","title":"Deprecated"},{"location":"changelog/#removed","text":"GET /job/{job_id} : Removed property error . Request information from GET /job/{job_id}/logs instead. GET /job/{job_id}/results : Metalink XML encoding has been removed. #205 Expires header has been removed, use expires property in the response body instead. GET /credentials/basic doesn't return a user_id . Instead request it from GET /me . GET /collections/{collectionId} : Removed optional STAC extensions from the API specification. Inform yourself about useful STAC extensions instead. #176","title":"Removed"},{"location":"changelog/#fixed","text":"Service parameters and attributes in GET /service_types and output format parameters in GET /file_formats (previously GET /output_formats ) now have a type , which was previously only mentioned in examples. Clarified how clients and back-ends should implement well-known discovery for GET ./well-known/openeo . #202","title":"Fixed"},{"location":"changelog/#042-2019-06-11","text":"","title":"[0.4.2] - 2019-06-11"},{"location":"changelog/#added_1","text":"Basic JSON Schema for process graph validation.","title":"Added"},{"location":"changelog/#changed_1","text":"Updated the process catalog, see the separate changelog.","title":"Changed"},{"location":"changelog/#removed_1","text":"Disallowed CommonMark in descriptions of process graph variables and process graph nodes.","title":"Removed"},{"location":"changelog/#fixed_1","text":"Improved documentation with several clarifications, better examples and more. SAR Bands had a required but undefined property. #187 Clarified how file paths in the URL must be encoded for file handling. OpenAPI nullable issues: Removed null from SAR Bands enum for OpenAPI code generator, is handled by nullable . OpenAPI-Specification#1900 nullable doesn't combine well with anyOf , allOf and oneOf , therefore placed nullable also in one of the sub-schemas. OpenAPI-Specification#1368","title":"Fixed"},{"location":"changelog/#041-2019-05-29","text":"","title":"[0.4.1] - 2019-05-29"},{"location":"changelog/#changed_2","text":"Updated the process catalog, see the separate changelog.","title":"Changed"},{"location":"changelog/#removed_2","text":"The property sar:absolute_orbit in GET /collections/{collection_id} has been removed. Sending a Bearer token to GET /credentials/oidc is not allowed any longer.","title":"Removed"},{"location":"changelog/#fixed_2","text":"Improved and clarified the documentation and descriptions. GET /collections/{collection_id} : properties in GET /collections/{collection_id} doesn't require any of the integrated STAC extensions any longer. The property sci:publications in GET /collections/{collection_id} was ported over incorrectly from STAC. The data type has been changed from object to array. GET /jobs/{job_id}/results was expected to return HTTP status code 424 with an error message, but it was specified in /jobs/{job_id}/estimate instead. The definition was moved. #177 path in GET and PUT /files/{user_id} is required again. Fixed several issues in the client development guidelines.","title":"Fixed"},{"location":"changelog/#040-2019-03-07","text":"","title":"[0.4.0] - 2019-03-07"},{"location":"changelog/#added_2","text":"GET /jobs/{job_id}/estimate can return the estimated required storage capacity. #122 GET /jobs/{job_id} has two new properties: progress indicates the batch job progress when running. #82 error states the error message when a job errored out. GET /jobs/{job_id}/result mirrors this error message in a response with HTTP status code 424. #165 GET /.well-known/openeo allows clients to choose between versions. #148 GET / (Capabilities): Requires to return a title ( title ), a description ( description ) and the back-end version ( backend_version ). #154 Billing plans have an additional required property paid . #157 Should provide a link to the Well-Known URI ( /.well-known/openeo ) in the new links property. GET /processes (Process discovery): Processes can be categorizes with the category property. Parameters can be ordered with the parameter_order property instead of having a random order. Support for references to other processes in descriptions. Processes and parameters can be declared to be experimental . GET /output_formats and GET /service_types can now provide links per entry. GET /udf_runtimes provide a list of UDF runtime environments. #87 GET /service_types allows to specify variables that can be used in process graphs. #172","title":"Added"},{"location":"changelog/#changed_3","text":"Completely new version of the processes. Changed process graph to a flexible graph-like structure, which also allows callbacks. #160 Updated GET /collections and GET /collections/{collection_id} to follow STAC v0.6.2. #158 , #173 The process_graph_id of stored process graphs, the service_id of services and the job_id of jobs has changed to id in responses. #130 The status property for jobs is now required. POST /preview renamed to POST /result . #162 GET / (Capabilities): version in the response was renamed to api_version . Endpoint paths must follow the openAPI specification. #128 Billing plan descriptions allow CommonMark. #164 /files/{user_id}/{path} File management: Clarified handling of folders. #146 GET method: The name property was renamed to path . #133 PUT method: Returns file meta data with a different response code. #163 GET /processes (Process discovery): The name property of processes has changed to id . #130 mime_type replaced with media_type in the input parameters and return values. The schema for exceptions follows the general schema for openEO errors. #139 Changed the structure of examples . POST /validation (Process graph validation): Returns HTTP status code 200 for valid and invalid process graphs and responds with a list of errors. #144 Allowed to call the endpoint without authentication. #151 Behavior for DELETE /jobs/{job_id}/results and POST /jobs/{job_id}/results specified depending on the job status. Clarified status changes in general. #142 Improved client development guidelines. #124 , #138","title":"Changed"},{"location":"changelog/#removed_3","text":"Numeric openEO error codes. Replaced in responses with textual error codes. #139 Query parameters to replace process graph variables in GET /process_graphs/{process_graph_id} . #147 min_parameters and dependencies for parameters in process descriptions returned by GET /processes . Replaced output format properties in favor of a save_result process, which has resulted in in the removal of: The default output format in GET /output_formats . #153 The output format properties in POST /result (fka POST /preview ), POST /jobs , PATCH /jobs and GET /jobs/{job_id} requests. #153 gis_data_type (not to be confused with gis_data_types ) in the parameters of output formats in GET /output_formats","title":"Removed"},{"location":"changelog/#fixed_3","text":"Added missing Access-Control-Expose-Headers header to required CORS headers. Some endpoints didn't include authentication information. GET /jobs/{job_id}/estimate : Property downloads_included had a wrong default value.","title":"Fixed"},{"location":"changelog/#031-2018-11-06","text":"","title":"[0.3.1] - 2018-11-06"},{"location":"changelog/#added_3","text":"createProcessGraph method to client development guidelines. JSON file with all specified errors. Textual error codes for each specified error. Allow setting a plan for POST /preview Default billing plan in GET / . #141 Job ID in JSON response for GET /jobs/{job_id}/results .","title":"Added"},{"location":"changelog/#changed_4","text":"Several optional fields such as output , title and description are now nullable instead of requiring to omit them. The output format is not required in POST /preview any more and thus allows falling back to the default. The output_format parameter in createJob and execute in client development guidelines. The extent parameters in filter_bbox and filter_daterange are formally required now.","title":"Changed"},{"location":"changelog/#deprecated_1","text":"Numeric openEO error codes are soon to be replaced with textual error codes. eo:resolution in collection bands is a duplicate of eo:gsd . Use eo:gsd instead.","title":"Deprecated"},{"location":"changelog/#fixed_4","text":"Fixed a wrong definition of the header OpenEO-Costs in POST /preview . Fixed typo in method authenticateOIDC in client development guidelines. Fixed the definition of spatial extents by swapping north and south. Replaced the outdated occurrences of srs with crs in spatial extents. Added missing required descriptions to process definitions. Added missing error messages. Fixed unclear specification for arrays used as process graph arguments. Fixed inconsist schema of openEO error responses: Field is now consistently named message instead of description .","title":"Fixed"},{"location":"changelog/#030-2018-09-21","text":"First version after proof of concept tackling many major issues. No changelog available.","title":"[0.3.0] - 2018-09-21"},{"location":"changelog/#002-2018-03-22","text":"Version for proof of concept. No changelog available.","title":"[0.0.2] - 2018-03-22"},{"location":"changelog/#001-2018-02-07","text":"Initial version.","title":"[0.0.1] - 2018-02-07"},{"location":"codeofconduct/","text":"Contributor Code of Conduct \u00b6 As contributors and maintainers of this project, we pledge to respect all people who contribute through reporting issues, posting feature requests, updating documentation, submitting pull requests or patches, and other activities. We are committed to making participation in this project a harassment-free experience for everyone, regardless of level of experience, gender, gender identity and expression, sexual orientation, disability, personal appearance, body size, race, ethnicity, age, or religion. Examples of unacceptable behavior by participants include the use of sexual language or imagery, derogatory comments or personal attacks, trolling, public or private harassment, insults, or other unprofessional conduct. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct. Project maintainers who do not follow the Code of Conduct may be removed from the project team. Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by opening an issue or contacting one or more of the project maintainers. This Code of Conduct is adapted from the Contributor Covenant , version 1.0.0, available at http://contributor-covenant.org/version/1/0/0/ .","title":"Contributor Code of Conduct"},{"location":"codeofconduct/#contributor-code-of-conduct","text":"As contributors and maintainers of this project, we pledge to respect all people who contribute through reporting issues, posting feature requests, updating documentation, submitting pull requests or patches, and other activities. We are committed to making participation in this project a harassment-free experience for everyone, regardless of level of experience, gender, gender identity and expression, sexual orientation, disability, personal appearance, body size, race, ethnicity, age, or religion. Examples of unacceptable behavior by participants include the use of sexual language or imagery, derogatory comments or personal attacks, trolling, public or private harassment, insults, or other unprofessional conduct. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct. Project maintainers who do not follow the Code of Conduct may be removed from the project team. Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by opening an issue or contacting one or more of the project maintainers. This Code of Conduct is adapted from the Contributor Covenant , version 1.0.0, available at http://contributor-covenant.org/version/1/0/0/ .","title":"Contributor Code of Conduct"},{"location":"errors/","text":"openEO error codes \u00b6 The following table of error codes is incomplete . These error codes will evolve over time. If you are missing any common error, please contribute it by adding an issue , creating a pull request or get in contact in our chat room . The whole table of error codes is available as JSON file , which can be used by implementors to automatically generate error responses. Categories Account Management EO Data Discovery File Management General Job Management Process Graph Management Processes Secondary Services Management Account Management openEO Error Code Description Example Message HTTP Status Code AuthenticationRequired The client did not provide any authentication details for a resource requiring authentication or the provided authentication details are not correct. Unauthorized. 401 AuthenticationSchemeInvalid Invalid authentication scheme (e.g. Bearer). Authentication method not supported. 403 CredentialsInvalid Credentials are not correct. 403 TokenInvalid Authorization token invalid or expired. Session has expired. 403 EO Data Discovery openEO Error Code Description Example Message HTTP Status Code CollectionNotFound The requested collection does not exist. Collection '{identifier}' does not exist. 404 File Management openEO Error Code Description Example Message HTTP Status Code ContentTypeInvalid The specified media (MIME) type used in the Content-Type header is not allowed. The media type is not supported. Allowed: {types} 400 FileContentInvalid The content of the file is invalid. File content is invalid. 400 FileLocked The file is locked by a running job or another process. File '{file}' is locked. 400 FileNotFound The requested file does not exist. File '{file}' does not exist. 404 FileOperationUnsupported The specified path is not a file and the operation is only supported for files. Path is likely a directory. Operation is only supported for files. 400 FilePathInvalid The specified path is invalid or not accessible. Path could contain invalid characters, point to an existing folder or a location outside of the user folder. File path is invalid. 400 FileSizeExceeded File exceeds allowed maximum file size. File size it too large. Maximum file size: {size} 400 FileTypeInvalid File format, file extension or media (MIME) type is not allowed. File type not allowed. Allowed file types: {types} 400 StorageFailure Server couldn't store file(s) due to server-side reasons. Unable to store file(s). 500 StorageQuotaExceeded The storage quota has been exceeded by the user. Insufficient Storage. 400 General openEO Error Code Description Example Message HTTP Status Code ContentTypeInvalid The specified media (MIME) type used in the Content-Type header is not allowed. The media type is not supported. Allowed: {types} 400 FeatureUnsupported The back-end responds with this error whenever an endpoint is specified in the openEO API, but is not supported. Feature not supported. 501 InfrastructureBusy Service is generally available, but the infrastructure can't handle it at the moment as too many requests are processed. Service is not available at the moment due to overloading. Please try again later. 503 InfrastructureMaintenance Service is currently not available, but the infrastructure is currently undergoing maintenance work. Service is not available at the moment due to maintenance work. Please try again later. 503 Internal An internal server error with a proprietary message. Server error: {message} 500 NotFound To be used if the requested resource does not exist. Note: There are specialized errors for missing jobs (JobNotFound), files (FileNotFound), etc. Unsupported endpoints MAY send an 'FeatureUnsupported' (501) error. Resource not found. 404 RequestTimeout The request took too long and timed out. Request timed out. 408 Job Management openEO Error Code Description Example Message HTTP Status Code BillingPlanInvalid The billing plan is not on the list of available plans. The billing plan is not valid. 400 BudgetInvalid The budget is too low as it is either smaller than or equal to 0 or below the costs. The budget is too low. 400 FormatArgumentInvalid The output format argument '{argument}' is invalid: {reason} 400 FormatArgumentUnsupported Output format argument '{argument}' is not supported. 400 FormatUnsuitable Data can't be transformed into the requested output format. 400 FormatUnsupported Output format not supported. 400 JobLocked The job is currently locked due to a running batch computation and can't be modified meanwhile. Job is locked due to a queued or running batch computation. 400 JobNotFinished Job has not finished computing the results yet. Please try again later. 400 JobNotFound The requested job does not exist. The job '{identifier}' does not exist. 404 JobNotStarted Job has not been queued or started yet or was canceled and not restarted by the user. Job hasn't been started yet. 400 NoDataForUpdate For PATCH requests: No valid data specified at all. No valid data specified to be updated. 400 PaymentRequired The budget required to fulfil the request are insufficient. Payment required. 402 ProcessGraphComplexity The process graph is too complex for synchronous processing and likely to time out. Please use a batch job instead. The process graph is too complex for for synchronous processing. Please use a batch job instead. 400 ProcessGraphMissing No valid process graph specified. 400 PropertyNotEditable For PATCH requests: The specified parameter can't be updated. It is read-only. Property '{property}' is read-only. 400 RequestTimeout The request took too long and timed out. Request timed out. 408 StorageFailure Server couldn't store file(s) due to server-side reasons. Unable to store file(s). 500 StorageQuotaExceeded The storage quota has been exceeded by the user. Insufficient Storage. 400 VariableDefaultValueTypeInvalid The default value for the process graph variable '{variable_id}' is not of type '{type}'. 400 VariableIdInvalid A specified variable ID is not valid. 400 VariableTypeInvalid The data type for the process graph variable '{variable_id}' is invalid. Must be one of: string, boolean, number, array or object. 400 VariableValueMissing No value specified for process graph variable '{variable_id}'. 400 Process Graph Management openEO Error Code Description Example Message HTTP Status Code NoDataForUpdate For PATCH requests: No valid data specified at all. No valid data specified to be updated. 400 ProcessGraphMissing No valid process graph specified. 400 ProcessGraphNotFound The requested process graph does not exist. Process graph '{identifier}' does not exist. 404 PropertyNotEditable For PATCH requests: The specified parameter can't be updated. It is read-only. Property '{property}' is read-only. 400 VariableDefaultValueTypeInvalid The default value for the process graph variable '{variable_id}' is not of type '{type}'. 400 VariableIdInvalid A specified variable ID is not valid. 400 VariableTypeInvalid The data type for the process graph variable '{variable_id}' is invalid. Must be one of: string, boolean, number, array or object. 400 VariableValueMissing No value specified for process graph variable '{variable_id}'. 400 Processes openEO Error Code Description Example Message HTTP Status Code CRSInvalid Invalid or unsupported CRS specified. CRS '{crs}' is invalid. 400 CollectionNotFound The requested collection does not exist. Collection '{identifier}' does not exist. 404 CoordinateOutOfBounds Coordinate is out of bounds. 400 FileContentInvalid The content of the file is invalid. File content is invalid. 400 FileNotFound The requested file does not exist. File '{file}' does not exist. 404 JobNotFound The requested job does not exist. The job '{identifier}' does not exist. 404 ProcessArgumentInvalid The argument '{argument}' in process '{process}' is invalid: {reason} 400 ProcessArgumentRequired Process '{process}' requires argument '{argument}'. 400 ProcessArgumentUnsupported Process '{process}' does not support argument '{argument}'. 400 ProcessArgumentsMissing Process '{process}' requires at least {min_parameters} parameters. 400 ProcessUnsupported Process '{process}' is not supported. 400 Secondary Services Management openEO Error Code Description Example Message HTTP Status Code BillingPlanInvalid The billing plan is not on the list of available plans. The billing plan is not valid. 400 BudgetInvalid The budget is too low as it is either smaller than or equal to 0 or below the costs. The budget is too low. 400 NoDataForUpdate For PATCH requests: No valid data specified at all. No valid data specified to be updated. 400 PaymentRequired The budget required to fulfil the request are insufficient. Payment required. 402 ProcessGraphMissing No valid process graph specified. 400 PropertyNotEditable For PATCH requests: The specified parameter can't be updated. It is read-only. Property '{property}' is read-only. 400 ServiceArgumentInvalid The secondary service argument '{argument}' is invalid: {reason} 400 ServiceArgumentRequired Required secondary service argument '{argument}' is missing. 400 ServiceArgumentUnsupported Secondary service argument '{argument}' is not supported. 400 ServiceNotFound The requested secondary service does not exist. Service '{identifier}' does not exist. 404 ServiceUnsupported Secondary service type is not supported. 400 VariableValueMissing No value specified for process graph variable '{variable_id}'. 400","title":"Error Codes"},{"location":"errors/#openeo-error-codes","text":"The following table of error codes is incomplete . These error codes will evolve over time. If you are missing any common error, please contribute it by adding an issue , creating a pull request or get in contact in our chat room . The whole table of error codes is available as JSON file , which can be used by implementors to automatically generate error responses.","title":"openEO error codes"},{"location":"examples-poc/","text":"Examples (proof of concept) \u00b6 This page gives a detailed description of the openEO proof of concept use cases. After the proof of concept, this stays in the API to have some basic examples. The proof of concept covered three clearly defined example use cases and how they are translated to sequences of API calls: Deriving minimum NDVI measurements over pixel time series of Sentinel 2 imagery Create a monthly aggregated Sentinel 1 product from a custom Python script Compute time series of zonal (regional) statistics of Sentinel 2 imagery over user-specified polygons Note : CORS and authentication is not included in these examples for simplicity. Repeating calls are also not included as it would not make much sense to list the same discovery requests (see Use Case 1, requests 1 to 6) for each use case individually. Use Case 1 \u00b6 Deriving minimum NDVI measurements over pixel time series of Sentinel 2 imagery. A similar example (computing an EVI) is also available. Requesting the API versions available at the back-end Request GET /.well-known/openeo Requesting the capabilities of the back-end Note: The actual request path depends on the response of the previous request. Request GET / Check which collections are available at the back-end Request GET /collections HTTP / 1.1 Request details about a specific collection (e.g. Sentinel 2) Note: The actual collection ID in the path depends on the response of the previous request. Request GET /collections/Sentinel-2 HTTP / 1.1 Check that needed processes are available Request GET /processes HTTP / 1.1 Request the supported secondary web service types Request GET /service_types HTTP / 1.1 Create a WMS service Request POST /services HTTP / 1.1 Content-Type : application/json; charset=utf-8 { \"title\" : \"Min. NDVI for Sentinel 2\" , \"description\" : \"Deriving minimum NDVI measurements over pixel time series of Sentinel 2 imagery.\" , \"process_graph\" : { \"loadco1\" : { \"process_id\" : \"load_collection\" , \"arguments\" : { \"id\" : \"Sentinel-2\" , \"spatial_extent\" : { \"west\" : { \"variable_id\" : \"spatial_extent_west\" }, \"east\" : { \"variable_id\" : \"spatial_extent_east\" }, \"north\" : { \"variable_id\" : \"spatial_extent_north\" }, \"south\" : { \"variable_id\" : \"spatial_extent_south\" } }, \"temporal_extent\" : [ \"2017-01-01\" , \"2017-02-01\" ] } }, \"ndvi1\" : { \"process_id\" : \"ndvi\" , \"arguments\" : { \"data\" : { \"from_node\" : \"loadco1\" } } }, \"reduce1\" : { \"process_id\" : \"reduce\" , \"arguments\" : { \"data\" : { \"from_node\" : \"ndvi1\" }, \"dimension\" : \"temporal\" , \"reducer\" : { \"callback\" : { \"min1\" : { \"process_id\" : \"min\" , \"arguments\" : { \"data\" : { \"from_argument\" : \"data\" } }, \"result\" : true } } } }, \"result\" : true } }, \"type\" : \"WMS\" , \"parameters\" : { \"version\" : \"1.1.1\" } } Response HTTP / 1.1 201 Created Location : /services/wms-a3cca9 OpenEO-Identifier : wms-a3cca9 Requesting the service information Request GET /services/wms-a3cca9 HTTP / 1.1 Download the data on demand from the WMS Omitted, not part of the openEO API. Use Case 2 \u00b6 Create a monthly aggregated Sentinel 1 product from a custom Python script. Upload python script Request PUT /files/s1_aggregate.py HTTP / 1.1 Content-Type : application/octet-stream <File content> Create a batch job Request POST /jobs HTTP / 1.1 Content-Type : application/json; charset=utf-8 { \"title\" : \"Monthly aggregation on Sentinel 1\" , \"description\" : \"Create a monthly aggregated Sentinel 1 product from a custom Python script.\" , \"process_graph\" : { \"loadco1\" : { \"process_id\" : \"load_collection\" , \"arguments\" : { \"id\" : \"Sentinel-1\" , \"spatial_extent\" : { \"west\" : 16.1 , \"east\" : 16.6 , \"north\" : 48.6 , \"south\" : 47.2 }, \"temporal_extent\" : [ \"2017-01-01\" , \"2017-02-01\" ] } }, \"reduce1\" : { \"process_id\" : \"reduce\" , \"arguments\" : { \"data\" : { \"from_node\" : \"loadco1\" }, \"dimension\" : \"temporal\" , \"reducer\" : { \"callback\" : { \"runudf1\" : { \"process_id\" : \"run_udf\" , \"arguments\" : { \"data\" : [ { \"from_argument\" : \"x\" }, { \"from_argument\" : \"y\" } ], \"udf\" : \"s1_aggregate.py\" , \"runtime\" : \"Python\" }, \"result\" : true } } }, \"binary\" : true }, \"result\" : true } } } Response HTTP / 1.1 201 Created Location : https://openeo.org/api/v0.4/jobs/132 OpenEO-Identifier : 132 Start batch processing the job Request POST /jobs/132/results HTTP / 1.1 Create a TMS service Request POST /services HTTP / 1.1 Content-Type : application/json; charset=utf-8 { \"title\" : \"Monthly aggregation on Sentinel 1\" , \"description\" : \"Create a monthly aggregated Sentinel 1 product from a custom Python script.\" , \"process_graph\" : { \"1\" : { \"process_id\" : \"load_result\" , \"arguments\" : { \"id\" : \"132\" }, \"result\" : true } }, \"type\" : \"TMS\" } Response HTTP / 1.1 201 Created Location : https://openeo.org/api/v0.4/services/tms-75ff8c OpenEO-Identifier : tms-75ff8c Requesting the service information Request GET https://openeo.org/api/v0.4/services/tms-75ff8c HTTP / 1.1 Download the data on demand from the WMS Omitted, not part of the openEO API. Use Case 3 \u00b6 Compute time series of zonal (regional) statistics of Sentinel 2 imagery over user-specified polygons. Create a batch job Request POST /jobs HTTP / 1.1 Content-Type : application/json; charset=utf-8 { \"title\" : \"Zonal Statistics / Sentinel 2\" , \"description\" : \"Compute time series of zonal (regional) statistics of Sentinel 2 imagery over user-specified polygons.\" , \"process_graph\" : { \"loadco1\" : { \"process_id\" : \"load_collection\" , \"arguments\" : { \"id\" : \"Sentinel-2\" , \"spatial_extent\" : { \"west\" : 16.1 , \"east\" : 16.6 , \"north\" : 48.6 , \"south\" : 47.2 }, \"temporal_extent\" : [ \"2017-01-01\" , \"2017-02-01\" ], \"bands\" : [ \"B8\" ] } }, \"reduce1\" : { \"process_id\" : \"reduce\" , \"arguments\" : { \"data\" : { \"from_node\" : \"loadco1\" }, \"dimension\" : \"spectral\" } }, \"aggreg1\" : { \"process_id\" : \"aggregate_polygon\" , \"arguments\" : { \"data\" : { \"from_node\" : \"reduce1\" }, \"polygons\" : { \"type\" : \"Polygon\" , \"coordinates\" : [ [ [ 16.138916 , 48.320647 ], [ 16.524124 , 48.320647 ], [ 16.524124 , 48.1386 ], [ 16.138916 , 48.1386 ], [ 16.138916 , 48.320647 ] ] ] }, \"reducer\" : { \"callback\" : { \"mean1\" : { \"process_id\" : \"mean\" , \"arguments\" : { \"data\" : { \"from_argument\" : \"data\" } }, \"result\" : true } } } } }, \"savere1\" : { \"process_id\" : \"save_result\" , \"arguments\" : { \"data\" : { \"from_node\" : \"aggreg1\" }, \"format\" : \"JSON\" }, \"result\" : true } } } Response HTTP / 1.1 201 Created Location : https://openeo.org/jobs/133 OpenEO-Identifier : 133 Start batch processing the job Request POST /jobs/133/results HTTP / 1.1 Retrieve download links (after the job has finished) Request GET /jobs/133/results HTTP / 1.1 Response HTTP / 1.1 200 OK Content-Type : application/json; charset=utf-8 Expires : Wed, 01 May 2019 00:00:00 GMT OpenEO-Costs : 0 { \"id\" : \"133\" , \"title\" : \"Zonal Statistics / Sentinel 2\" , \"description\" : \"Compute time series of zonal (regional) statistics of Sentinel 2 imagery over user-specified polygons.\" , \"updated\" : \"2019-02-01T09:36:18Z\" , \"links\" : [ { \"href\" : \"https://cdn.openeo.org/4854b51643548ab8a858e2b8282711d8/result.json\" , \"type\" : \"application/json\" } ] } Download file(s) Request GET https://cdn.openeo.org/4854b51643548ab8a858e2b8282711d8/result.json HTTP / 1.1 Response A JSON file containing the results, content omitted.","title":"Examples (proof of concept)"},{"location":"examples-poc/#examples-proof-of-concept","text":"This page gives a detailed description of the openEO proof of concept use cases. After the proof of concept, this stays in the API to have some basic examples. The proof of concept covered three clearly defined example use cases and how they are translated to sequences of API calls: Deriving minimum NDVI measurements over pixel time series of Sentinel 2 imagery Create a monthly aggregated Sentinel 1 product from a custom Python script Compute time series of zonal (regional) statistics of Sentinel 2 imagery over user-specified polygons Note : CORS and authentication is not included in these examples for simplicity. Repeating calls are also not included as it would not make much sense to list the same discovery requests (see Use Case 1, requests 1 to 6) for each use case individually.","title":"Examples (proof of concept)"},{"location":"examples-poc/#use-case-1","text":"Deriving minimum NDVI measurements over pixel time series of Sentinel 2 imagery. A similar example (computing an EVI) is also available. Requesting the API versions available at the back-end Request GET /.well-known/openeo Requesting the capabilities of the back-end Note: The actual request path depends on the response of the previous request. Request GET / Check which collections are available at the back-end Request GET /collections HTTP / 1.1 Request details about a specific collection (e.g. Sentinel 2) Note: The actual collection ID in the path depends on the response of the previous request. Request GET /collections/Sentinel-2 HTTP / 1.1 Check that needed processes are available Request GET /processes HTTP / 1.1 Request the supported secondary web service types Request GET /service_types HTTP / 1.1 Create a WMS service Request POST /services HTTP / 1.1 Content-Type : application/json; charset=utf-8 { \"title\" : \"Min. NDVI for Sentinel 2\" , \"description\" : \"Deriving minimum NDVI measurements over pixel time series of Sentinel 2 imagery.\" , \"process_graph\" : { \"loadco1\" : { \"process_id\" : \"load_collection\" , \"arguments\" : { \"id\" : \"Sentinel-2\" , \"spatial_extent\" : { \"west\" : { \"variable_id\" : \"spatial_extent_west\" }, \"east\" : { \"variable_id\" : \"spatial_extent_east\" }, \"north\" : { \"variable_id\" : \"spatial_extent_north\" }, \"south\" : { \"variable_id\" : \"spatial_extent_south\" } }, \"temporal_extent\" : [ \"2017-01-01\" , \"2017-02-01\" ] } }, \"ndvi1\" : { \"process_id\" : \"ndvi\" , \"arguments\" : { \"data\" : { \"from_node\" : \"loadco1\" } } }, \"reduce1\" : { \"process_id\" : \"reduce\" , \"arguments\" : { \"data\" : { \"from_node\" : \"ndvi1\" }, \"dimension\" : \"temporal\" , \"reducer\" : { \"callback\" : { \"min1\" : { \"process_id\" : \"min\" , \"arguments\" : { \"data\" : { \"from_argument\" : \"data\" } }, \"result\" : true } } } }, \"result\" : true } }, \"type\" : \"WMS\" , \"parameters\" : { \"version\" : \"1.1.1\" } } Response HTTP / 1.1 201 Created Location : /services/wms-a3cca9 OpenEO-Identifier : wms-a3cca9 Requesting the service information Request GET /services/wms-a3cca9 HTTP / 1.1 Download the data on demand from the WMS Omitted, not part of the openEO API.","title":"Use Case 1"},{"location":"examples-poc/#use-case-2","text":"Create a monthly aggregated Sentinel 1 product from a custom Python script. Upload python script Request PUT /files/s1_aggregate.py HTTP / 1.1 Content-Type : application/octet-stream <File content> Create a batch job Request POST /jobs HTTP / 1.1 Content-Type : application/json; charset=utf-8 { \"title\" : \"Monthly aggregation on Sentinel 1\" , \"description\" : \"Create a monthly aggregated Sentinel 1 product from a custom Python script.\" , \"process_graph\" : { \"loadco1\" : { \"process_id\" : \"load_collection\" , \"arguments\" : { \"id\" : \"Sentinel-1\" , \"spatial_extent\" : { \"west\" : 16.1 , \"east\" : 16.6 , \"north\" : 48.6 , \"south\" : 47.2 }, \"temporal_extent\" : [ \"2017-01-01\" , \"2017-02-01\" ] } }, \"reduce1\" : { \"process_id\" : \"reduce\" , \"arguments\" : { \"data\" : { \"from_node\" : \"loadco1\" }, \"dimension\" : \"temporal\" , \"reducer\" : { \"callback\" : { \"runudf1\" : { \"process_id\" : \"run_udf\" , \"arguments\" : { \"data\" : [ { \"from_argument\" : \"x\" }, { \"from_argument\" : \"y\" } ], \"udf\" : \"s1_aggregate.py\" , \"runtime\" : \"Python\" }, \"result\" : true } } }, \"binary\" : true }, \"result\" : true } } } Response HTTP / 1.1 201 Created Location : https://openeo.org/api/v0.4/jobs/132 OpenEO-Identifier : 132 Start batch processing the job Request POST /jobs/132/results HTTP / 1.1 Create a TMS service Request POST /services HTTP / 1.1 Content-Type : application/json; charset=utf-8 { \"title\" : \"Monthly aggregation on Sentinel 1\" , \"description\" : \"Create a monthly aggregated Sentinel 1 product from a custom Python script.\" , \"process_graph\" : { \"1\" : { \"process_id\" : \"load_result\" , \"arguments\" : { \"id\" : \"132\" }, \"result\" : true } }, \"type\" : \"TMS\" } Response HTTP / 1.1 201 Created Location : https://openeo.org/api/v0.4/services/tms-75ff8c OpenEO-Identifier : tms-75ff8c Requesting the service information Request GET https://openeo.org/api/v0.4/services/tms-75ff8c HTTP / 1.1 Download the data on demand from the WMS Omitted, not part of the openEO API.","title":"Use Case 2"},{"location":"examples-poc/#use-case-3","text":"Compute time series of zonal (regional) statistics of Sentinel 2 imagery over user-specified polygons. Create a batch job Request POST /jobs HTTP / 1.1 Content-Type : application/json; charset=utf-8 { \"title\" : \"Zonal Statistics / Sentinel 2\" , \"description\" : \"Compute time series of zonal (regional) statistics of Sentinel 2 imagery over user-specified polygons.\" , \"process_graph\" : { \"loadco1\" : { \"process_id\" : \"load_collection\" , \"arguments\" : { \"id\" : \"Sentinel-2\" , \"spatial_extent\" : { \"west\" : 16.1 , \"east\" : 16.6 , \"north\" : 48.6 , \"south\" : 47.2 }, \"temporal_extent\" : [ \"2017-01-01\" , \"2017-02-01\" ], \"bands\" : [ \"B8\" ] } }, \"reduce1\" : { \"process_id\" : \"reduce\" , \"arguments\" : { \"data\" : { \"from_node\" : \"loadco1\" }, \"dimension\" : \"spectral\" } }, \"aggreg1\" : { \"process_id\" : \"aggregate_polygon\" , \"arguments\" : { \"data\" : { \"from_node\" : \"reduce1\" }, \"polygons\" : { \"type\" : \"Polygon\" , \"coordinates\" : [ [ [ 16.138916 , 48.320647 ], [ 16.524124 , 48.320647 ], [ 16.524124 , 48.1386 ], [ 16.138916 , 48.1386 ], [ 16.138916 , 48.320647 ] ] ] }, \"reducer\" : { \"callback\" : { \"mean1\" : { \"process_id\" : \"mean\" , \"arguments\" : { \"data\" : { \"from_argument\" : \"data\" } }, \"result\" : true } } } } }, \"savere1\" : { \"process_id\" : \"save_result\" , \"arguments\" : { \"data\" : { \"from_node\" : \"aggreg1\" }, \"format\" : \"JSON\" }, \"result\" : true } } } Response HTTP / 1.1 201 Created Location : https://openeo.org/jobs/133 OpenEO-Identifier : 133 Start batch processing the job Request POST /jobs/133/results HTTP / 1.1 Retrieve download links (after the job has finished) Request GET /jobs/133/results HTTP / 1.1 Response HTTP / 1.1 200 OK Content-Type : application/json; charset=utf-8 Expires : Wed, 01 May 2019 00:00:00 GMT OpenEO-Costs : 0 { \"id\" : \"133\" , \"title\" : \"Zonal Statistics / Sentinel 2\" , \"description\" : \"Compute time series of zonal (regional) statistics of Sentinel 2 imagery over user-specified polygons.\" , \"updated\" : \"2019-02-01T09:36:18Z\" , \"links\" : [ { \"href\" : \"https://cdn.openeo.org/4854b51643548ab8a858e2b8282711d8/result.json\" , \"type\" : \"application/json\" } ] } Download file(s) Request GET https://cdn.openeo.org/4854b51643548ab8a858e2b8282711d8/result.json HTTP / 1.1 Response A JSON file containing the results, content omitted.","title":"Use Case 3"},{"location":"gettingstarted-backends/","text":"Getting started for back-end providers \u00b6 As a back-end provider who wants to provide its datasets, processes and infrastructure to a broader audience through a standardized interface you may want to implement a driver for openEO. First of all, you should go through the list of openEO repositories and check whether there is already a back-end driver that suits your needs. In this case you don't need to develop your own driver, but \"only\" need to ingest your data, adopt your required processes and set up the infrastructure. Please follow the documentation for the individual driver you want to use. If your preferred technology has no back-end driver yet, you may consider writing your own driver. All software written for openEO should follow the software development guidelines . You certainly need to understand the glossary , the architecture of openEO and the concepts behind processes and process graphs . This helps you read and understand the API specification . Technical API related documents like CORS and error handing should be read, too. If you do not want to start from scratch, you could try to generate a server stub from the OpenAPI 3.0 -based API specification with the OpenAPI Generator . If you are using Python to implement your driver you may reuse some common modules of the existing driver implementations: Python Driver Commons You can implement a back-end in iterations. It is recommended to start by implementing the Capabilities microservice. EO Data Discovery , Process Discovery are important for the client libraries to be available, too. Afterwards you should implement Batch Job Management or synchronous data processing . All other microservices can be added later and are not strictly required to run openEO services. Keep in mind that you don't need to implement all endpoints in the first iteration and that you can specify in the Capabilities, which endpoints you are supporting. For example, you could start by implementing the following endpoints in the first iteration: Well-Known Document: GET /.well-known/openeo Capabilities: GET / and GET /file_formats Data discovery: GET /collections and GET /collections/{collection_id} Process discovery: GET /processes Data processing: POST /result Authentication (if required): GET /credentials/basic Afterwards you can already start experimenting with your first process graphs and process EO data with our client libraries on your back-end. More information will follow soon, for example about back-end compliance testing.","title":"Back-end Providers"},{"location":"gettingstarted-backends/#getting-started-for-back-end-providers","text":"As a back-end provider who wants to provide its datasets, processes and infrastructure to a broader audience through a standardized interface you may want to implement a driver for openEO. First of all, you should go through the list of openEO repositories and check whether there is already a back-end driver that suits your needs. In this case you don't need to develop your own driver, but \"only\" need to ingest your data, adopt your required processes and set up the infrastructure. Please follow the documentation for the individual driver you want to use. If your preferred technology has no back-end driver yet, you may consider writing your own driver. All software written for openEO should follow the software development guidelines . You certainly need to understand the glossary , the architecture of openEO and the concepts behind processes and process graphs . This helps you read and understand the API specification . Technical API related documents like CORS and error handing should be read, too. If you do not want to start from scratch, you could try to generate a server stub from the OpenAPI 3.0 -based API specification with the OpenAPI Generator . If you are using Python to implement your driver you may reuse some common modules of the existing driver implementations: Python Driver Commons You can implement a back-end in iterations. It is recommended to start by implementing the Capabilities microservice. EO Data Discovery , Process Discovery are important for the client libraries to be available, too. Afterwards you should implement Batch Job Management or synchronous data processing . All other microservices can be added later and are not strictly required to run openEO services. Keep in mind that you don't need to implement all endpoints in the first iteration and that you can specify in the Capabilities, which endpoints you are supporting. For example, you could start by implementing the following endpoints in the first iteration: Well-Known Document: GET /.well-known/openeo Capabilities: GET / and GET /file_formats Data discovery: GET /collections and GET /collections/{collection_id} Process discovery: GET /processes Data processing: POST /result Authentication (if required): GET /credentials/basic Afterwards you can already start experimenting with your first process graphs and process EO data with our client libraries on your back-end. More information will follow soon, for example about back-end compliance testing.","title":"Getting started for back-end providers"},{"location":"gettingstarted-clients/","text":"Getting started for client developers \u00b6 For easy access to openEO back-ends it is essential to provide client libraries for users in their well-known programming languages or working environments. This can be either a client library for a specific programming language that hides the technical details of the openEO API or an application with a user interface, e.g. a GIS software plugin or a web-based tool. All software written for openEO should follow the software development guidelines . Client library developers \u00b6 If your preferred programming language is not part of the available client libraries you may consider writing your own client library. Our client libraries are basically translating the openEO API into native concepts of the programming languages. Working with openEO should feel like being a first-class citizen of the programming language. Get started by reading the guidelines to develop client libraries , which have been written to ensure the client libraries provide a consistent feel and behavior across programming languages. You certainly need to understand the glossary and the concepts behind processes and process graphs . This helps you understand the API specification and related documents. If you do not want to start from scratch, you could try to generate a client library stub from the OpenAPI 3.0 -based API specification with the OpenAPI Generator . Make sure the generated code complies to the client library guidelines mentioned above. More information will follow soon, for example about client testing. Applications and Software plugins \u00b6 Standalone applications and software plugins written in a certain programming language could use the existing client libraries to facilitate access to openEO back-ends. Web applications potentially could use the JavaScript client to access openEO back-ends. Back-Ends may also provide standardized web interfaces such as OGC WMS or OGC WCS to access processed EO data. More information will follow soon...","title":"Client Developers"},{"location":"gettingstarted-clients/#getting-started-for-client-developers","text":"For easy access to openEO back-ends it is essential to provide client libraries for users in their well-known programming languages or working environments. This can be either a client library for a specific programming language that hides the technical details of the openEO API or an application with a user interface, e.g. a GIS software plugin or a web-based tool. All software written for openEO should follow the software development guidelines .","title":"Getting started for client developers"},{"location":"gettingstarted-clients/#client-library-developers","text":"If your preferred programming language is not part of the available client libraries you may consider writing your own client library. Our client libraries are basically translating the openEO API into native concepts of the programming languages. Working with openEO should feel like being a first-class citizen of the programming language. Get started by reading the guidelines to develop client libraries , which have been written to ensure the client libraries provide a consistent feel and behavior across programming languages. You certainly need to understand the glossary and the concepts behind processes and process graphs . This helps you understand the API specification and related documents. If you do not want to start from scratch, you could try to generate a client library stub from the OpenAPI 3.0 -based API specification with the OpenAPI Generator . Make sure the generated code complies to the client library guidelines mentioned above. More information will follow soon, for example about client testing.","title":"Client library developers"},{"location":"gettingstarted-clients/#applications-and-software-plugins","text":"Standalone applications and software plugins written in a certain programming language could use the existing client libraries to facilitate access to openEO back-ends. Web applications potentially could use the JavaScript client to access openEO back-ends. Back-Ends may also provide standardized web interfaces such as OGC WMS or OGC WCS to access processed EO data. More information will follow soon...","title":"Applications and Software plugins"},{"location":"gettingstarted-users/","text":"Getting started for users \u00b6 Currently, there are three official client libraries and a web-based interface for openEO. If you are unfamiliar with programming, you could start using the web-based editor for openEO . It supports visual modelling of your algorithms and a simplified JavaScript based access to the openEO workflows and providers. If you are familiar with programming, you could choose a client library for three programming languages: JavaScript (client-side and server-side) Python R Follow the links above to find usage instructions for each of the client libraries. Contribute \u00b6 Didn't find your programming language? You can also access the openEO API implementations directly or start implementing your own client library . If you are missing any functionality in the API feel free to open an issue or actively start proposing API changes as Pull Requests. Feel free to contact us for further assistance.","title":"Users"},{"location":"gettingstarted-users/#getting-started-for-users","text":"Currently, there are three official client libraries and a web-based interface for openEO. If you are unfamiliar with programming, you could start using the web-based editor for openEO . It supports visual modelling of your algorithms and a simplified JavaScript based access to the openEO workflows and providers. If you are familiar with programming, you could choose a client library for three programming languages: JavaScript (client-side and server-side) Python R Follow the links above to find usage instructions for each of the client libraries.","title":"Getting started for users"},{"location":"gettingstarted-users/#contribute","text":"Didn't find your programming language? You can also access the openEO API implementations directly or start implementing your own client library . If you are missing any functionality in the API feel free to open an issue or actively start proposing API changes as Pull Requests. Feel free to contact us for further assistance.","title":"Contribute"},{"location":"glossary/","text":"Glossary \u00b6 This glossary introduces the major technical terms used in the openEO project. General terms \u00b6 EO : Earth observation API : application programming interface ( wikipedia ); a communication protocol between client and back-end client : software tool or environment that end-users directly interact with, e.g. R (RStudio), Python (Jupyter notebook), and JavaScript (web browser); R and Python are two major data science platforms; JavaScript is a major language for web development (cloud) back-end : server; computer infrastructure (one or more physical computers or virtual machines) used for storing EO data and processing it big Earth observation cloud back-end : server infrastructure where industry and researchers analyse large amounts of EO data Processes and process graphs \u00b6 The terms process and process graph have specific meanings in the openEO API specification. A process is an operation provided by the back end that performs a specific task on a set of parameters and returns a result. An example is computing a statistical operation, such as mean or median, on selected EO data. A process is similar to a function or method in programming languages. A process graph chains specific process calls together. Similarly to scripts in the context of programming, process graphs organize and automate the execution of one or more processes that could alternatively be executed individually. In a process graph, processes need to be specific, i.e. concrete values for input parameters need to be specified. These arguments can again be process graphs, scalar values, arrays or objects. EO data (Collections) \u00b6 In our domain, different terms are used to describe EO data(sets). Within openEO, a granule (sometimes also called item or asset in the specification) typically refers to a limited area and a single overpass leading to a very short observation period (seconds) or a temporal aggregation of such data (e.g. for 16-day MODIS composites). A collection is a sequence of granules sharing the same product specification. It typically corresponds to the series of products derived from data acquired by a sensor on board a satellite and having the same mode of operation. The CEOS OpenSearch Best Practice Document v1.2 lists the following synonyms used by other organizations: granule : dataset (ESA, ISO 19115), granule (NASA), product (ESA, CNES), scene (JAXA) collection : dataset series (ESA, ISO 19115), collection (CNES, NASA), dataset (JAXA), product (JAXA) In openEO, a back-end offers a set of collections to be processed. All collections can be requested using a client and are described using the STAC (SpatioTemporal Asset Catalog) metadata specification as STAC collections. A user can load (a subset of) a collection using a special process, which returns a (spatial) data cube. All further processing is then applied to the data cube on the back-end. Spatial data cubes \u00b6 A spatiotemporal data cube is a multidimensional array with one or more spatial or temporal dimensions. In the EO domain, it is common to be implicit about the temporal dimension and just refer to them as spatial data cubes in short. Special cases are raster and vector data cubes. The figure below shows the data of a four-dimensional (8 x 8 x 2 x 2) raster data cube, with dimension names and values: # dimension name dimension values 1 x 288790.5, 288819, 288847.5, 288876, 288904.5, 288933, 288961.5, 288990 2 y 9120747, 9120718, 9120690, 9120661, 9120633, 9120604, 9120576, 9120547 3 band red , green 4 time 2018-02-10 , 2018-02-17 dimensions x and time are aligned along the x-axis; y and band are aligned along the y-axis. Data cubes as defined here have a single value (scalar) for each unique combination of dimension values. The value pointed to by arrows corresponds to the combination of x=288847.5 (red arrow), y=9120661 (yellow arrow), band=red (blue arrow), time=2018-02-17 (green arrow), and its value is 84 (brown arrow). If the data concerns grayscale imagery, we could call this single value a pixel value . One should keep in mind that it is never a tuple of, say, {red, green, blue} values. \"Cell value of a single raster layer\" would be a better analogy; data cube cell value may be a good compromise. A data cube stores some additional properties per dimension such as: name axis / number type extents or nominal dimension values reference systems / projections resolutions Having these properties available allows to easily resample from one data cube to another for example. apply : processes that do not change dimensions \u00b6 Math process that does not reduce or change anything to the array dimensions. The process apply can be used to apply unary functions such as abs or sqrt to all values in a data cube. The process apply_dimension applies (maps) an n-ary function to a particular dimension. An example along the time dimension is to apply a moving average filter to implement temporal smoothing. An example of apply_dimension to the spatial dimensions is to do a historgram stretch for every spatial (grayscale) image of an image time series. filter : subsetting dimensions by dimension value selection \u00b6 The filter process makes a cube smaller by selecting specific value ranges for a particular dimension. Examples: a band filter that selects the red band a bounding box filter \"crops\" the collection to a spatial extent reduce : removing dimensions entirely by computation \u00b6 The reduce process removes a dimension by \"rolling up\" or summarizing the values along that dimension to a single value. For example: eliminate the time dimension by taking the mean along that dimension. Another example is taking the sum or max along the band dimension. aggregate : reducing resolution \u00b6 Aggregation computes new values from sets of values that are uniquely assigned to groups. It involves a grouping predicate (e.g. monthly, 100 m x 100 m grid cells, or a set of non-overlapping spatial polygons), and an reducer (e.g., mean ) that computes one or more new values from the original ones. In effect, aggregate combines the following three steps: split the data cube in groups, based on dimension constraints (time intervals, band groups, spatial polygons) apply a reducer to each group (similar to the reduce process, but reducing a group rather than an entire dimension) combine the result to a new data cube, with some dimensions having reduced resolution (or e.g. raster to vector converted) Examples: a weekly time series may be aggregated to monthly values by computing the mean for all values in a month (grouping predicate: months) spatial aggregation involves computing e.g. mean pixel values on a 100 x 100 m grid, from 10 m x 10 m pixels, where each original pixel is assigned uniquely to a larger pixel (grouping predicate: 100 m x 100 m grid cells) resample : changing data cube geometry \u00b6 Resampling considers the case where we have data at one resolution and coordinate reference system, and need values at another. In case we have values at a 100 m x 100 m grid and need values at a 10 m x 10 m grid, the original values will be reused many times, and may be simply assigned to the nearest high resolution grid cells (nearest neighbor method), or may be interpolated using various methods (e.g. by bilinear interpolation). This is often called upsampling or upscaling . Resampling from finer to coarser grid is a special case of aggregation often called downsampling or downscaling . When the target grid or time series has a lower resolution (larger grid cells) or lower frequency (longer time intervals) than the source grid, aggregation might be used for resampling. For example, if the resolutions are similar, (e.g. the source collection provides 10 day intervals and the target needs values for 16 day intervals), then some form of interpolation may be more appropriate than aggregation as defined here. User-defined function (UDF) \u00b6 The abbreviation UDF stands for user-defined function . With this concept, users are able to upload custom code and have it executed e.g. for every pixel of a scene, or applied to a particular dimension or set of dimensions, allowing custom server-side calculations. See the section on UDFs for more information. Data Processing modes \u00b6 Process graphs can be processed in three different ways: Results can be pre-computed by creating a batch job using POST /jobs . They are submitted to the back-end's processing system, but will remain inactive until POST /jobs/{job_id}/results has been called. They will run only once and store results after execution. Results can be downloaded. Batch jobs are typically time consuming and user interaction is not possible. This is the only mode that allows to get an estimate about time, volume and costs beforehand. A more dynamic way of processing and accessing data is to create a secondary web service . They allow web-based access using different protocols such as OGC WMS (Open Geospatial Consortium Web Map Service), OGC WCS (Web Coverage Service) or XYZ tiles . These protocols usually allow users to change the viewing extent or level of detail (zoom level). Therefore, computations often run on demand so that the requested data is calculated during the request. Back-ends should make sure to cache processed data to avoid additional/high costs and reduce waiting times for the user. Process graphs can also be executed on-demand (i.e. synchronously) by sending the process graph to POST /result . Results are delivered with the request itself and no job is created. Only lightweight computations, for example small previews, should be executed using this approach as timeouts or errors are to be expected for long-polling HTTP requests .","title":"Glossary"},{"location":"glossary/#glossary","text":"This glossary introduces the major technical terms used in the openEO project.","title":"Glossary"},{"location":"glossary/#general-terms","text":"EO : Earth observation API : application programming interface ( wikipedia ); a communication protocol between client and back-end client : software tool or environment that end-users directly interact with, e.g. R (RStudio), Python (Jupyter notebook), and JavaScript (web browser); R and Python are two major data science platforms; JavaScript is a major language for web development (cloud) back-end : server; computer infrastructure (one or more physical computers or virtual machines) used for storing EO data and processing it big Earth observation cloud back-end : server infrastructure where industry and researchers analyse large amounts of EO data","title":"General terms"},{"location":"glossary/#processes-and-process-graphs","text":"The terms process and process graph have specific meanings in the openEO API specification. A process is an operation provided by the back end that performs a specific task on a set of parameters and returns a result. An example is computing a statistical operation, such as mean or median, on selected EO data. A process is similar to a function or method in programming languages. A process graph chains specific process calls together. Similarly to scripts in the context of programming, process graphs organize and automate the execution of one or more processes that could alternatively be executed individually. In a process graph, processes need to be specific, i.e. concrete values for input parameters need to be specified. These arguments can again be process graphs, scalar values, arrays or objects.","title":"Processes and process graphs"},{"location":"glossary/#eo-data-collections","text":"In our domain, different terms are used to describe EO data(sets). Within openEO, a granule (sometimes also called item or asset in the specification) typically refers to a limited area and a single overpass leading to a very short observation period (seconds) or a temporal aggregation of such data (e.g. for 16-day MODIS composites). A collection is a sequence of granules sharing the same product specification. It typically corresponds to the series of products derived from data acquired by a sensor on board a satellite and having the same mode of operation. The CEOS OpenSearch Best Practice Document v1.2 lists the following synonyms used by other organizations: granule : dataset (ESA, ISO 19115), granule (NASA), product (ESA, CNES), scene (JAXA) collection : dataset series (ESA, ISO 19115), collection (CNES, NASA), dataset (JAXA), product (JAXA) In openEO, a back-end offers a set of collections to be processed. All collections can be requested using a client and are described using the STAC (SpatioTemporal Asset Catalog) metadata specification as STAC collections. A user can load (a subset of) a collection using a special process, which returns a (spatial) data cube. All further processing is then applied to the data cube on the back-end.","title":"EO data (Collections)"},{"location":"glossary/#spatial-data-cubes","text":"A spatiotemporal data cube is a multidimensional array with one or more spatial or temporal dimensions. In the EO domain, it is common to be implicit about the temporal dimension and just refer to them as spatial data cubes in short. Special cases are raster and vector data cubes. The figure below shows the data of a four-dimensional (8 x 8 x 2 x 2) raster data cube, with dimension names and values: # dimension name dimension values 1 x 288790.5, 288819, 288847.5, 288876, 288904.5, 288933, 288961.5, 288990 2 y 9120747, 9120718, 9120690, 9120661, 9120633, 9120604, 9120576, 9120547 3 band red , green 4 time 2018-02-10 , 2018-02-17 dimensions x and time are aligned along the x-axis; y and band are aligned along the y-axis. Data cubes as defined here have a single value (scalar) for each unique combination of dimension values. The value pointed to by arrows corresponds to the combination of x=288847.5 (red arrow), y=9120661 (yellow arrow), band=red (blue arrow), time=2018-02-17 (green arrow), and its value is 84 (brown arrow). If the data concerns grayscale imagery, we could call this single value a pixel value . One should keep in mind that it is never a tuple of, say, {red, green, blue} values. \"Cell value of a single raster layer\" would be a better analogy; data cube cell value may be a good compromise. A data cube stores some additional properties per dimension such as: name axis / number type extents or nominal dimension values reference systems / projections resolutions Having these properties available allows to easily resample from one data cube to another for example.","title":"Spatial data cubes"},{"location":"glossary/#apply-processes-that-do-not-change-dimensions","text":"Math process that does not reduce or change anything to the array dimensions. The process apply can be used to apply unary functions such as abs or sqrt to all values in a data cube. The process apply_dimension applies (maps) an n-ary function to a particular dimension. An example along the time dimension is to apply a moving average filter to implement temporal smoothing. An example of apply_dimension to the spatial dimensions is to do a historgram stretch for every spatial (grayscale) image of an image time series.","title":"apply: processes that do not change dimensions"},{"location":"glossary/#filter-subsetting-dimensions-by-dimension-value-selection","text":"The filter process makes a cube smaller by selecting specific value ranges for a particular dimension. Examples: a band filter that selects the red band a bounding box filter \"crops\" the collection to a spatial extent","title":"filter: subsetting dimensions by dimension value selection"},{"location":"glossary/#reduce-removing-dimensions-entirely-by-computation","text":"The reduce process removes a dimension by \"rolling up\" or summarizing the values along that dimension to a single value. For example: eliminate the time dimension by taking the mean along that dimension. Another example is taking the sum or max along the band dimension.","title":"reduce: removing dimensions entirely by computation"},{"location":"glossary/#aggregate-reducing-resolution","text":"Aggregation computes new values from sets of values that are uniquely assigned to groups. It involves a grouping predicate (e.g. monthly, 100 m x 100 m grid cells, or a set of non-overlapping spatial polygons), and an reducer (e.g., mean ) that computes one or more new values from the original ones. In effect, aggregate combines the following three steps: split the data cube in groups, based on dimension constraints (time intervals, band groups, spatial polygons) apply a reducer to each group (similar to the reduce process, but reducing a group rather than an entire dimension) combine the result to a new data cube, with some dimensions having reduced resolution (or e.g. raster to vector converted) Examples: a weekly time series may be aggregated to monthly values by computing the mean for all values in a month (grouping predicate: months) spatial aggregation involves computing e.g. mean pixel values on a 100 x 100 m grid, from 10 m x 10 m pixels, where each original pixel is assigned uniquely to a larger pixel (grouping predicate: 100 m x 100 m grid cells)","title":"aggregate: reducing resolution"},{"location":"glossary/#resample-changing-data-cube-geometry","text":"Resampling considers the case where we have data at one resolution and coordinate reference system, and need values at another. In case we have values at a 100 m x 100 m grid and need values at a 10 m x 10 m grid, the original values will be reused many times, and may be simply assigned to the nearest high resolution grid cells (nearest neighbor method), or may be interpolated using various methods (e.g. by bilinear interpolation). This is often called upsampling or upscaling . Resampling from finer to coarser grid is a special case of aggregation often called downsampling or downscaling . When the target grid or time series has a lower resolution (larger grid cells) or lower frequency (longer time intervals) than the source grid, aggregation might be used for resampling. For example, if the resolutions are similar, (e.g. the source collection provides 10 day intervals and the target needs values for 16 day intervals), then some form of interpolation may be more appropriate than aggregation as defined here.","title":"resample: changing data cube geometry"},{"location":"glossary/#user-defined-function-udf","text":"The abbreviation UDF stands for user-defined function . With this concept, users are able to upload custom code and have it executed e.g. for every pixel of a scene, or applied to a particular dimension or set of dimensions, allowing custom server-side calculations. See the section on UDFs for more information.","title":"User-defined function (UDF)"},{"location":"glossary/#data-processing-modes","text":"Process graphs can be processed in three different ways: Results can be pre-computed by creating a batch job using POST /jobs . They are submitted to the back-end's processing system, but will remain inactive until POST /jobs/{job_id}/results has been called. They will run only once and store results after execution. Results can be downloaded. Batch jobs are typically time consuming and user interaction is not possible. This is the only mode that allows to get an estimate about time, volume and costs beforehand. A more dynamic way of processing and accessing data is to create a secondary web service . They allow web-based access using different protocols such as OGC WMS (Open Geospatial Consortium Web Map Service), OGC WCS (Web Coverage Service) or XYZ tiles . These protocols usually allow users to change the viewing extent or level of detail (zoom level). Therefore, computations often run on demand so that the requested data is calculated during the request. Back-ends should make sure to cache processed data to avoid additional/high costs and reduce waiting times for the user. Process graphs can also be executed on-demand (i.e. synchronously) by sending the process graph to POST /result . Results are delivered with the request itself and no job is created. Only lightweight computations, for example small previews, should be executed using this approach as timeouts or errors are to be expected for long-polling HTTP requests .","title":"Data Processing modes"},{"location":"guidelines-clients/","text":"Client library development guidelines \u00b6 This is a proposal for workflows that client libraries should support to make the experience with each library similar and users can easily adopt examples and workflows. For best experience libraries should still embrace best practices common in their environments. This means clients can... choose which kind of casing they use (see below). feel free to implement aliases for methods. Conventions \u00b6 Casing \u00b6 Clients can use snake_case , camelCase or any method used commonly in their environment. For example, the API request to get a list of collections can either be names get_collections or getCollections . This applies for all names, including scopes, method names and parameters. Scopes \u00b6 Each method belongs to a scope. To achieve this in object-oriented (OO) programming languages, methods would be part of a class. If programming languages don't support scopes, you may need to simulate it somehow to prevent name collisions, e.g. by adding a prefix to the method names (like in the \"procedural style\" example below). Best practices for this will likely evolve over time. Example for the clientVersion method in openEO : Procedural style: openeo_client_version() Object-oriented style: OpenEO obj = new OpenEO (); obj . clientVersion (); If you can't store scope data in an object, you may need to pass these information as argument(s) to the method. Example: Procedural style: $connection = openeo_connect(\"https://openeo.org\"); openeo_capabilities($connection); Object-oriented style: OpenEO obj = new OpenEO (); Connection con = obj . connect ( \"https://openeo.org\" ); con . capabilities (); Scope categories \u00b6 Each scope is assigned to a scope category, of which there are three: Root category: Contains only the scope openEO . API category: Mostly methods hiding API calls to the back-ends. Methods may be implemented asynchronously. Contains the scopes Connection , File , Job , ProcessGraph , Service . Content : Mostly methods hiding the complexity of response content. Methods are usually implemented synchronously. Currently contains only the scope Capabilities . Method names should be prefixed if name collisions are likely. Method names across ALL the scopes that belong to the root or API categories MUST be unique. This is the case because the parameter in hasFeature(method_name) must be unambiguous. Method names of scopes in the Content category may collide with method names of scopes in the root / API categories and names should be prefixed if collisions of names between different scope categories are to be expected. Parameters \u00b6 The parameters usually follow the request schemes in the openAPI specification. The parameters should follow their characteristics, for example regarding the default values. Some methods have a long list of (optional) parameters. This is easy to implement in languages that support named parameters such as R. For example, creating a job in R with a budget would lead to this method call: createJob ( process_graph = { ... }, budget = 123 ) Other languages that only support non-named parameters (i.e. the order of parameters is fixed) need to fill many parameters with default values, which is not convenient for a user. The example above in PHP would be: createJob({...}, null, null, null, null, null, 123) To avoid such method calls client developers should consider to pass either an instance of a class, which contains all parameters as member variables or the required parameters directly and the optional parameters as a dictionary (see example below). This basically emulates named parameters. The member variables / dictionary keys should use the same names as the parameters. The exemplary method call in PHP could be improved as follows: createJob({...}, [budget => 123]) Method mappings \u00b6 Note: Some scopes for response JSON objects are still missing. We are open for proposals. Parameters with a leading ? are optional. Scope: openEO (root category) \u00b6 Description Client method Connect to a back-end, includes version discovery ( GET /.well-known/openeo ), requesting capabilities and authentication where required. Returns Connection . connect(url, ?authType, ?authOptions) Get client library version. clientVersion() Parameters authType in connect : null , basic or oidc (non-exclusive). Defaults to null (no authentication). authOptions in connect : May hold additional data for authentication, for example a username and password for basic authentication. Scope: Connection (API category) \u00b6 Description API Request Client method Get the capabilities of the back-end. Returns Capabilities . GET / capabilities() List the supported output file formats. GET /file_formats listFileTypes() List the supported secondary service types. GET /service_types listServiceTypes() List the supported UDF runtimes. GET /udf_runtimes listUdfRuntimes() List all collections available on the back-end. GET /collections listCollections() Get information about a single collection. GET /collections/{collection_id} describeCollection(collection_id) List all processes available on the back-end. GET /processes listProcesses() Authenticate with OpenID Connect (if not specified in connect ). GET /credentials/oidc authenticateOIDC(?options) Authenticate with HTTP Basic (if not specified in connect ). GET /credentials/basic authenticateBasic(username, password) Logout / Close session for the authenticated user Depends on authentication method logout() Get information about the authenticated user. GET /me describeAccount() Lists all files from a user. Returns a list of File . GET /files listFiles() Opens a (existing or non-existing) file without reading any information. Returns a File . None openFile(path) Validates a process graph. POST /validation validateProcessGraph(processGraph) Lists all process graphs of the authenticated user. Returns a list of ProcessGraph . GET /process_graphs listProcessGraphs() Creates a new stored process graph. Returns a ProcessGraph . POST /process_graphs createProcessGraph(processGraph, ?title, ?description) Get all information about a stored process graph. Returns a ProcessGraph . GET /process_graphs/{process_graph_id} getProcessGraphById(id) Executes a process graph synchronously. POST /result computeResult(processGraph, ?plan, ?budget) Lists all jobs of the authenticated user. Returns a list of Job . GET /jobs listJobs() Creates a new job. Returns a Job . POST /jobs createJob(processGraph, ?title, ?description, ?plan, ?budget, ?additional) Get all information about a job. Returns a Job . GET /jobs/{job_id} getJobById(id) Lists all secondary services of the authenticated user. Returns a list of Service . GET /services listServices() Creates a new secondary service. Returns a Service . POST /services createService(processGraph, type, ?title, ?description, ?enabled, ?parameters, ?plan, ?budget) Get all information about a service. Returns a Service . GET /services/{service_id} getServiceById(id) Parameters options in authenticateOIDC : May hold additional data required for OpenID connect authentication. Scope Capabilities (Content category) \u00b6 Should be prefixed with Capabilities if collisions of names between different scope categories are to be expected. Description Field Client method Get the implemented openEO version. api_version apiVersion() Get the back-end version. backend_version backendVersion() Get the name of the back-end. title title() Get the description of the back-end. description description() List all supported features / endpoints. endpoints listFeatures() Check whether a feature / endpoint is supported. endpoints > ... hasFeature(methodName) Get the default billing currency. billing > currency currency() List all billing plans. billing > plans listPlans() Parameters methodName in hasFeature : The name of a client method in any of the scopes that are part of the API category. E.g. hasFeature(\"describeAccount\") checks whether the GET /me endpoint is contained in the capabilities response's endpoints object. Scope: File (API category) \u00b6 The File scope internally knows the user_id and the path . Description API Request Client method Download a user file. GET /files/{path} downloadFile(target) Upload a user file. PUT /files/{path} uploadFile(source) Delete a user file. DELETE /files/{path} deleteFile() Parameters target in downloadFile : Path to a local file or folder. Scope: Job (API category) \u00b6 The Job scope internally knows the job_id . Description API Request Client method Get (and update on client-side) all job information. GET /jobs/{job_id} describeJob() Modify a job at the back-end. PATCH /jobs/{job_id} updateJob(?processGraph, ?title, ?description, ?plan, ?budget, ?additional) Delete a job DELETE /jobs/{job_id} deleteJob() Calculate an time/cost estimate for a job. GET /jobs/{job_id}/estimate estimateJob() Start / queue a job for processing. POST /jobs/{job_id}/results startJob() Stop / cancel job processing. DELETE /jobs/{job_id}/results stopJob() Get document with download links. GET /jobs/{job_id}/results listResults() Download job results. GET /jobs/{job_id}/results > ... downloadResults(target) Parameters target in downloadResults : Path to a local folder. Scope: ProcessGraph (API category) \u00b6 The ProcessGraph scope internally knows the process_graph_id . Description API Request Client method Get (and update on client-side) all information about a stored process graph. GET /process_graphs/{process_graph_id} describeProcessGraph() Modify a stored process graph at the back-end. PATCH /process_graphs/{process_graph_id} updateProcessGraph(?processGraph, ?title, ?description) Delete a stored process graph. DELETE /process_graphs/{process_graph_id} deleteProcessGraph() Scope: Service (API category) \u00b6 The Service scope internally knows the service_id . Description API Request Client method Get (and update on client-side) all information about a secondary web service. GET /services/{service_id} describeService() Modify a secondary web service at the back-end. PATCH /services/{service_id} updateService(?processGraph, ?title, ?description, ?enabled, ?parameters, ?plan, ?budget) Delete a secondary web service. DELETE /services/{service_id} deleteService() Processes \u00b6 The processes a back-end supports may be offered by the clients as methods in its own scope. The method names should follow the process names, but the conventions listed above can be applied here as well, e.g. converting filter_bands to filterBands . As parameters have no natural or technical ordering in the JSON objects, clients must come up with a reasonable ordering of parameters if required. This could be inspired by existing clients. The way of building a process graph from processes heavily depends on the technical capabilities of the programming language. Therefore it may differ between the client libraries. Follow the best practices of the programming language, e.g. support method chaining if possible. Workflow example \u00b6 Some simplified example workflows using different programming styles are listed below. The following steps are executed: Loading the client library. Connecting to a back-end and authenticating with username and password via OpenID Connect. Requesting the capabilities and showing the implemented openEO version of the back-end. Showing information about the \"Sentinel-2A\" collection. Showing information about all processes supported by the back-end. Building a simple process graph. Creating a job. Pushing the job to the processing queue. After a while, showing the job details, e.g. checking the job status. Once processing is finished, downloading the job results to the local directory /tmp/job_results/ . Please note that the examples below do not comply to the latest process specification. They are meant to show the differences in client development, but are no working examples! R (functional style) \u00b6 library ( openeo ) con = connect ( \"https://openeo.org\" , \"username\" , \"password\" ) cap = capabilities () cap %>% apiVersion () con %>% describeCollection ( \"Sentinel-2A\" ) con %>% listProcesses () processgraph = process ( \"load_collection\" , id = \"Sentinel-2A\" ) %>% process ( \"filter_bbox\" , west = 672000 , south = 5181000 , east = 652000 , north = 5161000 , crs = \"EPSG:32632\" ) %>% process ( \"filter_temporal\" , extent = c ( \"2017-01-01T00:00:00Z\" , \"2017-01-31T23:59:59Z\" )) %>% process ( \"ndvi\" , nir = \"B4\" , red = \"B8A\" ) %>% process ( \"min_time\" ) job = con %>% createJob ( processgraph ) job %>% startJob () job %>% describeJob () job %>% downloadResults ( \"/tmp/job_results/\" ) Python (mixed style) \u00b6 import openeo con = openeo . connect ( \"https://openeo.org\" , \"username\" , \"password\" ) cap = con . capabilities () print ( cap . api_version ()) print ( con . describe_collection ( \"Sentinel-2A\" )) print ( con . list_processes ()) processes = con . get_processes () pg = processes . load_collection ( id = \"Sentinel-2A\" ) pg = processes . filter_bbox ( pg , west = 672000 , south = 5181000 , east = 652000 , north = 5161000 , crs = \"EPSG:32632\" ) pg = processes . filter_temporal ( pg , extent = [ \"2017-01-01T00:00:00Z\" , \"2017-01-31T23:59:59Z\" ]) pg = processes . ndvi ( pg , nir = \"B4\" , red = \"B8A\" ) pg = processes . min_time ( pg ) job = con . create_job ( pg . graph ) job . start_job () print job . describe_job () job . download_results ( \"/tmp/job_results/\" ) Java (object oriented style) \u00b6 import org.openeo.* ; OpenEO obj = new OpenEO (); Connection con = obj . connect ( \"https://openeo.org\" , \"username\" , \"password\" ); Capabilities cap = con . capabilities (); System . out . println ( cap . apiVersion ()); System . out . println ( con . describeCollection ( \"Sentinel-2A\" )); System . out . println ( con . listProcesses ()); ProcessGraphBuilder pgb = con . getProcessGraphBuilder () // Chain processes... ProcessGraph processGraph = pgb . buildProcessGraph (); Job job = con . createJob ( processGraph ); job . startJob (); System . out . println ( job . describeJob ()); job . downloadResults ( \"/tmp/job_results/\" ); PHP (procedural style) \u00b6 require_once(\"/path/to/openeo.php\"); $connection = openeo_connect(\"http://openeo.org\", \"username\", \"password\"); $capabilities = openeo_capabilities($connection); echo openeo_api_version($capabilites); echo openeo_describe_collection($connection, \"Sentinel-2A\"); echo openeo_list_processes($connection); $pg = openeo_process($pg, \"load_collection\", [\"id\" => \"Sentinel-2A\"]); $pg = openeo_process($pg, \"filter_bbox\", [\"west\" => 672000, \"south\" => 5181000, \"east\" => 652000, \"north\" => 5161000, \"crs\" => \"EPSG:32632\"]); $pg = openeo_process($pg, \"filter_temporal\", [\"extent\" => [\"2017-01-01T00:00:00Z\", \"2017-01-31T23:59:59Z\"]]); $pg = openeo_process($pg, \"ndvi\", [\"red\" => \"B4\", \"nir\" => \"B8A\"]); $pg = openeo_process($pg, \"min_time\"); $job = openeo_create_job($connection, $pg); openeo_start_job($job); echo openeo_describe_job($job); openeo_download_results($job, \"/tmp/job_results/\");","title":"Client Library Development"},{"location":"guidelines-clients/#client-library-development-guidelines","text":"This is a proposal for workflows that client libraries should support to make the experience with each library similar and users can easily adopt examples and workflows. For best experience libraries should still embrace best practices common in their environments. This means clients can... choose which kind of casing they use (see below). feel free to implement aliases for methods.","title":"Client library development guidelines"},{"location":"guidelines-clients/#conventions","text":"","title":"Conventions"},{"location":"guidelines-clients/#casing","text":"Clients can use snake_case , camelCase or any method used commonly in their environment. For example, the API request to get a list of collections can either be names get_collections or getCollections . This applies for all names, including scopes, method names and parameters.","title":"Casing"},{"location":"guidelines-clients/#scopes","text":"Each method belongs to a scope. To achieve this in object-oriented (OO) programming languages, methods would be part of a class. If programming languages don't support scopes, you may need to simulate it somehow to prevent name collisions, e.g. by adding a prefix to the method names (like in the \"procedural style\" example below). Best practices for this will likely evolve over time. Example for the clientVersion method in openEO : Procedural style: openeo_client_version() Object-oriented style: OpenEO obj = new OpenEO (); obj . clientVersion (); If you can't store scope data in an object, you may need to pass these information as argument(s) to the method. Example: Procedural style: $connection = openeo_connect(\"https://openeo.org\"); openeo_capabilities($connection); Object-oriented style: OpenEO obj = new OpenEO (); Connection con = obj . connect ( \"https://openeo.org\" ); con . capabilities ();","title":"Scopes"},{"location":"guidelines-clients/#scope-categories","text":"Each scope is assigned to a scope category, of which there are three: Root category: Contains only the scope openEO . API category: Mostly methods hiding API calls to the back-ends. Methods may be implemented asynchronously. Contains the scopes Connection , File , Job , ProcessGraph , Service . Content : Mostly methods hiding the complexity of response content. Methods are usually implemented synchronously. Currently contains only the scope Capabilities . Method names should be prefixed if name collisions are likely. Method names across ALL the scopes that belong to the root or API categories MUST be unique. This is the case because the parameter in hasFeature(method_name) must be unambiguous. Method names of scopes in the Content category may collide with method names of scopes in the root / API categories and names should be prefixed if collisions of names between different scope categories are to be expected.","title":"Scope categories"},{"location":"guidelines-clients/#parameters","text":"The parameters usually follow the request schemes in the openAPI specification. The parameters should follow their characteristics, for example regarding the default values. Some methods have a long list of (optional) parameters. This is easy to implement in languages that support named parameters such as R. For example, creating a job in R with a budget would lead to this method call: createJob ( process_graph = { ... }, budget = 123 ) Other languages that only support non-named parameters (i.e. the order of parameters is fixed) need to fill many parameters with default values, which is not convenient for a user. The example above in PHP would be: createJob({...}, null, null, null, null, null, 123) To avoid such method calls client developers should consider to pass either an instance of a class, which contains all parameters as member variables or the required parameters directly and the optional parameters as a dictionary (see example below). This basically emulates named parameters. The member variables / dictionary keys should use the same names as the parameters. The exemplary method call in PHP could be improved as follows: createJob({...}, [budget => 123])","title":"Parameters"},{"location":"guidelines-clients/#method-mappings","text":"Note: Some scopes for response JSON objects are still missing. We are open for proposals. Parameters with a leading ? are optional.","title":"Method mappings"},{"location":"guidelines-clients/#scope-openeo-root-category","text":"Description Client method Connect to a back-end, includes version discovery ( GET /.well-known/openeo ), requesting capabilities and authentication where required. Returns Connection . connect(url, ?authType, ?authOptions) Get client library version. clientVersion()","title":"Scope: openEO (root category)"},{"location":"guidelines-clients/#scope-connection-api-category","text":"Description API Request Client method Get the capabilities of the back-end. Returns Capabilities . GET / capabilities() List the supported output file formats. GET /file_formats listFileTypes() List the supported secondary service types. GET /service_types listServiceTypes() List the supported UDF runtimes. GET /udf_runtimes listUdfRuntimes() List all collections available on the back-end. GET /collections listCollections() Get information about a single collection. GET /collections/{collection_id} describeCollection(collection_id) List all processes available on the back-end. GET /processes listProcesses() Authenticate with OpenID Connect (if not specified in connect ). GET /credentials/oidc authenticateOIDC(?options) Authenticate with HTTP Basic (if not specified in connect ). GET /credentials/basic authenticateBasic(username, password) Logout / Close session for the authenticated user Depends on authentication method logout() Get information about the authenticated user. GET /me describeAccount() Lists all files from a user. Returns a list of File . GET /files listFiles() Opens a (existing or non-existing) file without reading any information. Returns a File . None openFile(path) Validates a process graph. POST /validation validateProcessGraph(processGraph) Lists all process graphs of the authenticated user. Returns a list of ProcessGraph . GET /process_graphs listProcessGraphs() Creates a new stored process graph. Returns a ProcessGraph . POST /process_graphs createProcessGraph(processGraph, ?title, ?description) Get all information about a stored process graph. Returns a ProcessGraph . GET /process_graphs/{process_graph_id} getProcessGraphById(id) Executes a process graph synchronously. POST /result computeResult(processGraph, ?plan, ?budget) Lists all jobs of the authenticated user. Returns a list of Job . GET /jobs listJobs() Creates a new job. Returns a Job . POST /jobs createJob(processGraph, ?title, ?description, ?plan, ?budget, ?additional) Get all information about a job. Returns a Job . GET /jobs/{job_id} getJobById(id) Lists all secondary services of the authenticated user. Returns a list of Service . GET /services listServices() Creates a new secondary service. Returns a Service . POST /services createService(processGraph, type, ?title, ?description, ?enabled, ?parameters, ?plan, ?budget) Get all information about a service. Returns a Service . GET /services/{service_id} getServiceById(id)","title":"Scope: Connection (API category)"},{"location":"guidelines-clients/#scope-capabilities-content-category","text":"Should be prefixed with Capabilities if collisions of names between different scope categories are to be expected. Description Field Client method Get the implemented openEO version. api_version apiVersion() Get the back-end version. backend_version backendVersion() Get the name of the back-end. title title() Get the description of the back-end. description description() List all supported features / endpoints. endpoints listFeatures() Check whether a feature / endpoint is supported. endpoints > ... hasFeature(methodName) Get the default billing currency. billing > currency currency() List all billing plans. billing > plans listPlans()","title":"Scope Capabilities (Content category)"},{"location":"guidelines-clients/#scope-file-api-category","text":"The File scope internally knows the user_id and the path . Description API Request Client method Download a user file. GET /files/{path} downloadFile(target) Upload a user file. PUT /files/{path} uploadFile(source) Delete a user file. DELETE /files/{path} deleteFile()","title":"Scope: File (API category)"},{"location":"guidelines-clients/#scope-job-api-category","text":"The Job scope internally knows the job_id . Description API Request Client method Get (and update on client-side) all job information. GET /jobs/{job_id} describeJob() Modify a job at the back-end. PATCH /jobs/{job_id} updateJob(?processGraph, ?title, ?description, ?plan, ?budget, ?additional) Delete a job DELETE /jobs/{job_id} deleteJob() Calculate an time/cost estimate for a job. GET /jobs/{job_id}/estimate estimateJob() Start / queue a job for processing. POST /jobs/{job_id}/results startJob() Stop / cancel job processing. DELETE /jobs/{job_id}/results stopJob() Get document with download links. GET /jobs/{job_id}/results listResults() Download job results. GET /jobs/{job_id}/results > ... downloadResults(target)","title":"Scope: Job (API category)"},{"location":"guidelines-clients/#scope-processgraph-api-category","text":"The ProcessGraph scope internally knows the process_graph_id . Description API Request Client method Get (and update on client-side) all information about a stored process graph. GET /process_graphs/{process_graph_id} describeProcessGraph() Modify a stored process graph at the back-end. PATCH /process_graphs/{process_graph_id} updateProcessGraph(?processGraph, ?title, ?description) Delete a stored process graph. DELETE /process_graphs/{process_graph_id} deleteProcessGraph()","title":"Scope: ProcessGraph (API category)"},{"location":"guidelines-clients/#scope-service-api-category","text":"The Service scope internally knows the service_id . Description API Request Client method Get (and update on client-side) all information about a secondary web service. GET /services/{service_id} describeService() Modify a secondary web service at the back-end. PATCH /services/{service_id} updateService(?processGraph, ?title, ?description, ?enabled, ?parameters, ?plan, ?budget) Delete a secondary web service. DELETE /services/{service_id} deleteService()","title":"Scope: Service (API category)"},{"location":"guidelines-clients/#processes","text":"The processes a back-end supports may be offered by the clients as methods in its own scope. The method names should follow the process names, but the conventions listed above can be applied here as well, e.g. converting filter_bands to filterBands . As parameters have no natural or technical ordering in the JSON objects, clients must come up with a reasonable ordering of parameters if required. This could be inspired by existing clients. The way of building a process graph from processes heavily depends on the technical capabilities of the programming language. Therefore it may differ between the client libraries. Follow the best practices of the programming language, e.g. support method chaining if possible.","title":"Processes"},{"location":"guidelines-clients/#workflow-example","text":"Some simplified example workflows using different programming styles are listed below. The following steps are executed: Loading the client library. Connecting to a back-end and authenticating with username and password via OpenID Connect. Requesting the capabilities and showing the implemented openEO version of the back-end. Showing information about the \"Sentinel-2A\" collection. Showing information about all processes supported by the back-end. Building a simple process graph. Creating a job. Pushing the job to the processing queue. After a while, showing the job details, e.g. checking the job status. Once processing is finished, downloading the job results to the local directory /tmp/job_results/ . Please note that the examples below do not comply to the latest process specification. They are meant to show the differences in client development, but are no working examples!","title":"Workflow example"},{"location":"guidelines-clients/#r-functional-style","text":"library ( openeo ) con = connect ( \"https://openeo.org\" , \"username\" , \"password\" ) cap = capabilities () cap %>% apiVersion () con %>% describeCollection ( \"Sentinel-2A\" ) con %>% listProcesses () processgraph = process ( \"load_collection\" , id = \"Sentinel-2A\" ) %>% process ( \"filter_bbox\" , west = 672000 , south = 5181000 , east = 652000 , north = 5161000 , crs = \"EPSG:32632\" ) %>% process ( \"filter_temporal\" , extent = c ( \"2017-01-01T00:00:00Z\" , \"2017-01-31T23:59:59Z\" )) %>% process ( \"ndvi\" , nir = \"B4\" , red = \"B8A\" ) %>% process ( \"min_time\" ) job = con %>% createJob ( processgraph ) job %>% startJob () job %>% describeJob () job %>% downloadResults ( \"/tmp/job_results/\" )","title":"R (functional style)"},{"location":"guidelines-clients/#python-mixed-style","text":"import openeo con = openeo . connect ( \"https://openeo.org\" , \"username\" , \"password\" ) cap = con . capabilities () print ( cap . api_version ()) print ( con . describe_collection ( \"Sentinel-2A\" )) print ( con . list_processes ()) processes = con . get_processes () pg = processes . load_collection ( id = \"Sentinel-2A\" ) pg = processes . filter_bbox ( pg , west = 672000 , south = 5181000 , east = 652000 , north = 5161000 , crs = \"EPSG:32632\" ) pg = processes . filter_temporal ( pg , extent = [ \"2017-01-01T00:00:00Z\" , \"2017-01-31T23:59:59Z\" ]) pg = processes . ndvi ( pg , nir = \"B4\" , red = \"B8A\" ) pg = processes . min_time ( pg ) job = con . create_job ( pg . graph ) job . start_job () print job . describe_job () job . download_results ( \"/tmp/job_results/\" )","title":"Python (mixed style)"},{"location":"guidelines-clients/#java-object-oriented-style","text":"import org.openeo.* ; OpenEO obj = new OpenEO (); Connection con = obj . connect ( \"https://openeo.org\" , \"username\" , \"password\" ); Capabilities cap = con . capabilities (); System . out . println ( cap . apiVersion ()); System . out . println ( con . describeCollection ( \"Sentinel-2A\" )); System . out . println ( con . listProcesses ()); ProcessGraphBuilder pgb = con . getProcessGraphBuilder () // Chain processes... ProcessGraph processGraph = pgb . buildProcessGraph (); Job job = con . createJob ( processGraph ); job . startJob (); System . out . println ( job . describeJob ()); job . downloadResults ( \"/tmp/job_results/\" );","title":"Java (object oriented style)"},{"location":"guidelines-clients/#php-procedural-style","text":"require_once(\"/path/to/openeo.php\"); $connection = openeo_connect(\"http://openeo.org\", \"username\", \"password\"); $capabilities = openeo_capabilities($connection); echo openeo_api_version($capabilites); echo openeo_describe_collection($connection, \"Sentinel-2A\"); echo openeo_list_processes($connection); $pg = openeo_process($pg, \"load_collection\", [\"id\" => \"Sentinel-2A\"]); $pg = openeo_process($pg, \"filter_bbox\", [\"west\" => 672000, \"south\" => 5181000, \"east\" => 652000, \"north\" => 5161000, \"crs\" => \"EPSG:32632\"]); $pg = openeo_process($pg, \"filter_temporal\", [\"extent\" => [\"2017-01-01T00:00:00Z\", \"2017-01-31T23:59:59Z\"]]); $pg = openeo_process($pg, \"ndvi\", [\"red\" => \"B4\", \"nir\" => \"B8A\"]); $pg = openeo_process($pg, \"min_time\"); $job = openeo_create_job($connection, $pg); openeo_start_job($job); echo openeo_describe_job($job); openeo_download_results($job, \"/tmp/job_results/\");","title":"PHP (procedural style)"},{"location":"guidelines-software/","text":"Software Development Guidelines \u00b6 This document describes guidelines for software developers, written for the openEO project. Since the openEO infrastructure will encompasses several programming languages and software environments, this document does not prescribe particular tools or platforms but rather focuses on general principles and methods behind them. License: all software developed in the openEO project and published on the openEO GitHub organisation shall be licensed under the Apache 2.0 license . If software repositories deviate from this, or contain code or other artifacts that deviates from this, this shall be described in the README.md file. Location: Official openEO software is developed under the openEO GitHub organisation . Proof-of-concept versus sustainable: each repository shall indicate its status: either proof-of-concept , or sustainable . Proof-of-concept code is meant to work but comes without quality assurance. Software repositories with proof-of-concept developments shall clearly say so in the first paragraph of the README.md file. Sustainable code should undergo standard quality checks , and point out its documentation . Sustainable code shall undergo code review ; no direct commits to master; any commit shall come in the form of a PR, commit after review. Sustainable code shall be written in a Test-driven manner , and repositories shall at the top of their README.md give indication of the degree to which code is covered by tests. Continuous integration shall be used to indicate code currently passes its test on CI platforms. A Code of conduct describes the rules and constraints to developers and contributors. Version numbers of sustainable software releases shall follow Semantic Versioning 2.0.0 . Software quality guidelines \u00b6 software shall be written in such a way that another person can understand its intention comment lines shall be used sparsely, but effectively reuse of unstable or esoteric libraries shall be avoided Software documentation guidelines \u00b6 Software documentation shall include: * installation instructions * usage instructions * explain in detail the intention of the software * pointers to reference documents explaining overarching concepts Each repository's README.md shall point to the documentation. Reference documentation shall be written using well-defined reference documentation language, such as RFC2119 or arc42 , and refer to the definitions used. Software review \u00b6 sustainable software development shall take place by always having two persons involved in a change to the master branch: individuals push to branches, pull request indicate readiness to be taken up in the master branch, a second developer reviews the pull request before merging it into the master branch. software review discussions shall be intelligible for external developers, and serve as implicit documentation of development decisions taken Test-driven development \u00b6 Software shall be developed in a test-driven fashion, meaning that while the code is written, tests are developed that verify, to a reasonable extent, the correctness of the code. Tools such as codecov.io to automatically indicate the amount of code covered by tests, and code that is not covered by tests shall be used in combination with a continuous integration framework. Continuous integration \u00b6 Repositories containing running software shall use an appropriate continuous integration platform, such as Travis CI or similar, to show whether the current build passes all checks. This helps understand contributors that the software passes tests on an independent platform, and may give insights in the way the software is compiled, deployed and tested. Additional guidelines \u00b6 There is specific guideline for client library development .","title":"Software Development"},{"location":"guidelines-software/#software-development-guidelines","text":"This document describes guidelines for software developers, written for the openEO project. Since the openEO infrastructure will encompasses several programming languages and software environments, this document does not prescribe particular tools or platforms but rather focuses on general principles and methods behind them. License: all software developed in the openEO project and published on the openEO GitHub organisation shall be licensed under the Apache 2.0 license . If software repositories deviate from this, or contain code or other artifacts that deviates from this, this shall be described in the README.md file. Location: Official openEO software is developed under the openEO GitHub organisation . Proof-of-concept versus sustainable: each repository shall indicate its status: either proof-of-concept , or sustainable . Proof-of-concept code is meant to work but comes without quality assurance. Software repositories with proof-of-concept developments shall clearly say so in the first paragraph of the README.md file. Sustainable code should undergo standard quality checks , and point out its documentation . Sustainable code shall undergo code review ; no direct commits to master; any commit shall come in the form of a PR, commit after review. Sustainable code shall be written in a Test-driven manner , and repositories shall at the top of their README.md give indication of the degree to which code is covered by tests. Continuous integration shall be used to indicate code currently passes its test on CI platforms. A Code of conduct describes the rules and constraints to developers and contributors. Version numbers of sustainable software releases shall follow Semantic Versioning 2.0.0 .","title":"Software Development Guidelines"},{"location":"guidelines-software/#software-quality-guidelines","text":"software shall be written in such a way that another person can understand its intention comment lines shall be used sparsely, but effectively reuse of unstable or esoteric libraries shall be avoided","title":"Software quality guidelines"},{"location":"guidelines-software/#software-documentation-guidelines","text":"Software documentation shall include: * installation instructions * usage instructions * explain in detail the intention of the software * pointers to reference documents explaining overarching concepts Each repository's README.md shall point to the documentation. Reference documentation shall be written using well-defined reference documentation language, such as RFC2119 or arc42 , and refer to the definitions used.","title":"Software documentation guidelines"},{"location":"guidelines-software/#software-review","text":"sustainable software development shall take place by always having two persons involved in a change to the master branch: individuals push to branches, pull request indicate readiness to be taken up in the master branch, a second developer reviews the pull request before merging it into the master branch. software review discussions shall be intelligible for external developers, and serve as implicit documentation of development decisions taken","title":"Software review"},{"location":"guidelines-software/#test-driven-development","text":"Software shall be developed in a test-driven fashion, meaning that while the code is written, tests are developed that verify, to a reasonable extent, the correctness of the code. Tools such as codecov.io to automatically indicate the amount of code covered by tests, and code that is not covered by tests shall be used in combination with a continuous integration framework.","title":"Test-driven development"},{"location":"guidelines-software/#continuous-integration","text":"Repositories containing running software shall use an appropriate continuous integration platform, such as Travis CI or similar, to show whether the current build passes all checks. This helps understand contributors that the software passes tests on an independent platform, and may give insights in the way the software is compiled, deployed and tested.","title":"Continuous integration"},{"location":"guidelines-software/#additional-guidelines","text":"There is specific guideline for client library development .","title":"Additional guidelines"},{"location":"processreference/","text":"Placeholder for generated process specifications.","title":"Process Reference"},{"location":"udfs/","text":"User-defined functions \u00b6 The abbreviation UDF stands for user-defined function . With this concept, users are able to upload custom code and have it executed e.g. for every pixel of a scene, allowing custom calculations on server-side data. UDFs are currently developed and evaluated outside of the core API. More information regarding the current draft for UDFs can be found in a separate repository . There is additional documentation available for the UDF Framework and the UDF API .","title":"UDFs"},{"location":"udfs/#user-defined-functions","text":"The abbreviation UDF stands for user-defined function . With this concept, users are able to upload custom code and have it executed e.g. for every pixel of a scene, allowing custom calculations on server-side data. UDFs are currently developed and evaluated outside of the core API. More information regarding the current draft for UDFs can be found in a separate repository . There is additional documentation available for the UDF Framework and the UDF API .","title":"User-defined functions"}]}