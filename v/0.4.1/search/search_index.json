{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"openEO - Concepts and API Reference \u00b6 Note: The specification is currently still an early version, with the potential for some major things to change. The core is now fleshed out, so everybody is encouraged to try it out and give feedback (for example by adding issues ). But the goal is to actually be able to act on that feedback, which will mean changes are quite possible. openEO develops an open application programming interface (API) that connects clients like R, Python and JavaScript to big Earth observation cloud back-ends in a simple and unified way. The acronym openEO contracts two concepts: open : used here in the context of open source software; open source software is available in source code form, and can be freely modified and redistributed; the openEO project will create open source software, reusable under a liberal open source license (Apache 2.0) EO : Earth observation Jointly, the openEO targets the processing and analysis of Earth observation data. The main objectives of the project are the following concepts: Simplicity : nowadays, many end-users use Python or R to analyse data and JavaScript to develop web applications; analysing large amounts of EO imagery should be equally simple, and seamlessly integrate with existing workflows Unification : current EO cloud back-ends all have a different API , making EO data analysis hard to validate and reproduce and back-ends difficult to compare in terms of capability and costs, or to combine them in a joint analysis across back-ends. A unified API can resolve many of these problems. The following pages introduce the core concepts of the project. Make sure to introduce yourself to the major technical terms used in the openEO project by reading the glossary . The openEO API defines a HTTP API that lets cloud back-ends with large Earth observation datasets communicate with front end analysis applications in an interoperable way. This documentation describes important API concepts and design decisions and gives a complete API reference documentation . As an overview, the openEO API specifies how to discover which Earth observation data and processes are available at cloud back-ends, execute (chained) processes on back-ends, run user-defined functions (UDFs) on back-ends where UDFs can be exposed to the data in different ways, download (intermediate) results, and manage user content including billing . The API is defined as an OpenAPI 3.0 JSON file. openEO , A Common, Open Source Interface between Earth Observation Data Infrastructures and Front-End Applications is a H2020 project funded under call EO-2-2017: EO Big Data Shift, under proposal number 776242. It will run from Oct 2017 to Sept 2020. This project has received funding from the European Union\u2019s Horizon 2020 research and innovation programme under grant agreement No 776242. The contents of this website reflects only the authors\u2019 view; the European Commission is not responsible for any use that may be made of the information it provides.","title":"Introduction"},{"location":"#openeo-concepts-and-api-reference","text":"Note: The specification is currently still an early version, with the potential for some major things to change. The core is now fleshed out, so everybody is encouraged to try it out and give feedback (for example by adding issues ). But the goal is to actually be able to act on that feedback, which will mean changes are quite possible. openEO develops an open application programming interface (API) that connects clients like R, Python and JavaScript to big Earth observation cloud back-ends in a simple and unified way. The acronym openEO contracts two concepts: open : used here in the context of open source software; open source software is available in source code form, and can be freely modified and redistributed; the openEO project will create open source software, reusable under a liberal open source license (Apache 2.0) EO : Earth observation Jointly, the openEO targets the processing and analysis of Earth observation data. The main objectives of the project are the following concepts: Simplicity : nowadays, many end-users use Python or R to analyse data and JavaScript to develop web applications; analysing large amounts of EO imagery should be equally simple, and seamlessly integrate with existing workflows Unification : current EO cloud back-ends all have a different API , making EO data analysis hard to validate and reproduce and back-ends difficult to compare in terms of capability and costs, or to combine them in a joint analysis across back-ends. A unified API can resolve many of these problems. The following pages introduce the core concepts of the project. Make sure to introduce yourself to the major technical terms used in the openEO project by reading the glossary . The openEO API defines a HTTP API that lets cloud back-ends with large Earth observation datasets communicate with front end analysis applications in an interoperable way. This documentation describes important API concepts and design decisions and gives a complete API reference documentation . As an overview, the openEO API specifies how to discover which Earth observation data and processes are available at cloud back-ends, execute (chained) processes on back-ends, run user-defined functions (UDFs) on back-ends where UDFs can be exposed to the data in different ways, download (intermediate) results, and manage user content including billing . The API is defined as an OpenAPI 3.0 JSON file. openEO , A Common, Open Source Interface between Earth Observation Data Infrastructures and Front-End Applications is a H2020 project funded under call EO-2-2017: EO Big Data Shift, under proposal number 776242. It will run from Oct 2017 to Sept 2020. This project has received funding from the European Union\u2019s Horizon 2020 research and innovation programme under grant agreement No 776242. The contents of this website reflects only the authors\u2019 view; the European Commission is not responsible for any use that may be made of the information it provides.","title":"openEO - Concepts and API Reference"},{"location":"apireference-subscriptions/","text":"openEO API for Subscriptions 0.4.0 documentation \u00b6 The openEO API specification for interoperable cloud-based processing of large Earth observation datasets. This is a subset of the openEO API that handles WebSocket-based protocols for subscriptions and notifications. openeo.authorize , openeo.welcome , openeo.subscribe and openeo.unsubsribe MUST be implemeneted by all back-ends. Security considerations: A handshake has to be performed directly after establishing the WebSocket connection. The client MUST send a openeo.authorize request and receives a openeo.welcome message after a successful authorization. The WebSocket connections MUST be closed by servers once a request with invalid authorization credentials is sent. Servers are allowed to close connections to clients that have not sent a openeo.authorize request 30 seconds after establishing a WebSocket connection. Table of Contents \u00b6 Topics Schemas Topics \u00b6 subscribe openeo.authorize \u00b6 Message Starts a handshake with the server to authorize the client. The client MUST send a openeo.authorize request directly after establishing the WebSocket connection and a openeo.welcome will be sent by the server after successful authorization. The WebSocket connections MUST be closed if invalid authorization credentials are sent. Payload Name Type Description Accepted values authorization (required) string Takes the same values as the HTTP Authorization header that is accepted by most openEO endpoints. The value is concatenated from the Authorization scheme (usually Bearer ), a space and the actual token for authorization. Any message (required) object Any message.issued (required) string Date and time when the message was sent, formatted as a RFC 3339 date-time. Any message.topic (required) string Message type Any Example { \"authorization\" : \"Bearer eyJhbGciOiJIUzI1NiJ9.e30.4E_Bsx-pJi3kOW9wVXN8CgbATwP09D9V5gxh9-9zSZ0\" , \"message\" : { \"issued\" : \"2018-08-07T14:06:36Z\" , \"topic\" : \"openeo.authorize\" } } publish openeo.welcome \u00b6 Message Welcome message for clients. Sends the supported topics, excluding openeo.authorize , openeo.subscribe , openeo.unsubscribe and openeo.welcome (because these MUST be implemented by every back-end anyway). This message MUST be sent by all servers directly after receiving the openeo.authorize message with valid credentials. Payload Name Type Description Accepted values message (required) object Any message.issued (required) string Date and time when the message was sent, formatted as a RFC 3339 date-time. Any message.topic (required) string Message type Any payload (required) object Any payload.topics (required) array(string) Any Example { \"message\" : { \"issued\" : \"2018-08-07T14:06:36Z\" , \"topic\" : \"openeo.welcome\" }, \"payload\" : { \"topics\" : [ \"openeo.jobs.output\" , \"openeo.jobs.status\" , \"openeo.data\" ] } } subscribe openeo.subscribe \u00b6 Message Subscribes to certain topics. Additional parameters that may be used to restrict the scope of the subscription are described in the specific messages. For example, a restriction of a subscription to a specific job. The WebSocket connections MUST be closed if invalid authorization credentials are sent. Payload Name Type Description Accepted values authorization (required) string Takes the same values as the HTTP Authorization header that is accepted by most openEO endpoints. The value is concatenated from the Authorization scheme (usually Bearer ), a space and the actual token for authorization. Any message (required) object Any message.issued (required) string Date and time when the message was sent, formatted as a RFC 3339 date-time. Any message.topic (required) string Message type Any payload (required) object Any payload.topics (required) array(object) A list of topics to (un)subscribe to/from. Any payload.topics.topic (required) string Any Example { \"authorization\" : \"Bearer eyJhbGciOiJIUzI1NiJ9.e30.4E_Bsx-pJi3kOW9wVXN8CgbATwP09D9V5gxh9-9zSZ0\" , \"message\" : { \"issued\" : \"2018-08-07T14:06:36Z\" , \"topic\" : \"openeo.subscribe\" }, \"payload\" : { \"topics\" : [ { \"topic\" : \"openeo.jobs.status\" , \"job_id\" : \"a3cca2b2aa1e3b5b\" }, { \"topic\" : \"openeo.files\" } ] } } subscribe openeo.unsubscribe \u00b6 Message Unsubscribes from certain topics. The WebSocket connections MUST be closed if invalid authorization credentials are sent. Payload Name Type Description Accepted values authorization (required) string Takes the same values as the HTTP Authorization header that is accepted by most openEO endpoints. The value is concatenated from the Authorization scheme (usually Bearer ), a space and the actual token for authorization. Any message (required) object Any message.issued (required) string Date and time when the message was sent, formatted as a RFC 3339 date-time. Any message.topic (required) string Message type Any payload (required) object Any payload.topics (required) array(object) A list of topics to (un)subscribe to/from. Any payload.topics.topic (required) string Any Example { \"authorization\" : \"Bearer eyJhbGciOiJIUzI1NiJ9.e30.4E_Bsx-pJi3kOW9wVXN8CgbATwP09D9V5gxh9-9zSZ0\" , \"message\" : { \"issued\" : \"2018-08-07T14:06:36Z\" , \"topic\" : \"openeo.unsubscribe\" }, \"payload\" : { \"topics\" : [ { \"topic\" : \"openeo.jobs.status\" , \"job_id\" : \"a3cca2b2aa1e3b5b\" }, { \"topic\" : \"openeo.files\" } ] } } publish openeo.jobs.output \u00b6 Message Data written to the output console with processes of a job. Subscriptions to this message can be restricted to a certain job by specifying a job_id . Payload Name Type Description Accepted values message (required) object Any message.issued (required) string Date and time when the message was sent, formatted as a RFC 3339 date-time. Any message.topic (required) string Message type Any payload (required) object Any payload.job_id (required) string Unique identifier of a job that is generated by the back-end during job submission. Any payload.output (required) Output data of any type Any Example { \"message\" : { \"issued\" : \"2018-08-07T14:06:36Z\" , \"topic\" : \"openeo.jobs.debug\" }, \"payload\" : { \"job_id\" : \"a3cca2b2aa1e3b5b\" , \"output\" : \"Hello world!\" } } publish openeo.jobs.debug \u00b6 Message Debugging information from job execution. Subscriptions to this message can be restricted to a certain job by specifying a job_id . Payload Name Type Description Accepted values message (required) object Any message.issued (required) string Date and time when the message was sent, formatted as a RFC 3339 date-time. Any message.topic (required) string Message type Any payload (required) object Any payload.job_id (required) string Unique identifier of a job that is generated by the back-end during job submission. Any payload.message (required) string The thrown debug message Any payload.process object Process throwing the debug message. Any payload.process.name (required) string Name of the process. Any payload.process.parameters (required) object Key-value pairs for the parameters of the process. The keys are the parameter names and the values are the actual values specified for the parameter. Specify the empty object if the process does not have any parameters or was called without parameters. Any Example { \"message\" : { \"issued\" : \"2018-08-07T14:06:36Z\" , \"topic\" : \"openeo.jobs.debug\" }, \"payload\" : { \"job_id\" : \"a3cca2b2aa1e3b5b\" , \"message\" : \"Invalid CRS specified, defaulting to EPSG:4326.\" , \"process\" : { \"name\" : \"filter_bbox\" , \"parameters\" : { \"crs\" : 9999 , \"west\" : 55 , \"south\" : 10 , \"east\" : 56 , \"north\" : 11 } } } } publish openeo.jobs.status \u00b6 Message Inform about a status change of a job. Subscriptions to this message can be restricted to a certain job by specifying a job_id . Payload Name Type Description Accepted values message (required) object Any message.issued (required) string Date and time when the message was sent, formatted as a RFC 3339 date-time. Any message.topic (required) string Message type Any payload (required) object Any payload.job_id (required) string Unique identifier of a job that is generated by the back-end during job submission. Any payload.status (required) string Current status submitted , queued , running , canceled , finished , error payload.progress number Progress of a running job in percent. Any Example { \"message\" : { \"issued\" : \"2018-08-07T14:06:36Z\" , \"topic\" : \"openeo.jobs.status\" }, \"payload\" : { \"job_id\" : \"a3cca2b2aa1e3b5b\" , \"status\" : \"running\" , \"progress\" : 75.5 } } publish openeo.files \u00b6 Message Inform about changes regarding the user files. Subscriptions to this message can't be restricted to a certain file or folder. Payload Name Type Description Accepted values message (required) object Any message.issued (required) string Date and time when the message was sent, formatted as a RFC 3339 date-time. Any message.topic (required) string Message type Any payload (required) object Any payload.user_id (required) string Unique identifier of the user. Any payload.path (required) string Path of the file, relative to the user's root directory. MUST NOT start with a slash and MUST NOT be url-encoded. Any payload.action (required) string Describes what has changed. created , updated , deleted Example { \"message\" : { \"issued\" : \"2018-08-07T14:06:36Z\" , \"topic\" : \"openeo.files\" }, \"payload\" : { \"user_id\" : \"john_doe\" , \"path\" : \"new_file.txt\" , \"action\" : \"created\" } } publish openeo.data \u00b6 Message Inform about changes regarding an EO dataset. At least one of the temporal_extent and spatial_extent fields MUST be specified. Subscriptions to this message can be restricted to a certain collection by specifying name and providing a valid collection name. Payload Name Type Description Accepted values message (required) object Any message.issued (required) string Date and time when the message was sent, formatted as a RFC 3339 date-time. Any message.topic (required) string Message type Any payload (required) object Any payload.name (required) string Unique identifier for EO datasets. Any payload.temporal_extent array(string) MUST be specified if the temporal extent of the dataset has changed. The temporal extent is always specified as an array, that consists of either a single timestamp or a start and an end time, each element formatted as a RFC 3339 date-time. Specifies the temporal extent of the data that has changed, not of the whole dataset. Example: The dataset covers images from beginning of 2015 until the end of 2018. A single image has been added, captured at the first day in 2019 at 01:00:00 UTC (1am). The spatial extent specified here must be an array containing a single string: 2019-01-01T01:00:00Z . Any payload.spatial_extent object MUST always be specified. It is the spatial extent of the data that was changed. Specifies the spatial extent of the data that has changed, not of the whole dataset. Example: The dataset covers the whole world and an image of Austria has been added. The spatial extent specified here must be the bounding box of Austria. Any payload.spatial_extent.crs oneOf Coordinate reference system specified as EPSG code or PROJ definition. Defaults to 4326 (EPSG code 4326) unless the client explicitly requests a different coordinate reference system. Any payload.spatial_extent.crs.0 integer Any payload.spatial_extent.crs.1 string Any payload.spatial_extent.west (required) number West (lower left corner, coordinate axis 1). Any payload.spatial_extent.south (required) number South (lower left corner, coordinate axis 2). Any payload.spatial_extent.east (required) number East (upper right corner, coordinate axis 1). Any payload.spatial_extent.north (required) number North (upper right corner, coordinate axis 2). Any payload.spatial_extent.base number Base (optional, lower left corner, coordinate axis 3). Any payload.spatial_extent.height number Height (optional, upper right corner, coordinate axis 3). Any Example of payload (generated) { \"message\" : { \"issued\" : \"2019-03-11T12:54:41Z\" , \"topic\" : \"openeo.sample\" }, \"payload\" : { \"name\" : \"MOD18Q1\" , \"temporal_extent\" : [ \"2016-01-01T02:30:00Z\" , \"2016-01-01T04:45:00Z\" ], \"spatial_extent\" : { \"crs\" : 7099 , \"west\" : 0 , \"south\" : 0 , \"east\" : 0 , \"north\" : 0 , \"base\" : 0 , \"height\" : 0 } } } Schemas \u00b6 authorization Name Type Description Accepted values authorization string Any Example \"Bearer eyJhbGciOiJIUzI1NiJ9.e30.4E_Bsx-pJi3kOW9wVXN8CgbATwP09D9V5gxh9-9zSZ0\" message Name Type Description Accepted values issued (required) string Date and time when the message was sent, formatted as a RFC 3339 date-time. Any topic (required) string Message type Any Example (generated) { \"issued\" : \"2019-03-11T12:54:41Z\" , \"topic\" : \"openeo.sample\" } collection_name Name Type Description Accepted values collection_name string Any Example \"MOD18Q1\" job_id Name Type Description Accepted values job_id string Any Example \"a3cca2b2aa1e3b5b\" user_id Name Type Description Accepted values user_id string Any Example \"john_doe\" topics Name Type Description Accepted values topics array(object) Any topics.topic (required) string Any Example [ { \"topic\" : \"openeo.jobs.output\" , \"job_id\" : 123 }, { \"topic\" : \"openeo.jobs.status\" }, { \"topic\" : \"openeo.data\" , \"name\" : \"MOD18Q1\" } ]","title":"Subscriptions API Reference"},{"location":"apireference-subscriptions/#openeo-api-for-subscriptions-040-documentation","text":"The openEO API specification for interoperable cloud-based processing of large Earth observation datasets. This is a subset of the openEO API that handles WebSocket-based protocols for subscriptions and notifications. openeo.authorize , openeo.welcome , openeo.subscribe and openeo.unsubsribe MUST be implemeneted by all back-ends. Security considerations: A handshake has to be performed directly after establishing the WebSocket connection. The client MUST send a openeo.authorize request and receives a openeo.welcome message after a successful authorization. The WebSocket connections MUST be closed by servers once a request with invalid authorization credentials is sent. Servers are allowed to close connections to clients that have not sent a openeo.authorize request 30 seconds after establishing a WebSocket connection.","title":"openEO API for Subscriptions 0.4.0 documentation"},{"location":"apireference-subscriptions/#table-of-contents","text":"Topics Schemas","title":"Table of Contents"},{"location":"apireference-subscriptions/#topics","text":"","title":"Topics"},{"location":"apireference-subscriptions/#subscribe-openeoauthorize","text":"","title":"subscribe openeo.authorize"},{"location":"apireference-subscriptions/#publish-openeowelcome","text":"","title":"publish openeo.welcome"},{"location":"apireference-subscriptions/#subscribe-openeosubscribe","text":"","title":"subscribe openeo.subscribe"},{"location":"apireference-subscriptions/#subscribe-openeounsubscribe","text":"","title":"subscribe openeo.unsubscribe"},{"location":"apireference-subscriptions/#publish-openeojobsoutput","text":"","title":"publish openeo.jobs.output"},{"location":"apireference-subscriptions/#publish-openeojobsdebug","text":"","title":"publish openeo.jobs.debug"},{"location":"apireference-subscriptions/#publish-openeojobsstatus","text":"","title":"publish openeo.jobs.status"},{"location":"apireference-subscriptions/#publish-openeofiles","text":"","title":"publish openeo.files"},{"location":"apireference-subscriptions/#publish-openeodata","text":"","title":"publish openeo.data"},{"location":"apireference-subscriptions/#schemas","text":"","title":"Schemas"},{"location":"apireference/","text":"Placeholder for generated API specification.","title":"Core API Reference"},{"location":"arch/","text":"Architecture \u00b6 The openEO API defines a language how clients communicate to back-ends in order to analyze large Earth observation datasets. The API will be implemented by drivers for specific back-ends. Some first architecture considerations are listed below. The openEO API is a contract between clients and back-ends that describes the communication only Each back-end runs its own API instance including the specific back-end driver. There is no API instance that runs more than one driver. Clients in R, Python, and JavaScript connect directly to the back-ends and communicate with the back-ends over HTTPS according to the openEO API specification. API instances can run on back-end servers or additional intermediate layers, which then communicate to back-ends in a back-end specific way. Back-ends may add functionality and extend the API wherever there is need. There will be a central back-end registry service (openEO Hub), to allow users to search for back-ends with specific functionality and or data. The openEO API may define profiles in order to group specific functionality. Figure: Architecture Microservices \u00b6 To simplify and structure the development, the API is divided into a few microservices. Microservice Description Capabilities This microservice reports on the capabilities of the back-end, i.e. which API endpoints are implemented, which authentication methods are supported, and whether and how UDFs can be executed at the back-end. EO Data Discovery Describes which collections are available at the back-end. Process Discovery Provides services to find out which processes a back-end provides, i.e., what users can do with the available data. UDF Discovery and execution of user-defined functions. Batch Job Management Organizes and manages batch jobs that run processes on back-ends. File Management Organizes and manages user-uploaded files. Process Graph Management Organizes and manages user-defined process graphs. Secondary Services Management External web services to access data and job results such as a OGC WMTS service. Account Management User management, accounting and authentication.","title":"Architecture"},{"location":"arch/#architecture","text":"The openEO API defines a language how clients communicate to back-ends in order to analyze large Earth observation datasets. The API will be implemented by drivers for specific back-ends. Some first architecture considerations are listed below. The openEO API is a contract between clients and back-ends that describes the communication only Each back-end runs its own API instance including the specific back-end driver. There is no API instance that runs more than one driver. Clients in R, Python, and JavaScript connect directly to the back-ends and communicate with the back-ends over HTTPS according to the openEO API specification. API instances can run on back-end servers or additional intermediate layers, which then communicate to back-ends in a back-end specific way. Back-ends may add functionality and extend the API wherever there is need. There will be a central back-end registry service (openEO Hub), to allow users to search for back-ends with specific functionality and or data. The openEO API may define profiles in order to group specific functionality. Figure: Architecture","title":"Architecture"},{"location":"arch/#microservices","text":"To simplify and structure the development, the API is divided into a few microservices. Microservice Description Capabilities This microservice reports on the capabilities of the back-end, i.e. which API endpoints are implemented, which authentication methods are supported, and whether and how UDFs can be executed at the back-end. EO Data Discovery Describes which collections are available at the back-end. Process Discovery Provides services to find out which processes a back-end provides, i.e., what users can do with the available data. UDF Discovery and execution of user-defined functions. Batch Job Management Organizes and manages batch jobs that run processes on back-ends. File Management Organizes and manages user-uploaded files. Process Graph Management Organizes and manages user-defined process graphs. Secondary Services Management External web services to access data and job results such as a OGC WMTS service. Account Management User management, accounting and authentication.","title":"Microservices"},{"location":"changelog/","text":"Changelog \u00b6 All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning . [0.4.0] - 2019-03-07 \u00b6 Added \u00b6 GET /jobs/{job_id}/estimate can return the estimated required storage capacity. #122 GET /jobs/{job_id} has two new properties: progress indicates the batch job progress when running. #82 error states the error message when a job errored out. GET /jobs/{job_id}/result mirrors this error message in a response with HTTP status code 424. #165 GET /.well-known/openeo allows clients to choose between versions. #148 GET / (Capabilities): Requires to return a title ( title ), a description ( description ) and the back-end version ( backend_version ). #154 Billing plans have an additional required property paid . #157 Should provide a link to the Well-Known URI ( /.well-known/openeo ) in the new links property. GET /processes (Process discovery): Processes can be categorizes with the category property. Parameters can be ordered with the parameter_order property instead of having a random order. Support for references to other processes in descriptions. Processes and parameters can be declared to be experimental . GET /output_formats and GET /service_types can now provide links per entry. GET /udf_runtimes provide a list of UDF runtime environments. #87 GET /service_types allows to specify variables that can be used in process graphs. #172 Changed \u00b6 Completely new version of the processes. Changed process graph to a flexible graph-like structure, which also allows callbacks. #160 Updated GET /collections and GET /collections/{collection_id} to follow STAC v0.6.2. #158 , #173 The process_graph_id of stored process graphs, the service_id of services and the job_id of jobs has changed to id in responses. #130 The status property for jobs is now required. POST /preview renamed to POST /result . #162 GET / (Capabilities): version in the response was renamed to api_version . Endpoint paths must follow the openAPI specification. #128 Billing plan descriptions allow CommonMark. #164 /files/{user_id}/{path} File management: Clarified handling of folders. #146 GET method: The name property was renamed to path . #133 PUT method: Returns file meta data with a different response code. #163 GET /processes (Process discovery): The name property of processes has changed to id . #130 mime_type replaced with media_type in the input parameters and return values. The schema for exceptions follows the general schema for openEO errors. #139 Changed the structure of examples . POST /validation (Process graph validation): Returns HTTP status code 200 for valid and invalid process graphs and responds with a list of errors. [#144] Allowed to call the endpoint without authentication. #151 Behavior for DELETE /jobs/{job_id}/results and POST /jobs/{job_id}/results specified depending on the job status. Clarified status changes in general. #142 Improved client development guidelines. #124 , #138 Removed \u00b6 Numeric openEO error codes. Replaced in responses with textual error codes. #139 Query parameters to replace process graph variables in GET /process_graphs/{process_graph_id} . #147 min_parameters and dependencies for parameters in process descriptions returned by GET /processes . Replaced output format properties in favor of a save_result process, which has resulted in in the removal of: The default output format in GET /output_formats . #153 The output format properties in POST /result (fka POST /preview ), POST /jobs , PATCH /jobs and GET /jobs/{job_id} requests. #153 gis_data_type (not to be confused with gis_data_types ) in the parameters of output formats in GET /output_formats Fixed \u00b6 Added missing Access-Control-Expose-Headers header to required CORS headers. Some endpoints didn't include authentication information. GET /jobs/{job_id}/estimate : Property downloads_included had a wrong default value. [0.3.1] - 2018-11-06 \u00b6 Added \u00b6 createProcessGraph method to client development guidelines. JSON file with all specified errors. Textual error codes for each specified error. Allow setting a plan for POST /preview Default billing plan in GET / . #141 Job ID in JSON response for GET /jobs/{job_id}/results . Changed \u00b6 Several optional fields such as output , title and description are now nullable instead of requiring to omit them. The output format is not required in POST /preview any more and thus allows falling back to the default. The output_format parameter in createJob and execute in client development guidelines. The extent parameters in filter_bbox and filter_daterange are formally required now. Deprecated \u00b6 Numeric openEO error codes are soon to be replaced with textual error codes. eo:resolution in collection bands is a duplicate of eo:gsd . Use eo:gsd instead. Fixed \u00b6 Fixed a wrong definition of the header OpenEO-Costs in POST /preview . Fixed typo in method authenticateOIDC in client development guidelines. Fixed the definition of spatial extents by swapping north and south. Replaced the outdated occurrences of srs with crs in spatial extents. Added missing required descriptions to process definitions. Added missing error messages. Fixed unclear specification for arrays used as process graph arguments. Fixed inconsist schema of openEO error responses: Field is now consistently named message instead of description . [0.3.0] - 2018-09-21 \u00b6 First version after proof of concept tackling many major issues. No changelog available. [0.0.2] - 2018-03-22 \u00b6 Version for proof of concept. No changelog available. [0.0.1] - 2018-02-07 \u00b6 Initial version.","title":"Changelog"},{"location":"changelog/#changelog","text":"All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning .","title":"Changelog"},{"location":"changelog/#040-2019-03-07","text":"","title":"[0.4.0] - 2019-03-07"},{"location":"changelog/#added","text":"GET /jobs/{job_id}/estimate can return the estimated required storage capacity. #122 GET /jobs/{job_id} has two new properties: progress indicates the batch job progress when running. #82 error states the error message when a job errored out. GET /jobs/{job_id}/result mirrors this error message in a response with HTTP status code 424. #165 GET /.well-known/openeo allows clients to choose between versions. #148 GET / (Capabilities): Requires to return a title ( title ), a description ( description ) and the back-end version ( backend_version ). #154 Billing plans have an additional required property paid . #157 Should provide a link to the Well-Known URI ( /.well-known/openeo ) in the new links property. GET /processes (Process discovery): Processes can be categorizes with the category property. Parameters can be ordered with the parameter_order property instead of having a random order. Support for references to other processes in descriptions. Processes and parameters can be declared to be experimental . GET /output_formats and GET /service_types can now provide links per entry. GET /udf_runtimes provide a list of UDF runtime environments. #87 GET /service_types allows to specify variables that can be used in process graphs. #172","title":"Added"},{"location":"changelog/#changed","text":"Completely new version of the processes. Changed process graph to a flexible graph-like structure, which also allows callbacks. #160 Updated GET /collections and GET /collections/{collection_id} to follow STAC v0.6.2. #158 , #173 The process_graph_id of stored process graphs, the service_id of services and the job_id of jobs has changed to id in responses. #130 The status property for jobs is now required. POST /preview renamed to POST /result . #162 GET / (Capabilities): version in the response was renamed to api_version . Endpoint paths must follow the openAPI specification. #128 Billing plan descriptions allow CommonMark. #164 /files/{user_id}/{path} File management: Clarified handling of folders. #146 GET method: The name property was renamed to path . #133 PUT method: Returns file meta data with a different response code. #163 GET /processes (Process discovery): The name property of processes has changed to id . #130 mime_type replaced with media_type in the input parameters and return values. The schema for exceptions follows the general schema for openEO errors. #139 Changed the structure of examples . POST /validation (Process graph validation): Returns HTTP status code 200 for valid and invalid process graphs and responds with a list of errors. [#144] Allowed to call the endpoint without authentication. #151 Behavior for DELETE /jobs/{job_id}/results and POST /jobs/{job_id}/results specified depending on the job status. Clarified status changes in general. #142 Improved client development guidelines. #124 , #138","title":"Changed"},{"location":"changelog/#removed","text":"Numeric openEO error codes. Replaced in responses with textual error codes. #139 Query parameters to replace process graph variables in GET /process_graphs/{process_graph_id} . #147 min_parameters and dependencies for parameters in process descriptions returned by GET /processes . Replaced output format properties in favor of a save_result process, which has resulted in in the removal of: The default output format in GET /output_formats . #153 The output format properties in POST /result (fka POST /preview ), POST /jobs , PATCH /jobs and GET /jobs/{job_id} requests. #153 gis_data_type (not to be confused with gis_data_types ) in the parameters of output formats in GET /output_formats","title":"Removed"},{"location":"changelog/#fixed","text":"Added missing Access-Control-Expose-Headers header to required CORS headers. Some endpoints didn't include authentication information. GET /jobs/{job_id}/estimate : Property downloads_included had a wrong default value.","title":"Fixed"},{"location":"changelog/#031-2018-11-06","text":"","title":"[0.3.1] - 2018-11-06"},{"location":"changelog/#added_1","text":"createProcessGraph method to client development guidelines. JSON file with all specified errors. Textual error codes for each specified error. Allow setting a plan for POST /preview Default billing plan in GET / . #141 Job ID in JSON response for GET /jobs/{job_id}/results .","title":"Added"},{"location":"changelog/#changed_1","text":"Several optional fields such as output , title and description are now nullable instead of requiring to omit them. The output format is not required in POST /preview any more and thus allows falling back to the default. The output_format parameter in createJob and execute in client development guidelines. The extent parameters in filter_bbox and filter_daterange are formally required now.","title":"Changed"},{"location":"changelog/#deprecated","text":"Numeric openEO error codes are soon to be replaced with textual error codes. eo:resolution in collection bands is a duplicate of eo:gsd . Use eo:gsd instead.","title":"Deprecated"},{"location":"changelog/#fixed_1","text":"Fixed a wrong definition of the header OpenEO-Costs in POST /preview . Fixed typo in method authenticateOIDC in client development guidelines. Fixed the definition of spatial extents by swapping north and south. Replaced the outdated occurrences of srs with crs in spatial extents. Added missing required descriptions to process definitions. Added missing error messages. Fixed unclear specification for arrays used as process graph arguments. Fixed inconsist schema of openEO error responses: Field is now consistently named message instead of description .","title":"Fixed"},{"location":"changelog/#030-2018-09-21","text":"First version after proof of concept tackling many major issues. No changelog available.","title":"[0.3.0] - 2018-09-21"},{"location":"changelog/#002-2018-03-22","text":"Version for proof of concept. No changelog available.","title":"[0.0.2] - 2018-03-22"},{"location":"changelog/#001-2018-02-07","text":"Initial version.","title":"[0.0.1] - 2018-02-07"},{"location":"codeofconduct/","text":"Contributor Code of Conduct \u00b6 As contributors and maintainers of this project, we pledge to respect all people who contribute through reporting issues, posting feature requests, updating documentation, submitting pull requests or patches, and other activities. We are committed to making participation in this project a harassment-free experience for everyone, regardless of level of experience, gender, gender identity and expression, sexual orientation, disability, personal appearance, body size, race, ethnicity, age, or religion. Examples of unacceptable behavior by participants include the use of sexual language or imagery, derogatory comments or personal attacks, trolling, public or private harassment, insults, or other unprofessional conduct. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct. Project maintainers who do not follow the Code of Conduct may be removed from the project team. Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by opening an issue or contacting one or more of the project maintainers. This Code of Conduct is adapted from the Contributor Covenant , version 1.0.0, available at http://contributor-covenant.org/version/1/0/0/ .","title":"Contributor Code of Conduct"},{"location":"codeofconduct/#contributor-code-of-conduct","text":"As contributors and maintainers of this project, we pledge to respect all people who contribute through reporting issues, posting feature requests, updating documentation, submitting pull requests or patches, and other activities. We are committed to making participation in this project a harassment-free experience for everyone, regardless of level of experience, gender, gender identity and expression, sexual orientation, disability, personal appearance, body size, race, ethnicity, age, or religion. Examples of unacceptable behavior by participants include the use of sexual language or imagery, derogatory comments or personal attacks, trolling, public or private harassment, insults, or other unprofessional conduct. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct. Project maintainers who do not follow the Code of Conduct may be removed from the project team. Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by opening an issue or contacting one or more of the project maintainers. This Code of Conduct is adapted from the Contributor Covenant , version 1.0.0, available at http://contributor-covenant.org/version/1/0/0/ .","title":"Contributor Code of Conduct"},{"location":"collections/","text":"Data Discovery (Collections) \u00b6 openEO strives for compatibility with STAC and OGC WFS as far as possible. Implementing the data discovery endpoints of openEO should also produce valid STAC 0.6 and WFS 3.0 responses, including a partial compatibility with their APIs (see below). WARNING : STAC and OGC WFS 3, as well as openEO, are still under development. Therefore, it is very likely that further changes and adjustments will be made in the future. Extensions \u00b6 STAC has several extensions that can be used to better describe your data. Clients and server are not required to implement all of them, so be aware that some clients may not be able to read all your meta data. Some commonly used extensions that are integrated in the openEO API specification are: EO (Electro-Optical) extension SAR extension Commons extension Scientific extension Data Cube extension Non-Common Properties extension Links \u00b6 For data discovery in general and each collection you can specify a set of references. These can be alternate representations, e.g. data discovery via OGC WCS or OGC CSW, references to a license, references to actual raw data for downloading, detailed information about pre-processing, etc. Note : STAC requires to add a link with relation type self (see below). Although this is not technically necessary for openEO and we do not enforce to provide such a link with our validation tools, we still recommend to provide it anyway for compatibility reasons. Common link relation types \u00b6 The following table lists relation types that are commonly used as rel types in the links. The scope 'Collections' refers to the links that are related to a specific collection, 'Discovery' refers to links that are related to data discovery in general and are not about a specific collection. Type Description Scope self Absolute URL to the data discovery endpoint or the collection itself. Discovery +Collections root / parent URL to the data discovery endpoint. Collections child URL to a child STAC Catalog or STAC Dataset. Collections item URL to a STAC Item. Collections license The license URL for the dataset SHOULD be specified if the license field is set to proprietary . If there is no public license URL available, it is RECOMMENDED to supplement the collection with the license text in a separate file and link to this file. Collections alternate An alternative representation of the metadata. This could be a secondary web service such as OGC WCS or OGC CSW or a metadata document following another standard such as ISO 19115, INSPIRE or DCAT. Discovery +Collections about A resource that is related or further explains the entity, e.g. a user guide. Discovery +Collections derived_from Allows referencing the data this collection was derived from. Collections cite-as For all DOI names specified, the respective DOI links SHOULD be added to the links section of the catalog with the rel type cite-as . Collections More relation types may be listed in the STAC documentation. Compatibility with WFS and STAC APIs \u00b6 The data discovery endpoints GET /collections and GET /collections/{name} are compatible with WFS 3 and STAC. The only limitation with regard to response compatibility is that openEO allows open date ranges and WFS does not (see issue WFS_FES#155 ). Additionally, STAC and WFS define additional endpoints that need to be implemented to be fully compatible. The additional information can easily be integrated into an openEO API implementation. A rough list of actions for compatibility is available below, but please refer to their specifications to find out the full details. WFS 3.0 compatibility \u00b6 As of now, WFS 3.0 requires more endpoints for full compatibility. You should make the following changes to your API to implement a valid WFS: Add a links property to the GET / request that links to the WFS endpoints. Implement GET /api and return the WFS OpenAPI document. Implement GET /conformance and specify which conformance classes your WFS conforms to. Implement GET /collections/{collectionId}/items and GET /collections/{collectionId}/items/{featureId} to support retrieval of individual features. STAC compatibility \u00b6 As of now, STAC has two more required endpoints that need to be implemented: GET /stac POST /stac/search Provide data for download \u00b6 If you'd like to provide your data for download in addition to offering the cloud processing service, you can implement the full STAC API. Therefore you can implement the endpoints GET /collections/{collectionId}/items and GET /collections/{collection-name}/items/{featureId} to support retrieval of individual items. To benefit from the STAC ecosystem it is also recommended to implement the GET /stac endpoint. To allow searching for items you can also implement POST /stac/search . Further information can be found in the STAC API respository and in the corresponding OpenAPI specification .","title":"Data Discovery"},{"location":"collections/#data-discovery-collections","text":"openEO strives for compatibility with STAC and OGC WFS as far as possible. Implementing the data discovery endpoints of openEO should also produce valid STAC 0.6 and WFS 3.0 responses, including a partial compatibility with their APIs (see below). WARNING : STAC and OGC WFS 3, as well as openEO, are still under development. Therefore, it is very likely that further changes and adjustments will be made in the future.","title":"Data Discovery (Collections)"},{"location":"collections/#extensions","text":"STAC has several extensions that can be used to better describe your data. Clients and server are not required to implement all of them, so be aware that some clients may not be able to read all your meta data. Some commonly used extensions that are integrated in the openEO API specification are: EO (Electro-Optical) extension SAR extension Commons extension Scientific extension Data Cube extension Non-Common Properties extension","title":"Extensions"},{"location":"collections/#links","text":"For data discovery in general and each collection you can specify a set of references. These can be alternate representations, e.g. data discovery via OGC WCS or OGC CSW, references to a license, references to actual raw data for downloading, detailed information about pre-processing, etc. Note : STAC requires to add a link with relation type self (see below). Although this is not technically necessary for openEO and we do not enforce to provide such a link with our validation tools, we still recommend to provide it anyway for compatibility reasons.","title":"Links"},{"location":"collections/#common-link-relation-types","text":"The following table lists relation types that are commonly used as rel types in the links. The scope 'Collections' refers to the links that are related to a specific collection, 'Discovery' refers to links that are related to data discovery in general and are not about a specific collection. Type Description Scope self Absolute URL to the data discovery endpoint or the collection itself. Discovery +Collections root / parent URL to the data discovery endpoint. Collections child URL to a child STAC Catalog or STAC Dataset. Collections item URL to a STAC Item. Collections license The license URL for the dataset SHOULD be specified if the license field is set to proprietary . If there is no public license URL available, it is RECOMMENDED to supplement the collection with the license text in a separate file and link to this file. Collections alternate An alternative representation of the metadata. This could be a secondary web service such as OGC WCS or OGC CSW or a metadata document following another standard such as ISO 19115, INSPIRE or DCAT. Discovery +Collections about A resource that is related or further explains the entity, e.g. a user guide. Discovery +Collections derived_from Allows referencing the data this collection was derived from. Collections cite-as For all DOI names specified, the respective DOI links SHOULD be added to the links section of the catalog with the rel type cite-as . Collections More relation types may be listed in the STAC documentation.","title":"Common link relation types"},{"location":"collections/#compatibility-with-wfs-and-stac-apis","text":"The data discovery endpoints GET /collections and GET /collections/{name} are compatible with WFS 3 and STAC. The only limitation with regard to response compatibility is that openEO allows open date ranges and WFS does not (see issue WFS_FES#155 ). Additionally, STAC and WFS define additional endpoints that need to be implemented to be fully compatible. The additional information can easily be integrated into an openEO API implementation. A rough list of actions for compatibility is available below, but please refer to their specifications to find out the full details.","title":"Compatibility with WFS and STAC APIs"},{"location":"collections/#wfs-30-compatibility","text":"As of now, WFS 3.0 requires more endpoints for full compatibility. You should make the following changes to your API to implement a valid WFS: Add a links property to the GET / request that links to the WFS endpoints. Implement GET /api and return the WFS OpenAPI document. Implement GET /conformance and specify which conformance classes your WFS conforms to. Implement GET /collections/{collectionId}/items and GET /collections/{collectionId}/items/{featureId} to support retrieval of individual features.","title":"WFS 3.0 compatibility"},{"location":"collections/#stac-compatibility","text":"As of now, STAC has two more required endpoints that need to be implemented: GET /stac POST /stac/search","title":"STAC compatibility"},{"location":"collections/#provide-data-for-download","text":"If you'd like to provide your data for download in addition to offering the cloud processing service, you can implement the full STAC API. Therefore you can implement the endpoints GET /collections/{collectionId}/items and GET /collections/{collection-name}/items/{featureId} to support retrieval of individual items. To benefit from the STAC ecosystem it is also recommended to implement the GET /stac endpoint. To allow searching for items you can also implement POST /stac/search . Further information can be found in the STAC API respository and in the corresponding OpenAPI specification .","title":"Provide data for download"},{"location":"cors/","text":"Cross-Origin Resource Sharing (CORS) \u00b6 Cross-origin resource sharing (CORS) is a mechanism that allows restricted resources [...] on a web page to be requested from another domain outside the domain from which the first resource was served. [...] CORS defines a way in which a browser and server can interact to determine whether or not it is safe to allow the cross-origin request. It allows for more freedom and functionality than purely same-origin requests, but is more secure than simply allowing all cross-origin requests. Source: https://en.wikipedia.org/wiki/Cross-origin_resource_sharing openEO-based back-ends are usually hosted on a different domain / host than the client that is requesting data from the back-end. Therefore most requests to the back-end are blocked by all modern browsers. This leads to the problem that the JavaScript library (and the Web Editor) can't access any back-end. Therefore, all back-end providers SHOULD support CORS. Without supporting CORS users can't access the back-end with browser-based clients, i.e. the JavaScript client . CORS is a recommendation of the W3C organization. The following chapters will explain how back-end providers can implement CORS support. 1. Supporting the OPTIONS method \u00b6 All endpoints must respond to the OPTIONS HTTP method. This is a response for the preflight requests made by the browsers. It needs to respond with a status code of 204 and send the HTTP headers shown in the table below. No body needs to be provided. Name Description Example Access-Control-Allow-Origin Allowed origin for the request, including protocol, host and port. It is RECOMMENDED to return the value of the request's origin header. If no Origin is sent to the back-end CORS headers SHOULD NOT be sent at all. http://client.isp.com:80 Access-Control-Allow-Credentials If authorization is implemented by the back-end the value MUST be true . true Access-Control-Allow-Headers Comma-separated list of HTTP headers allowed to be send. MUST contain at least Authorization if authorization is implemented by the back-end. Authorization, Content-Type Access-Control-Allow-Methods Comma-separated list of HTTP methods allowed to be requested. Back-ends MUST list all implemented HTTP methods for the endpoint here. OPTIONS, GET, POST, PATCH, PUT, DELETE Access-Control-Expose-Headers Some endpoints send non-safelisted HTTP response headers such as OpenEO-Identifier and OpenEO-Costs . All headers except Cache-Control , Content-Language , Content-Type , Expires , Last-Modified and Pragma must be listed in this header. Currently, the openEO API requires at least the following headers to be listed: Location, OpenEO-Identifier, OpenEO-Costs . Location, OpenEO-Identifier, OpenEO-Costs Content-Type SHOULD return the content type delivered by the request that the permission is requested for. application/json Example request and response \u00b6 Request: OPTIONS /api/v1/jobs HTTP / 1.1 Host : openeo.cloudprovider.com Origin : http://client.org:8080 Access-Control-Request-Method : POST Access-Control-Request-Headers : Authorization, Content-Type Response: HTTP / 1.1 204 No Content Access-Control-Allow-Origin : http://client.org:8080 Access-Control-Allow-Credentials : true Access-Control-Allow-Methods : OPTIONS, GET, POST, PATCH, PUT, DELETE Access-Control-Allow-Headers : Authorization, Content-Type Content-Type : application/json 2. Sending CORS headers \u00b6 The following headers MUST be included with every response: Name Description Example Access-Control-Allow-Origin Allowed origin for the request, including protocol, host and port. It is RECOMMENDED to return the value of the request's origin header. If no Origin is sent to the back-end CORS headers SHOULD NOT be sent at all. http://client.isp.com:80 Access-Control-Allow-Credentials If authorization is implemented by the back-end the value MUST be true . true Access-Control-Expose-Headers Some endpoints send non-safelisted HTTP response headers such as OpenEO-Identifier and OpenEO-Costs . All headers except Cache-Control , Content-Language , Content-Type , Expires , Last-Modified and Pragma must be listed in this header. Currently, the openEO API requires at least the following headers to be listed: Location, OpenEO-Identifier, OpenEO-Costs . Location, OpenEO-Identifier, OpenEO-Costs Tip : Most server can send the required headers and the responses to the OPTIONS requests globally. Otherwise you may want to use a proxy server to add the headers and OPTIONS responses.","title":"CORS"},{"location":"cors/#cross-origin-resource-sharing-cors","text":"Cross-origin resource sharing (CORS) is a mechanism that allows restricted resources [...] on a web page to be requested from another domain outside the domain from which the first resource was served. [...] CORS defines a way in which a browser and server can interact to determine whether or not it is safe to allow the cross-origin request. It allows for more freedom and functionality than purely same-origin requests, but is more secure than simply allowing all cross-origin requests. Source: https://en.wikipedia.org/wiki/Cross-origin_resource_sharing openEO-based back-ends are usually hosted on a different domain / host than the client that is requesting data from the back-end. Therefore most requests to the back-end are blocked by all modern browsers. This leads to the problem that the JavaScript library (and the Web Editor) can't access any back-end. Therefore, all back-end providers SHOULD support CORS. Without supporting CORS users can't access the back-end with browser-based clients, i.e. the JavaScript client . CORS is a recommendation of the W3C organization. The following chapters will explain how back-end providers can implement CORS support.","title":"Cross-Origin Resource Sharing (CORS)"},{"location":"cors/#1-supporting-the-options-method","text":"All endpoints must respond to the OPTIONS HTTP method. This is a response for the preflight requests made by the browsers. It needs to respond with a status code of 204 and send the HTTP headers shown in the table below. No body needs to be provided. Name Description Example Access-Control-Allow-Origin Allowed origin for the request, including protocol, host and port. It is RECOMMENDED to return the value of the request's origin header. If no Origin is sent to the back-end CORS headers SHOULD NOT be sent at all. http://client.isp.com:80 Access-Control-Allow-Credentials If authorization is implemented by the back-end the value MUST be true . true Access-Control-Allow-Headers Comma-separated list of HTTP headers allowed to be send. MUST contain at least Authorization if authorization is implemented by the back-end. Authorization, Content-Type Access-Control-Allow-Methods Comma-separated list of HTTP methods allowed to be requested. Back-ends MUST list all implemented HTTP methods for the endpoint here. OPTIONS, GET, POST, PATCH, PUT, DELETE Access-Control-Expose-Headers Some endpoints send non-safelisted HTTP response headers such as OpenEO-Identifier and OpenEO-Costs . All headers except Cache-Control , Content-Language , Content-Type , Expires , Last-Modified and Pragma must be listed in this header. Currently, the openEO API requires at least the following headers to be listed: Location, OpenEO-Identifier, OpenEO-Costs . Location, OpenEO-Identifier, OpenEO-Costs Content-Type SHOULD return the content type delivered by the request that the permission is requested for. application/json","title":"1. Supporting the OPTIONS method"},{"location":"cors/#example-request-and-response","text":"Request: OPTIONS /api/v1/jobs HTTP / 1.1 Host : openeo.cloudprovider.com Origin : http://client.org:8080 Access-Control-Request-Method : POST Access-Control-Request-Headers : Authorization, Content-Type Response: HTTP / 1.1 204 No Content Access-Control-Allow-Origin : http://client.org:8080 Access-Control-Allow-Credentials : true Access-Control-Allow-Methods : OPTIONS, GET, POST, PATCH, PUT, DELETE Access-Control-Allow-Headers : Authorization, Content-Type Content-Type : application/json","title":"Example request and response"},{"location":"cors/#2-sending-cors-headers","text":"The following headers MUST be included with every response: Name Description Example Access-Control-Allow-Origin Allowed origin for the request, including protocol, host and port. It is RECOMMENDED to return the value of the request's origin header. If no Origin is sent to the back-end CORS headers SHOULD NOT be sent at all. http://client.isp.com:80 Access-Control-Allow-Credentials If authorization is implemented by the back-end the value MUST be true . true Access-Control-Expose-Headers Some endpoints send non-safelisted HTTP response headers such as OpenEO-Identifier and OpenEO-Costs . All headers except Cache-Control , Content-Language , Content-Type , Expires , Last-Modified and Pragma must be listed in this header. Currently, the openEO API requires at least the following headers to be listed: Location, OpenEO-Identifier, OpenEO-Costs . Location, OpenEO-Identifier, OpenEO-Costs Tip : Most server can send the required headers and the responses to the OPTIONS requests globally. Otherwise you may want to use a proxy server to add the headers and OPTIONS responses.","title":"2. Sending CORS headers"},{"location":"errors/","text":"Status and error handling \u00b6 The success of requests MUST be indicated using HTTP status codes according to RFC 7231 . If the API responds with a status code between 100 and 399 the back-end indicates that the request has been handled successfully. In general an error is communicated with a status code between 400 and 599. Client errors are defined as a client passing invalid data to the service and the service correctly rejecting that data. Examples include invalid credentials, incorrect parameters, unknown versions, or similar. These are generally \"4xx\" HTTP error codes and are the result of a client passing incorrect or invalid data. Client errors do not contribute to overall API availability. Server errors are defined as the server failing to correctly return in response to a valid client request. These are generally \"5xx\" HTTP error codes. Server errors do contribute to the overall API availability. Calls that fail due to rate limiting or quota failures MUST NOT count as server errors. JSON error object \u00b6 A JSON error object SHOULD be sent with all responses that have a status code between 400 and 599. { \"id\" : \"936DA01F-9ABD-4D9D-80C7-02AF85C822A8\" , \"code\" : \"SampleError\" , \"message\" : \"A sample error message.\" , \"url\" : \"http://www.openeo.org/docs/errors/SampleError\" } Sending code and message is REQUIRED. A back-end MAY add a free-form id (unique identifier) to the error response to be able to log and track errors with further non-disclosable details. The code is either one of the standardized textual openEO error codes below or a proprietary error code. The message explains the reason the server is rejecting the request. For \"4xx\" error codes the message explains how the client needs to modify the request. By default the message MUST be sent in English language. Content Negotiation is used to localize the error messages: If an Accept-Language header is sent by the client and a translation is available, the message should be translated accordingly and the Content-Language header must be present in the response. See \" How to localize your API \" for more information. url is an OPTIONAL attribute and contains a link to a resource that is explaining the error and potential solutions in-depth. Standardized status codes \u00b6 The openEO API usually uses the following HTTP status codes for successful requests: 200 OK : Indicates a successful request with a response body being sent. 201 Created Indicates a successful request that successfully created a new resource. Sends a Location header to the newly created resource without a response body. 202 Accepted Indicates a successful request that successfully queued the creation of a new resource, but it has not been created yet. The response is sent without a response body. 204 No Content : Indicates a successful request without a response body being sent. The openEO API often uses the following HTTP status codes for failed requests: 400 Bad Request : The back-end responds with this error code whenever the error has its origin on client side and no other HTTP status code in the 400 range is suitable. 401 Unauthorized : The client did not provide any authentication details for a resource requiring authentication or the provided authentication details are not correct. 403 Forbidden : The client did provided correct authentication details, but the privileges/permissions of the provided credentials do not allow to request the resource. 404 Not Found : The resource specified by the path does not exist, i.e. one of the resources belonging to the specified identifiers are not available at the back-end. Note: Unsupported endpoints MUST use HTTP status code 501. 500 Internal Server Error : The error has its origin on server side and no other status code in the 500 range is suitable. 501 Not Implemented : An endpoint is specified in the openEO API, but is not supported. If a HTTP status code in the 400 range is returned, the client SHOULD NOT repeat the request without modifications. For HTTP status code in the 500 range, the client MAY repeat the same request later. All HTTP status codes defined in RFC 7231 in the 400 and 500 ranges can be used as openEO error code in addition to the most used status codes mentioned here. Responding with openEO error codes 400 and 500 SHOULD be avoided in favor of any more specific standardized or proprietary openEO error code. openEO error codes \u00b6 The following table of error codes is incomplete . These error codes will evolve over time. If you are missing any common error, please contribute it by adding an issue , creating a pull request or get in contact in our chat room . The whole table of error codes is available as JSON file , which can be used by implementors to automatically generate error responses. Categories Account Management EO Data Discovery File Management General Job Management Process Graph Management Processes Secondary Services Management Subscriptions Account Management openEO Error Code Description Message HTTP Status Code AuthenticationRequired The client did not provide any authentication details for a resource requiring authentication or the provided authentication details are not correct. Unauthorized. 401 AuthenticationSchemeInvalid Invalid authentication scheme (e.g. Bearer). Authentication method not supported. 403 CredentialsInvalid Credentials are not correct. 403 TokenInvalid Authorization token invalid or expired. Session has expired. 403 EO Data Discovery openEO Error Code Description Message HTTP Status Code CollectionNotFound The requested collection does not exist. Collection does not exist. 404 File Management openEO Error Code Description Message HTTP Status Code ContentTypeInvalid The specified media (MIME) type used in the Content-Type header is not allowed. Media type specified in the request is not supported. Supported media types: {types} 400 FileContentInvalid The content of the file is invalid. File content is invalid. 400 FileLocked The file is locked by a running job or another process. File '{file}' is locked. 400 FileNotFound The requested file does not exist. File does not exist. 404 FileOperationUnsupported The specified path is not a file and the operation is only supported for files. Path is likely a directory. Operation is only supported for files. 400 FilePathInvalid The specified path is invalid or not accessible. Path could contain invalid characters, an invalid user ID or point to an existing folder or a location outside of the user folder. File path is invalid. 400 FileSizeExceeded File exceeds allowed maximum file size. File size it too large. Maximum file size: {size} 400 FileTypeInvalid File format, file extension or media (MIME) type is not allowed. File type not allowed. Allowed file types: {types} 400 StorageFailure Server couldn't store file(s) due to server-side reasons. Unable to store file(s). 500 StorageQuotaExceeded The storage quota has been exceeded by the user. Insufficient Storage. 400 General openEO Error Code Description Message HTTP Status Code ContentTypeInvalid The specified media (MIME) type used in the Content-Type header is not allowed. Media type specified in the request is not supported. Supported media types: {types} 400 FeatureUnsupported The back-end responds with this error whenever an endpoint is specified in the openEO API, but is not supported. Feature not supported. 501 InfrastructureBusy Service is generally available, but the infrastructure can't handle it at the moment as too many requests are processed. Service is not available at the moment due to overloading. Please try again later. 503 InfrastructureMaintenance Service is currently not available, but the infrastructure is currently undergoing maintenance work. Service is not available at the moment due to maintenance work. Please try again later. 503 Internal An internal server error with a proprietary message. Server error: {message} 500 NotFound To be used if the requested resource does not exist. Note: Unsupported endpoints MUST send an 'FeatureUnsupported' error. There are also specialized errors for missing jobs (JobNotFound), files (FileNotFound), etc. Resource not found. 404 Timeout The request took too long and timed out. Request timed out. 408 Job Management openEO Error Code Description Message HTTP Status Code BillingPlanInvalid The specified billing plan is not on the list of available plans. The specified billing plan is not valid. 400 BudgetInvalid The specified budget is too low as it is either smaller than or equal to 0 or below the costs. The specified budget is too low. 400 FormatArgumentInvalid The value specified for the output format argument '{argument}' is invalid: {reason} 400 FormatArgumentUnsupported Output format argument '{argument}' is not supported. 400 FormatUnsuitable Data can't be transformed into the requested output format. 400 FormatUnsupported Output format not supported. 400 JobLocked The job is currently locked due to a running batch computation and can't be modified meanwhile. Job is locked due to a queued or running batch computation. 400 JobNotFinished Job has not finished computing the results yet. Please try again later. 400 JobNotFound The requested job does not exist. The job does not exist. 404 JobNotStarted Job has not been queued or started yet or was canceled and not restarted by the user. Job hasn't been started yet. 400 NoDataForUpdate For PATCH requests: No valid data specified at all. No valid data specified to be updated. 400 PaymentRequired The budget required to fulfil the request are insufficient. Payment required. 402 ProcessGraphMissing No valid process graph specified. 400 PropertyNotEditable For PATCH requests: The specified parameter can't be updated. It is read-only. Specified property '{property}' is read-only. 400 StorageFailure Server couldn't store file(s) due to server-side reasons. Unable to store file(s). 500 StorageQuotaExceeded The storage quota has been exceeded by the user. Insufficient Storage. 400 Timeout The request took too long and timed out. Request timed out. 408 VariableDefaultValueTypeInvalid The default value specified for the process graph variable '{variable_id}' is not of type '{type}'. 400 VariableIdInvalid A specified variable ID is not valid. 400 VariableTypeInvalid The data type specified for the process graph variable '{variable_id}' is invalid. Must be one of: string, boolean, number, array or object. 400 VariableValueMissing No value specified for process graph variable '{variable_id}'. 400 Process Graph Management openEO Error Code Description Message HTTP Status Code NoDataForUpdate For PATCH requests: No valid data specified at all. No valid data specified to be updated. 400 ProcessGraphMissing No valid process graph specified. 400 ProcessGraphNotFound The requested process graph does not exist. Process graph does not exist. 404 PropertyNotEditable For PATCH requests: The specified parameter can't be updated. It is read-only. Specified property '{property}' is read-only. 400 VariableDefaultValueTypeInvalid The default value specified for the process graph variable '{variable_id}' is not of type '{type}'. 400 VariableIdInvalid A specified variable ID is not valid. 400 VariableTypeInvalid The data type specified for the process graph variable '{variable_id}' is invalid. Must be one of: string, boolean, number, array or object. 400 VariableValueMissing No value specified for process graph variable '{variable_id}'. 400 Processes openEO Error Code Description Message HTTP Status Code CRSInvalid Invalid or unsupported CRS specified. CRS '{crs}' is invalid. 400 CollectionNotFound The requested collection does not exist. Collection does not exist. 404 CoordinateOutOfBounds Coordinate is out of bounds. 400 FileContentInvalid The content of the file is invalid. File content is invalid. 400 FileNotFound The requested file does not exist. File does not exist. 404 JobNotFound The requested job does not exist. The job does not exist. 404 ProcessArgumentInvalid The value specified for the process argument '{argument}' in process '{process}' is invalid: {reason} 400 ProcessArgumentRequired Process '{process}' requires argument '{argument}'. 400 ProcessArgumentUnsupported Process '{process}' does not support argument '{argument}'. 400 ProcessArgumentsMissing Process '{process}' requires at least '{min_parameters}' parameters. 400 ProcessUnsupported Process '{process}' is not supported. 400 Secondary Services Management openEO Error Code Description Message HTTP Status Code BillingPlanInvalid The specified billing plan is not on the list of available plans. The specified billing plan is not valid. 400 BudgetInvalid The specified budget is too low as it is either smaller than or equal to 0 or below the costs. The specified budget is too low. 400 NoDataForUpdate For PATCH requests: No valid data specified at all. No valid data specified to be updated. 400 PaymentRequired The budget required to fulfil the request are insufficient. Payment required. 402 ProcessGraphMissing No valid process graph specified. 400 PropertyNotEditable For PATCH requests: The specified parameter can't be updated. It is read-only. Specified property '{property}' is read-only. 400 ServiceArgumentInvalid The value specified for the secondary service argument '{argument}' is invalid: {reason} 400 ServiceArgumentRequired Required secondary service argument '{argument}' is missing. 400 ServiceArgumentUnsupported Secondary service argument '{argument}' is not supported. 400 ServiceNotFound The requested secondary service does not exist. Service does not exist. 404 ServiceUnsupported Secondary service type is not supported. 400 VariableValueMissing No value specified for process graph variable '{variable_id}'. 400 Subscriptions openEO Error Code Description Message HTTP Status Code WebSocketUpgradeNotRequested In order to subscribe the connection must be upgradable to WebSockets. Client sent invalid request to establish subscriptions. 400","title":"Error Handling"},{"location":"errors/#status-and-error-handling","text":"The success of requests MUST be indicated using HTTP status codes according to RFC 7231 . If the API responds with a status code between 100 and 399 the back-end indicates that the request has been handled successfully. In general an error is communicated with a status code between 400 and 599. Client errors are defined as a client passing invalid data to the service and the service correctly rejecting that data. Examples include invalid credentials, incorrect parameters, unknown versions, or similar. These are generally \"4xx\" HTTP error codes and are the result of a client passing incorrect or invalid data. Client errors do not contribute to overall API availability. Server errors are defined as the server failing to correctly return in response to a valid client request. These are generally \"5xx\" HTTP error codes. Server errors do contribute to the overall API availability. Calls that fail due to rate limiting or quota failures MUST NOT count as server errors.","title":"Status and error handling"},{"location":"errors/#json-error-object","text":"A JSON error object SHOULD be sent with all responses that have a status code between 400 and 599. { \"id\" : \"936DA01F-9ABD-4D9D-80C7-02AF85C822A8\" , \"code\" : \"SampleError\" , \"message\" : \"A sample error message.\" , \"url\" : \"http://www.openeo.org/docs/errors/SampleError\" } Sending code and message is REQUIRED. A back-end MAY add a free-form id (unique identifier) to the error response to be able to log and track errors with further non-disclosable details. The code is either one of the standardized textual openEO error codes below or a proprietary error code. The message explains the reason the server is rejecting the request. For \"4xx\" error codes the message explains how the client needs to modify the request. By default the message MUST be sent in English language. Content Negotiation is used to localize the error messages: If an Accept-Language header is sent by the client and a translation is available, the message should be translated accordingly and the Content-Language header must be present in the response. See \" How to localize your API \" for more information. url is an OPTIONAL attribute and contains a link to a resource that is explaining the error and potential solutions in-depth.","title":"JSON error object"},{"location":"errors/#standardized-status-codes","text":"The openEO API usually uses the following HTTP status codes for successful requests: 200 OK : Indicates a successful request with a response body being sent. 201 Created Indicates a successful request that successfully created a new resource. Sends a Location header to the newly created resource without a response body. 202 Accepted Indicates a successful request that successfully queued the creation of a new resource, but it has not been created yet. The response is sent without a response body. 204 No Content : Indicates a successful request without a response body being sent. The openEO API often uses the following HTTP status codes for failed requests: 400 Bad Request : The back-end responds with this error code whenever the error has its origin on client side and no other HTTP status code in the 400 range is suitable. 401 Unauthorized : The client did not provide any authentication details for a resource requiring authentication or the provided authentication details are not correct. 403 Forbidden : The client did provided correct authentication details, but the privileges/permissions of the provided credentials do not allow to request the resource. 404 Not Found : The resource specified by the path does not exist, i.e. one of the resources belonging to the specified identifiers are not available at the back-end. Note: Unsupported endpoints MUST use HTTP status code 501. 500 Internal Server Error : The error has its origin on server side and no other status code in the 500 range is suitable. 501 Not Implemented : An endpoint is specified in the openEO API, but is not supported. If a HTTP status code in the 400 range is returned, the client SHOULD NOT repeat the request without modifications. For HTTP status code in the 500 range, the client MAY repeat the same request later. All HTTP status codes defined in RFC 7231 in the 400 and 500 ranges can be used as openEO error code in addition to the most used status codes mentioned here. Responding with openEO error codes 400 and 500 SHOULD be avoided in favor of any more specific standardized or proprietary openEO error code.","title":"Standardized status codes"},{"location":"errors/#openeo-error-codes","text":"The following table of error codes is incomplete . These error codes will evolve over time. If you are missing any common error, please contribute it by adding an issue , creating a pull request or get in contact in our chat room . The whole table of error codes is available as JSON file , which can be used by implementors to automatically generate error responses.","title":"openEO error codes"},{"location":"examples-poc/","text":"Examples (proof of concept) \u00b6 This page gives a detailed description of the openEO proof of concept use cases. After the proof of concept, this stays in the API to have some basic examples. The proof of concept covered three clearly defined example use cases and how they are translated to sequences of API calls: Deriving minimum NDVI measurements over pixel time series of Sentinel 2 imagery Create a monthly aggregated Sentinel 1 product from a custom Python script Compute time series of zonal (regional) statistics of Sentinel 2 imagery over user-specified polygons Note : CORS and authentication is not included in these examples for simplicity. Repeating calls are also not included as it would not make much sense to list the same discovery requests (see Use Case 1, requests 1 to 6) for each use case individually. Use Case 1 \u00b6 Deriving minimum NDVI measurements over pixel time series of Sentinel 2 imagery. A similar example (computing an EVI) is also available. Requesting the API versions available at the back-end Request GET /.well-known/openeo Requesting the capabilities of the back-end Note: The actual request path depends on the response of the previous request. Request GET / Check which collections are available at the back-end Request GET /collections HTTP / 1.1 Request details about a specific collection (e.g. Sentinel 2) Note: The actual collection ID in the path depends on the response of the previous request. Request GET /collections/Sentinel-2 HTTP / 1.1 Check that needed processes are available Request GET /processes HTTP / 1.1 Request the supported secondary web service types Request GET /service_types HTTP / 1.1 Create a WMS service Request POST /services HTTP / 1.1 Content-Type : application/json; charset=utf-8 { \"title\" : \"Min. NDVI for Sentinel 2\" , \"description\" : \"Deriving minimum NDVI measurements over pixel time series of Sentinel 2 imagery.\" , \"process_graph\" : { \"loadco1\" : { \"process_id\" : \"load_collection\" , \"arguments\" : { \"id\" : \"Sentinel-2\" , \"spatial_extent\" : { \"west\" : { \"variable_id\" : \"spatial_extent_west\" }, \"east\" : { \"variable_id\" : \"spatial_extent_east\" }, \"north\" : { \"variable_id\" : \"spatial_extent_north\" }, \"south\" : { \"variable_id\" : \"spatial_extent_south\" } }, \"temporal_extent\" : [ \"2017-01-01\" , \"2017-02-01\" ] } }, \"ndvi1\" : { \"process_id\" : \"ndvi\" , \"arguments\" : { \"data\" : { \"from_node\" : \"loadco1\" } } }, \"reduce1\" : { \"process_id\" : \"reduce\" , \"arguments\" : { \"data\" : { \"from_node\" : \"ndvi1\" }, \"dimension\" : \"temporal\" , \"reducer\" : { \"callback\" : { \"min1\" : { \"process_id\" : \"min\" , \"arguments\" : { \"data\" : { \"from_argument\" : \"data\" } }, \"result\" : true } } } }, \"result\" : true } }, \"type\" : \"WMS\" , \"parameters\" : { \"version\" : \"1.1.1\" } } Response HTTP / 1.1 201 Created Location : /services/wms-a3cca9 OpenEO-Identifier : wms-a3cca9 Requesting the service information Request GET /services/wms-a3cca9 HTTP / 1.1 Download the data on demand from the WMS Omitted, not part of the openEO API. Use Case 2 \u00b6 Create a monthly aggregated Sentinel 1 product from a custom Python script. Upload python script Request PUT /files/john_doe/s1_aggregate.py HTTP / 1.1 Content-Type : application/octet-stream <File content> Create a batch job Request POST /jobs HTTP / 1.1 Content-Type : application/json; charset=utf-8 { \"title\" : \"Monthly aggregation on Sentinel 1\" , \"description\" : \"Create a monthly aggregated Sentinel 1 product from a custom Python script.\" , \"process_graph\" : { \"loadco1\" : { \"process_id\" : \"load_collection\" , \"arguments\" : { \"id\" : \"Sentinel-1\" , \"spatial_extent\" : { \"west\" : 16.1 , \"east\" : 16.6 , \"north\" : 48.6 , \"south\" : 47.2 }, \"temporal_extent\" : [ \"2017-01-01\" , \"2017-02-01\" ] } }, \"reduce1\" : { \"process_id\" : \"reduce\" , \"arguments\" : { \"data\" : { \"from_node\" : \"loadco1\" }, \"dimension\" : \"temporal\" , \"reducer\" : { \"callback\" : { \"runudf1\" : { \"process_id\" : \"run_udf\" , \"arguments\" : { \"data\" : [ { \"from_argument\" : \"x\" }, { \"from_argument\" : \"y\" } ], \"udf\" : \"s1_aggregate.py\" , \"runtime\" : \"Python\" }, \"result\" : true } } }, \"binary\" : true }, \"result\" : true } } } Response HTTP / 1.1 201 Created Location : https://openeo.org/api/v0.4/jobs/132 OpenEO-Identifier : 132 Start batch processing the job Request POST /jobs/132/results HTTP / 1.1 Create a TMS service Request POST /services HTTP / 1.1 Content-Type : application/json; charset=utf-8 { \"title\" : \"Monthly aggregation on Sentinel 1\" , \"description\" : \"Create a monthly aggregated Sentinel 1 product from a custom Python script.\" , \"process_graph\" : { \"1\" : { \"process_id\" : \"load_result\" , \"arguments\" : { \"id\" : \"132\" }, \"result\" : true } }, \"type\" : \"TMS\" } Response HTTP / 1.1 201 Created Location : https://openeo.org/api/v0.4/services/tms-75ff8c OpenEO-Identifier : tms-75ff8c Requesting the service information Request GET https://openeo.org/api/v0.4/services/tms-75ff8c HTTP / 1.1 Download the data on demand from the WMS Omitted, not part of the openEO API. Use Case 3 \u00b6 Compute time series of zonal (regional) statistics of Sentinel 2 imagery over user-specified polygons. Create a batch job Request POST /jobs HTTP / 1.1 Content-Type : application/json; charset=utf-8 { \"title\" : \"Zonal Statistics / Sentinel 2\" , \"description\" : \"Compute time series of zonal (regional) statistics of Sentinel 2 imagery over user-specified polygons.\" , \"process_graph\" : { \"loadco1\" : { \"process_id\" : \"load_collection\" , \"arguments\" : { \"id\" : \"Sentinel-2\" , \"spatial_extent\" : { \"west\" : 16.1 , \"east\" : 16.6 , \"north\" : 48.6 , \"south\" : 47.2 }, \"temporal_extent\" : [ \"2017-01-01\" , \"2017-02-01\" ], \"bands\" : [ \"B8\" ] } }, \"reduce1\" : { \"process_id\" : \"reduce\" , \"arguments\" : { \"data\" : { \"from_node\" : \"loadco1\" }, \"dimension\" : \"spectral\" } }, \"aggreg1\" : { \"process_id\" : \"aggregate_polygon\" , \"arguments\" : { \"data\" : { \"from_node\" : \"reduce1\" }, \"polygons\" : { \"type\" : \"Polygon\" , \"coordinates\" : [ [ [ 16.138916 , 48.320647 ], [ 16.524124 , 48.320647 ], [ 16.524124 , 48.1386 ], [ 16.138916 , 48.1386 ], [ 16.138916 , 48.320647 ] ] ] }, \"reducer\" : { \"callback\" : { \"mean1\" : { \"process_id\" : \"mean\" , \"arguments\" : { \"data\" : { \"from_argument\" : \"data\" } }, \"result\" : true } } } } }, \"savere1\" : { \"process_id\" : \"save_result\" , \"arguments\" : { \"data\" : { \"from_node\" : \"aggreg1\" }, \"format\" : \"JSON\" }, \"result\" : true } } } Response HTTP / 1.1 201 Created Location : https://openeo.org/jobs/133 OpenEO-Identifier : 133 Start batch processing the job Request POST /jobs/133/results HTTP / 1.1 Retrieve download links (after the job has finished) Request GET /jobs/133/results HTTP / 1.1 Response HTTP / 1.1 200 OK Content-Type : application/json; charset=utf-8 Expires : Wed, 01 May 2019 00:00:00 GMT OpenEO-Costs : 0 { \"id\" : \"133\" , \"title\" : \"Zonal Statistics / Sentinel 2\" , \"description\" : \"Compute time series of zonal (regional) statistics of Sentinel 2 imagery over user-specified polygons.\" , \"updated\" : \"2019-02-01T09:36:18Z\" , \"links\" : [ { \"href\" : \"https://cdn.openeo.org/4854b51643548ab8a858e2b8282711d8/result.json\" , \"type\" : \"application/json\" } ] } Download file(s) Request GET https://cdn.openeo.org/4854b51643548ab8a858e2b8282711d8/result.json HTTP / 1.1 Response A JSON file containing the results, content omitted.","title":"Examples (proof of concept)"},{"location":"examples-poc/#examples-proof-of-concept","text":"This page gives a detailed description of the openEO proof of concept use cases. After the proof of concept, this stays in the API to have some basic examples. The proof of concept covered three clearly defined example use cases and how they are translated to sequences of API calls: Deriving minimum NDVI measurements over pixel time series of Sentinel 2 imagery Create a monthly aggregated Sentinel 1 product from a custom Python script Compute time series of zonal (regional) statistics of Sentinel 2 imagery over user-specified polygons Note : CORS and authentication is not included in these examples for simplicity. Repeating calls are also not included as it would not make much sense to list the same discovery requests (see Use Case 1, requests 1 to 6) for each use case individually.","title":"Examples (proof of concept)"},{"location":"examples-poc/#use-case-1","text":"Deriving minimum NDVI measurements over pixel time series of Sentinel 2 imagery. A similar example (computing an EVI) is also available. Requesting the API versions available at the back-end Request GET /.well-known/openeo Requesting the capabilities of the back-end Note: The actual request path depends on the response of the previous request. Request GET / Check which collections are available at the back-end Request GET /collections HTTP / 1.1 Request details about a specific collection (e.g. Sentinel 2) Note: The actual collection ID in the path depends on the response of the previous request. Request GET /collections/Sentinel-2 HTTP / 1.1 Check that needed processes are available Request GET /processes HTTP / 1.1 Request the supported secondary web service types Request GET /service_types HTTP / 1.1 Create a WMS service Request POST /services HTTP / 1.1 Content-Type : application/json; charset=utf-8 { \"title\" : \"Min. NDVI for Sentinel 2\" , \"description\" : \"Deriving minimum NDVI measurements over pixel time series of Sentinel 2 imagery.\" , \"process_graph\" : { \"loadco1\" : { \"process_id\" : \"load_collection\" , \"arguments\" : { \"id\" : \"Sentinel-2\" , \"spatial_extent\" : { \"west\" : { \"variable_id\" : \"spatial_extent_west\" }, \"east\" : { \"variable_id\" : \"spatial_extent_east\" }, \"north\" : { \"variable_id\" : \"spatial_extent_north\" }, \"south\" : { \"variable_id\" : \"spatial_extent_south\" } }, \"temporal_extent\" : [ \"2017-01-01\" , \"2017-02-01\" ] } }, \"ndvi1\" : { \"process_id\" : \"ndvi\" , \"arguments\" : { \"data\" : { \"from_node\" : \"loadco1\" } } }, \"reduce1\" : { \"process_id\" : \"reduce\" , \"arguments\" : { \"data\" : { \"from_node\" : \"ndvi1\" }, \"dimension\" : \"temporal\" , \"reducer\" : { \"callback\" : { \"min1\" : { \"process_id\" : \"min\" , \"arguments\" : { \"data\" : { \"from_argument\" : \"data\" } }, \"result\" : true } } } }, \"result\" : true } }, \"type\" : \"WMS\" , \"parameters\" : { \"version\" : \"1.1.1\" } } Response HTTP / 1.1 201 Created Location : /services/wms-a3cca9 OpenEO-Identifier : wms-a3cca9 Requesting the service information Request GET /services/wms-a3cca9 HTTP / 1.1 Download the data on demand from the WMS Omitted, not part of the openEO API.","title":"Use Case 1"},{"location":"examples-poc/#use-case-2","text":"Create a monthly aggregated Sentinel 1 product from a custom Python script. Upload python script Request PUT /files/john_doe/s1_aggregate.py HTTP / 1.1 Content-Type : application/octet-stream <File content> Create a batch job Request POST /jobs HTTP / 1.1 Content-Type : application/json; charset=utf-8 { \"title\" : \"Monthly aggregation on Sentinel 1\" , \"description\" : \"Create a monthly aggregated Sentinel 1 product from a custom Python script.\" , \"process_graph\" : { \"loadco1\" : { \"process_id\" : \"load_collection\" , \"arguments\" : { \"id\" : \"Sentinel-1\" , \"spatial_extent\" : { \"west\" : 16.1 , \"east\" : 16.6 , \"north\" : 48.6 , \"south\" : 47.2 }, \"temporal_extent\" : [ \"2017-01-01\" , \"2017-02-01\" ] } }, \"reduce1\" : { \"process_id\" : \"reduce\" , \"arguments\" : { \"data\" : { \"from_node\" : \"loadco1\" }, \"dimension\" : \"temporal\" , \"reducer\" : { \"callback\" : { \"runudf1\" : { \"process_id\" : \"run_udf\" , \"arguments\" : { \"data\" : [ { \"from_argument\" : \"x\" }, { \"from_argument\" : \"y\" } ], \"udf\" : \"s1_aggregate.py\" , \"runtime\" : \"Python\" }, \"result\" : true } } }, \"binary\" : true }, \"result\" : true } } } Response HTTP / 1.1 201 Created Location : https://openeo.org/api/v0.4/jobs/132 OpenEO-Identifier : 132 Start batch processing the job Request POST /jobs/132/results HTTP / 1.1 Create a TMS service Request POST /services HTTP / 1.1 Content-Type : application/json; charset=utf-8 { \"title\" : \"Monthly aggregation on Sentinel 1\" , \"description\" : \"Create a monthly aggregated Sentinel 1 product from a custom Python script.\" , \"process_graph\" : { \"1\" : { \"process_id\" : \"load_result\" , \"arguments\" : { \"id\" : \"132\" }, \"result\" : true } }, \"type\" : \"TMS\" } Response HTTP / 1.1 201 Created Location : https://openeo.org/api/v0.4/services/tms-75ff8c OpenEO-Identifier : tms-75ff8c Requesting the service information Request GET https://openeo.org/api/v0.4/services/tms-75ff8c HTTP / 1.1 Download the data on demand from the WMS Omitted, not part of the openEO API.","title":"Use Case 2"},{"location":"examples-poc/#use-case-3","text":"Compute time series of zonal (regional) statistics of Sentinel 2 imagery over user-specified polygons. Create a batch job Request POST /jobs HTTP / 1.1 Content-Type : application/json; charset=utf-8 { \"title\" : \"Zonal Statistics / Sentinel 2\" , \"description\" : \"Compute time series of zonal (regional) statistics of Sentinel 2 imagery over user-specified polygons.\" , \"process_graph\" : { \"loadco1\" : { \"process_id\" : \"load_collection\" , \"arguments\" : { \"id\" : \"Sentinel-2\" , \"spatial_extent\" : { \"west\" : 16.1 , \"east\" : 16.6 , \"north\" : 48.6 , \"south\" : 47.2 }, \"temporal_extent\" : [ \"2017-01-01\" , \"2017-02-01\" ], \"bands\" : [ \"B8\" ] } }, \"reduce1\" : { \"process_id\" : \"reduce\" , \"arguments\" : { \"data\" : { \"from_node\" : \"loadco1\" }, \"dimension\" : \"spectral\" } }, \"aggreg1\" : { \"process_id\" : \"aggregate_polygon\" , \"arguments\" : { \"data\" : { \"from_node\" : \"reduce1\" }, \"polygons\" : { \"type\" : \"Polygon\" , \"coordinates\" : [ [ [ 16.138916 , 48.320647 ], [ 16.524124 , 48.320647 ], [ 16.524124 , 48.1386 ], [ 16.138916 , 48.1386 ], [ 16.138916 , 48.320647 ] ] ] }, \"reducer\" : { \"callback\" : { \"mean1\" : { \"process_id\" : \"mean\" , \"arguments\" : { \"data\" : { \"from_argument\" : \"data\" } }, \"result\" : true } } } } }, \"savere1\" : { \"process_id\" : \"save_result\" , \"arguments\" : { \"data\" : { \"from_node\" : \"aggreg1\" }, \"format\" : \"JSON\" }, \"result\" : true } } } Response HTTP / 1.1 201 Created Location : https://openeo.org/jobs/133 OpenEO-Identifier : 133 Start batch processing the job Request POST /jobs/133/results HTTP / 1.1 Retrieve download links (after the job has finished) Request GET /jobs/133/results HTTP / 1.1 Response HTTP / 1.1 200 OK Content-Type : application/json; charset=utf-8 Expires : Wed, 01 May 2019 00:00:00 GMT OpenEO-Costs : 0 { \"id\" : \"133\" , \"title\" : \"Zonal Statistics / Sentinel 2\" , \"description\" : \"Compute time series of zonal (regional) statistics of Sentinel 2 imagery over user-specified polygons.\" , \"updated\" : \"2019-02-01T09:36:18Z\" , \"links\" : [ { \"href\" : \"https://cdn.openeo.org/4854b51643548ab8a858e2b8282711d8/result.json\" , \"type\" : \"application/json\" } ] } Download file(s) Request GET https://cdn.openeo.org/4854b51643548ab8a858e2b8282711d8/result.json HTTP / 1.1 Response A JSON file containing the results, content omitted.","title":"Use Case 3"},{"location":"gettingstarted-backends/","text":"Getting started for back-end providers \u00b6 As a back-end provider who wants to provide its datasets, processes and infrastructure to a broader audience through a standardized interface you may want to implement a driver for openEO. First of all, you should go through the list of openEO repositories and check whether there is already a back-end driver that suits your needs. In this case you don't need to develop your own driver, but \"only\" need to ingest your data, adopt your required processes and set-up the infrastructure. Please follow the documentation for the individual driver you want to use. If your preferred technology has no back-end driver yet, you may consider writing your own driver. All software written for openEO should follow the software development guidelines . You certainly need to understand the architecture of openEO and concepts behind jobs , processes and process graphs . This helps you read and understand the API specification . Technical API related documents like CORS and error handing should be read, too. If you do not want to start from scratch, you could try to generate a server stub from the OpenAPI 3.0 -based API specification with the OpenAPI Generator . If you are using Python implement your driver you may re-use some common modules of existing driver implementations: Python Driver Commons You can implement a back-end in iterations. It is recommended to start by implementing the Capabilities microservice. EO Data Discovery , Process Discovery are important for the client libraries to be available, too. Afterwards you should implement Batch Job Management or synchronous data processing . All other microservices can be added later and are not strictly required to run openEO services. Keep in mind that you don't need to implement all endpoints in the first iteration and that you can specify in the Capabilities, which endpoints you are supporting. For example, you could start by implementing the following endpoints in the first iteration: Well-Known Document: GET /.well-known/openeo Capabilities: GET / and GET /output_formats Data discovery: GET /collections and GET /collections/{collection_id} Process discovery: GET /processes Data processing: POST /result Authentication (if required): GET /credentials/basic Afterwards you can already start experimenting with your first process graphs and process EO data with our client libraries on your back-end. More information will follow soon, for example about back-end compliance testing.","title":"Back-end Providers"},{"location":"gettingstarted-backends/#getting-started-for-back-end-providers","text":"As a back-end provider who wants to provide its datasets, processes and infrastructure to a broader audience through a standardized interface you may want to implement a driver for openEO. First of all, you should go through the list of openEO repositories and check whether there is already a back-end driver that suits your needs. In this case you don't need to develop your own driver, but \"only\" need to ingest your data, adopt your required processes and set-up the infrastructure. Please follow the documentation for the individual driver you want to use. If your preferred technology has no back-end driver yet, you may consider writing your own driver. All software written for openEO should follow the software development guidelines . You certainly need to understand the architecture of openEO and concepts behind jobs , processes and process graphs . This helps you read and understand the API specification . Technical API related documents like CORS and error handing should be read, too. If you do not want to start from scratch, you could try to generate a server stub from the OpenAPI 3.0 -based API specification with the OpenAPI Generator . If you are using Python implement your driver you may re-use some common modules of existing driver implementations: Python Driver Commons You can implement a back-end in iterations. It is recommended to start by implementing the Capabilities microservice. EO Data Discovery , Process Discovery are important for the client libraries to be available, too. Afterwards you should implement Batch Job Management or synchronous data processing . All other microservices can be added later and are not strictly required to run openEO services. Keep in mind that you don't need to implement all endpoints in the first iteration and that you can specify in the Capabilities, which endpoints you are supporting. For example, you could start by implementing the following endpoints in the first iteration: Well-Known Document: GET /.well-known/openeo Capabilities: GET / and GET /output_formats Data discovery: GET /collections and GET /collections/{collection_id} Process discovery: GET /processes Data processing: POST /result Authentication (if required): GET /credentials/basic Afterwards you can already start experimenting with your first process graphs and process EO data with our client libraries on your back-end. More information will follow soon, for example about back-end compliance testing.","title":"Getting started for back-end providers"},{"location":"gettingstarted-clients/","text":"Getting started for client developers \u00b6 For easy access to openEO back-ends it is essential to provide client libraries for users in their well-known programming languages or working environments. This can be either a client library for a specific programming language that hides the technical details of the openEO API or an application with a user interface, e.g. a GIS software plugin or a web-based tool. All software written for openEO should follow the software development guidelines . Client library developers \u00b6 If your preferred programming language is not part of the available client libraries you may consider writing your own client library. Our client libraries are basically translating the openEO API into native concepts of the programming languages. Working with openEO should feel like being a first-class citizen of the programming language. Get started by reading the guidelines to develop client libraries , which have been written to ensure the client libraries provide a consistent feel and behavior across programming languages. You certainly need to understand the concepts behind jobs , processes and process graphs . This helps you understand the API specification and related documents. If you do not want to start from scratch, you could try to generate a client library stub from the OpenAPI 3.0 -based API specification with the OpenAPI Generator . Make sure the generated code complies to the client library guidelines mentioned above. More information will follow soon, for example about client testing. Applications and Software plugins \u00b6 Standalone applications and software plugins written in a certain programming language could use the existing client libraries to facilitate access to openEO back-ends. Web applications potentially could use the JavaScript client to access openEO back-ends. Back-Ends may also provide standardized web interfaces such as OGC WMS or OGC WCS to access processed EO data. More information will follow soon...","title":"Client Developers"},{"location":"gettingstarted-clients/#getting-started-for-client-developers","text":"For easy access to openEO back-ends it is essential to provide client libraries for users in their well-known programming languages or working environments. This can be either a client library for a specific programming language that hides the technical details of the openEO API or an application with a user interface, e.g. a GIS software plugin or a web-based tool. All software written for openEO should follow the software development guidelines .","title":"Getting started for client developers"},{"location":"gettingstarted-clients/#client-library-developers","text":"If your preferred programming language is not part of the available client libraries you may consider writing your own client library. Our client libraries are basically translating the openEO API into native concepts of the programming languages. Working with openEO should feel like being a first-class citizen of the programming language. Get started by reading the guidelines to develop client libraries , which have been written to ensure the client libraries provide a consistent feel and behavior across programming languages. You certainly need to understand the concepts behind jobs , processes and process graphs . This helps you understand the API specification and related documents. If you do not want to start from scratch, you could try to generate a client library stub from the OpenAPI 3.0 -based API specification with the OpenAPI Generator . Make sure the generated code complies to the client library guidelines mentioned above. More information will follow soon, for example about client testing.","title":"Client library developers"},{"location":"gettingstarted-clients/#applications-and-software-plugins","text":"Standalone applications and software plugins written in a certain programming language could use the existing client libraries to facilitate access to openEO back-ends. Web applications potentially could use the JavaScript client to access openEO back-ends. Back-Ends may also provide standardized web interfaces such as OGC WMS or OGC WCS to access processed EO data. More information will follow soon...","title":"Applications and Software plugins"},{"location":"gettingstarted-users/","text":"Getting started for users \u00b6 Currently, there are three official client libraries and a web-based interface for openEO. If you are unfamiliar with programming, you could start using the web-based editor for openEO . It supports visual modelling of your algorithms and a simplified JavaScript based access to the openEO workflows and providers. If you are familiar with programming, you could choose a client library for three programming languages: JavaScript (client-side and server-side) Python R Follow the links above to find usage instructions for each of the client libraries. Contribute \u00b6 Didn't find your programming language? You can also access the openEO API implementations directly or start implementing your own client library . If you are missing any functionality in the API feel free to open an issue or actively start proposing API changes as Pull Requests. Make sure to read the API Development Guidelines before. Feel free to contact us for further assistance.","title":"Users"},{"location":"gettingstarted-users/#getting-started-for-users","text":"Currently, there are three official client libraries and a web-based interface for openEO. If you are unfamiliar with programming, you could start using the web-based editor for openEO . It supports visual modelling of your algorithms and a simplified JavaScript based access to the openEO workflows and providers. If you are familiar with programming, you could choose a client library for three programming languages: JavaScript (client-side and server-side) Python R Follow the links above to find usage instructions for each of the client libraries.","title":"Getting started for users"},{"location":"gettingstarted-users/#contribute","text":"Didn't find your programming language? You can also access the openEO API implementations directly or start implementing your own client library . If you are missing any functionality in the API feel free to open an issue or actively start proposing API changes as Pull Requests. Make sure to read the API Development Guidelines before. Feel free to contact us for further assistance.","title":"Contribute"},{"location":"glossary/","text":"Glossary \u00b6 This glossary introduces the major technical terms used in the openEO project. General terms \u00b6 API : application programming interface ( wikipedia ); a communication protocol between client and back-end client : software environment (software) that end-users directly interact with, e.g. R (rstudio), Python (jupyter notebook), and JavaScript (web browser); R and Python are two major data science platforms; JavaScript is a major language for web development (cloud) back-end : server; computer infrastructure (one or more physical computers or virtual machines) used for storing EO data and processing it big Earth observation cloud back-end : server infrastructure where industry and researchers analyse large amounts of EO data Processes and process graphs \u00b6 The terms process and process graph have specific meanings in the openEO API specification. A process is an operation provided by the back end that performs a specific task on a set of parameters and returns a result. An example is computing a statistical operation, such as mean or median, on selected EO data. A process is similar to a function or method in programming languages. A process graph chains specific process calls. Similarly to scripts in the context of programming, process graphs organize and automate the execution of one or more processes that could alternatively be executed individually. In a process graph, processes need to be specific, i.e. concrete values for input parameters need to be specified. These arguments can again be process graphs, scalar values, arrays or objects. EO data (Collections) \u00b6 In our domain, different terms are used to describe EO data(sets). Within openEO, a granule (sometimes also called item or asset in the specification) typically refers to a limited area and a single overpass leading to a very short observation period (seconds) or a temporal aggregation of such data (e.g. for 16-day MODIS composites) while a collection is an aggregation of granules sharing the same product specification. It typically corresponds to the series of products derived from data acquired by a sensor on board a satellite and having the same mode of operation. The CEOS OpenSearch Best Practice Document v1.2 lists synonyms used (by organizations) for: granule : dataset (ESA, ISO 19115), granule (NASA), product (ESA, CNES), scene (JAXA) collection : dataset series (ESA, ISO 19115), collection (CNES, NASA), dataset (JAXA), product (JAXA) In openEO, a back-end offers a set of collections to be processed. All collections can be requested using a client and are described using the STAC metadata specification as STAC collections. A user can load (a subset of) a collection using a special process for processing on the back-end. This process returns a (spatial) data cube. All further processing is then applied to the data cube. Spatial data cubes \u00b6 A spatial data cube is an array with one or more dimensions referring to spatial dimensions. Special cases are raster and vector data cubes. The figure below shows the data of a four-dimensional (8 x 8 x 2 x 2) raster data cube, with dimension names and values: # dimension name dimension values 1 x 288790.5, 288819, 288847.5, 288876, 288904.5, 288933, 288961.5, 288990 2 y 9120747, 9120718, 9120690, 9120661, 9120633, 9120604, 9120576, 9120547 3 band red , green 4 time 2018-02-10 , 2018-02-17 dimensions x and time are aligned along the x-axis; y and band are aligned along the y-axis. Data cubes as defined here have a single value (scalar) for each unique combination of dimension values. The value pointed to by arrows corresponds to the combination of x=288847.5 (red arrow), y=9120661 (yellow arrow), band=red (blue arrow), time=2018-02-17 (green arrow), and its value is 84 (brown arrow). If the data concerns grayscale imagery, we could call this single value a pixel value . One should keep in mind that it is never a tuple of, say, {red, green, blue} values. \"Cell value of a single raster layer\" would be a better analogy; data cube cell value may be a good compromise. A data cube stores some additional properties per dimension such as: name axis / number type extents or nominal dimension values reference systems / projections resolutions Having these properties available allows to easily resample from one data cube to another for example. apply : processes that do not change dimensions \u00b6 Math process that do not reduce do not change anything to the array dimensions. The process apply can be used to apply unary functions such as abs or sqrt to all values in a data cube. The process apply_dimension applies (maps) an n-ary function to a particular dimension. An example would be to apply sort to the time dimension, in order to get every time series sorted. A more realistic example would for instance apply a moving average filter to every time series. An example of apply_dimension to the spatial dimensions is to do a historgram stretch for every spatial (grayscale) image of an image time series. filter : subsetting dimensions by dimension value selection \u00b6 The filter process makes a cube smaller by selecting specific values for a particular dimension. Examples: a band filter that selects the red band a bounding box filter selects a spatial extent reduce : removing dimensions entirely by computation \u00b6 reduce reduces the number of dimensions by computation. For instance, using the reducer proces mean , we can compute the mean of the two time steps, and by that remove the time dimension. Example: a time series reduction may return a regression slope for every (grayscale) pixel time series aggregate : reducing resolution \u00b6 Aggregation computes new values from sets of values that are uniquely assigned to groups. It involves a grouping predicate (e.g. monthly, 100 m x 100 m grid cells, or a set of non-overlapping spatial polygons), and an reducer (e.g., mean ) that computes one or more new values from the original ones. In effect, aggregate combines the following three steps: split the data cube in groups, based on dimension constraints (time intervals, band groups, spatial polygons) apply a reducer to each group (similar to the reduce process, but reducing a group rather than an entire dimension) combine the result to a new data cube, with some dimensions having reduced resolution (or e.g. raster to vector converted) Examples: a weekly time series may be aggregated to monthly values by computing the mean for all values in a month (grouping predicate: months) spatial aggregation involves computing e.g. mean pixel values on a 100 x 100 m grid, from 10 m x 10 m pixels, where each original pixel is assigned uniquely to a larger pixel (grouping predicate: 100 m x 100 m grid cells) resample : changing data cube geometry \u00b6 Resampling considers the case where we have data at one resolution and coordinate reference system, and need values at another. In case we have values at a 100 m x 100 m grid and need values at a 10 m x 10 m grid, the original values will be reused many times, and may be simply assigned to the nearest high resolution grid cells (nearest neighbor method), or may be interpolated using various methods (e.g. by bilinear interpolation). This is often called upsampling or upscaling . Resampling from finer to coarser grid is a special case of aggregation often called downsampling or downscaling . When the target grid or time series has a lower resolution (larger grid cells) or lower frequency (longer time intervals) than the source grid, aggregation might be used for resampling. For example, if the resolutions are similar, (e.g. the source collection provides 10 day intervals and the target needs values for 16 day intervals), then some form of interpolation may be more appropriate than aggregation as defined here. User-defined function (UDF) \u00b6 The abbreviation UDF stands for user-defined function . With this concept, users are able to upload custom code and have it executed e.g. for every pixel of a scene, or applied to a particular dimension or set of dimensions, allowing custom server-side calculations. See the section on UDFs for more information. Data Processing modes \u00b6 Process graphs can be processed in three different ways: Results can be pre-computed by creating a batch job using POST /jobs . They are submitted to the back-end's processing system, but will remain inactive until POST /jobs/{job_id}/results has been called. They will run only once and store results after execution. Results can be downloaded. Batch jobs are typically time consuming so that user interaction is not possible. This is the only mode that allows to get an estimate about time, volume and costs beforehand. A more dynamic way of processing and accessing data is to create a secondary web service . They allow web-based access using different protocols such as OGC WMS , OGC WCS or XYZ tiles . These protocols usually allow users to change the viewing extent or level of detail (zoom level). Therefore, computations often run on demand so that the requested data is calculated during the request. Back-ends should make sure to cache processed data to avoid additional/high costs and reduce waiting times for the user. Process graphs can also be executed on-demand (i.e. synchronously) by sending the process graph to POST /result . Results are delivered with the request itself and no job is created. Only lightweight computations, for example small previews, should be executed using this approach as timeouts are to be expected for long-polling HTTP requests .","title":"Glossary"},{"location":"glossary/#glossary","text":"This glossary introduces the major technical terms used in the openEO project.","title":"Glossary"},{"location":"glossary/#general-terms","text":"API : application programming interface ( wikipedia ); a communication protocol between client and back-end client : software environment (software) that end-users directly interact with, e.g. R (rstudio), Python (jupyter notebook), and JavaScript (web browser); R and Python are two major data science platforms; JavaScript is a major language for web development (cloud) back-end : server; computer infrastructure (one or more physical computers or virtual machines) used for storing EO data and processing it big Earth observation cloud back-end : server infrastructure where industry and researchers analyse large amounts of EO data","title":"General terms"},{"location":"glossary/#processes-and-process-graphs","text":"The terms process and process graph have specific meanings in the openEO API specification. A process is an operation provided by the back end that performs a specific task on a set of parameters and returns a result. An example is computing a statistical operation, such as mean or median, on selected EO data. A process is similar to a function or method in programming languages. A process graph chains specific process calls. Similarly to scripts in the context of programming, process graphs organize and automate the execution of one or more processes that could alternatively be executed individually. In a process graph, processes need to be specific, i.e. concrete values for input parameters need to be specified. These arguments can again be process graphs, scalar values, arrays or objects.","title":"Processes and process graphs"},{"location":"glossary/#eo-data-collections","text":"In our domain, different terms are used to describe EO data(sets). Within openEO, a granule (sometimes also called item or asset in the specification) typically refers to a limited area and a single overpass leading to a very short observation period (seconds) or a temporal aggregation of such data (e.g. for 16-day MODIS composites) while a collection is an aggregation of granules sharing the same product specification. It typically corresponds to the series of products derived from data acquired by a sensor on board a satellite and having the same mode of operation. The CEOS OpenSearch Best Practice Document v1.2 lists synonyms used (by organizations) for: granule : dataset (ESA, ISO 19115), granule (NASA), product (ESA, CNES), scene (JAXA) collection : dataset series (ESA, ISO 19115), collection (CNES, NASA), dataset (JAXA), product (JAXA) In openEO, a back-end offers a set of collections to be processed. All collections can be requested using a client and are described using the STAC metadata specification as STAC collections. A user can load (a subset of) a collection using a special process for processing on the back-end. This process returns a (spatial) data cube. All further processing is then applied to the data cube.","title":"EO data (Collections)"},{"location":"glossary/#spatial-data-cubes","text":"A spatial data cube is an array with one or more dimensions referring to spatial dimensions. Special cases are raster and vector data cubes. The figure below shows the data of a four-dimensional (8 x 8 x 2 x 2) raster data cube, with dimension names and values: # dimension name dimension values 1 x 288790.5, 288819, 288847.5, 288876, 288904.5, 288933, 288961.5, 288990 2 y 9120747, 9120718, 9120690, 9120661, 9120633, 9120604, 9120576, 9120547 3 band red , green 4 time 2018-02-10 , 2018-02-17 dimensions x and time are aligned along the x-axis; y and band are aligned along the y-axis. Data cubes as defined here have a single value (scalar) for each unique combination of dimension values. The value pointed to by arrows corresponds to the combination of x=288847.5 (red arrow), y=9120661 (yellow arrow), band=red (blue arrow), time=2018-02-17 (green arrow), and its value is 84 (brown arrow). If the data concerns grayscale imagery, we could call this single value a pixel value . One should keep in mind that it is never a tuple of, say, {red, green, blue} values. \"Cell value of a single raster layer\" would be a better analogy; data cube cell value may be a good compromise. A data cube stores some additional properties per dimension such as: name axis / number type extents or nominal dimension values reference systems / projections resolutions Having these properties available allows to easily resample from one data cube to another for example.","title":"Spatial data cubes"},{"location":"glossary/#apply-processes-that-do-not-change-dimensions","text":"Math process that do not reduce do not change anything to the array dimensions. The process apply can be used to apply unary functions such as abs or sqrt to all values in a data cube. The process apply_dimension applies (maps) an n-ary function to a particular dimension. An example would be to apply sort to the time dimension, in order to get every time series sorted. A more realistic example would for instance apply a moving average filter to every time series. An example of apply_dimension to the spatial dimensions is to do a historgram stretch for every spatial (grayscale) image of an image time series.","title":"apply: processes that do not change dimensions"},{"location":"glossary/#filter-subsetting-dimensions-by-dimension-value-selection","text":"The filter process makes a cube smaller by selecting specific values for a particular dimension. Examples: a band filter that selects the red band a bounding box filter selects a spatial extent","title":"filter: subsetting dimensions by dimension value selection"},{"location":"glossary/#reduce-removing-dimensions-entirely-by-computation","text":"reduce reduces the number of dimensions by computation. For instance, using the reducer proces mean , we can compute the mean of the two time steps, and by that remove the time dimension. Example: a time series reduction may return a regression slope for every (grayscale) pixel time series","title":"reduce: removing dimensions entirely by computation"},{"location":"glossary/#aggregate-reducing-resolution","text":"Aggregation computes new values from sets of values that are uniquely assigned to groups. It involves a grouping predicate (e.g. monthly, 100 m x 100 m grid cells, or a set of non-overlapping spatial polygons), and an reducer (e.g., mean ) that computes one or more new values from the original ones. In effect, aggregate combines the following three steps: split the data cube in groups, based on dimension constraints (time intervals, band groups, spatial polygons) apply a reducer to each group (similar to the reduce process, but reducing a group rather than an entire dimension) combine the result to a new data cube, with some dimensions having reduced resolution (or e.g. raster to vector converted) Examples: a weekly time series may be aggregated to monthly values by computing the mean for all values in a month (grouping predicate: months) spatial aggregation involves computing e.g. mean pixel values on a 100 x 100 m grid, from 10 m x 10 m pixels, where each original pixel is assigned uniquely to a larger pixel (grouping predicate: 100 m x 100 m grid cells)","title":"aggregate: reducing resolution"},{"location":"glossary/#resample-changing-data-cube-geometry","text":"Resampling considers the case where we have data at one resolution and coordinate reference system, and need values at another. In case we have values at a 100 m x 100 m grid and need values at a 10 m x 10 m grid, the original values will be reused many times, and may be simply assigned to the nearest high resolution grid cells (nearest neighbor method), or may be interpolated using various methods (e.g. by bilinear interpolation). This is often called upsampling or upscaling . Resampling from finer to coarser grid is a special case of aggregation often called downsampling or downscaling . When the target grid or time series has a lower resolution (larger grid cells) or lower frequency (longer time intervals) than the source grid, aggregation might be used for resampling. For example, if the resolutions are similar, (e.g. the source collection provides 10 day intervals and the target needs values for 16 day intervals), then some form of interpolation may be more appropriate than aggregation as defined here.","title":"resample: changing data cube geometry"},{"location":"glossary/#user-defined-function-udf","text":"The abbreviation UDF stands for user-defined function . With this concept, users are able to upload custom code and have it executed e.g. for every pixel of a scene, or applied to a particular dimension or set of dimensions, allowing custom server-side calculations. See the section on UDFs for more information.","title":"User-defined function (UDF)"},{"location":"glossary/#data-processing-modes","text":"Process graphs can be processed in three different ways: Results can be pre-computed by creating a batch job using POST /jobs . They are submitted to the back-end's processing system, but will remain inactive until POST /jobs/{job_id}/results has been called. They will run only once and store results after execution. Results can be downloaded. Batch jobs are typically time consuming so that user interaction is not possible. This is the only mode that allows to get an estimate about time, volume and costs beforehand. A more dynamic way of processing and accessing data is to create a secondary web service . They allow web-based access using different protocols such as OGC WMS , OGC WCS or XYZ tiles . These protocols usually allow users to change the viewing extent or level of detail (zoom level). Therefore, computations often run on demand so that the requested data is calculated during the request. Back-ends should make sure to cache processed data to avoid additional/high costs and reduce waiting times for the user. Process graphs can also be executed on-demand (i.e. synchronously) by sending the process graph to POST /result . Results are delivered with the request itself and no job is created. Only lightweight computations, for example small previews, should be executed using this approach as timeouts are to be expected for long-polling HTTP requests .","title":"Data Processing modes"},{"location":"guidelines-api/","text":"API Development Guidelines \u00b6 To provide the smoothest possible experience, it's important to have these APIs follow consistent design guidelines, thus making using them easy and intuitive. Language \u00b6 Language \u00b6 Generally, English language MUST be used for all names, documentation etc. In the specification the key words \u201cMUST\u201d, \u201cMUST NOT\u201d, \u201cREQUIRED\u201d, \u201cSHALL\u201d, \u201cSHALL NOT\u201d, \u201cSHOULD\u201d, \u201cSHOULD NOT\u201d, \u201cRECOMMENDED\u201d, \u201cMAY\u201d, and \u201cOPTIONAL\u201d in this document are to be interpreted as described in RFC 2119 . Casing \u00b6 Unless otherwise stated the API works case sensitive. All names SHOULD be written in snake case, i.e. words are separated with one underscore character (_) and no spaces, with all letters lowercased. Example: hello_world . This applies particularly to endpoints and JSON property names. HTTP header fields follow their respective casing conventions, e.g. Content-Type or OpenEO-Costs , despite being case-insensitive according to RFC 7230 . Technical requirements \u00b6 HTTP \u00b6 The API developed by the openEO project uses HTTP REST Level 2 for communication between client and back-end server. Public APIs MUST be available via HTTPS only and all inbound calls MUST be HTTPS. Verbs Endpoints SHOULD use meaningful HTTP verbs (e.g. GET, POST, PUT, PATCH, DELETE). If there is a need to transfer big chunks of data via GET requests, POST requests MAY be used as a replacement as they support to send data via request body. Unless otherwise stated, PATCH requests are only defined to work on the direct (first-level) children of the full JSON object. Therefore, changing a property on a deeper level of the full JSON object always requires to send the whole JSON object defined by the first-level property. Resource naming Naming of endpoints SHOULD follow the REST principles. Therefore, endpoints SHOULD be centered around resources. Resource identifiers MUST be named with a noun in plural form except for single actions that can not be modelled with the regular HTTP verbs. Single actions MUST be single endpoint with a single HTTP verb (POST is RECOMMENDED) and no other endpoints beneath it. Cross-Origin Resource Sharing (CORS) All back-end providers SHOULD support CORS. More information can be found in the corresponding section . Status codes and error handling The success of requests MUST be indicated using HTTP status codes according to RFC 7231 . More information can be found in the section about status und error handling . Requests and response formats \u00b6 JSON Web-based communication, especially when a mobile or other low-bandwidth client is involved, has moved quickly in the direction of JSON for a variety of reasons, including its tendency to be lighter weight and its ease of consumption with JavaScript-based clients. Therefore, services SHOULD use JSON as the default encoding. Other response formats can be requested using Content Negotiation . Clients and servers MUST NOT rely on the order in which properties appears in JSON responses. When supported by the service, clients MAY request that array elements be returned in a specific order. Collections SHOULD NOT include nested JSON objects if those information can be requested from the individual resources. Temporal data Date, time, intervals and durations MUST be formatted based on ISO 8601 or any profile ( RFC 3339 is strongly recommended) if there is an appropriate encoding available in the standard. All temporal data MUST by specified based on the Gregorian calendar.","title":"API Specification"},{"location":"guidelines-api/#api-development-guidelines","text":"To provide the smoothest possible experience, it's important to have these APIs follow consistent design guidelines, thus making using them easy and intuitive.","title":"API Development Guidelines"},{"location":"guidelines-api/#language","text":"","title":"Language"},{"location":"guidelines-api/#language_1","text":"Generally, English language MUST be used for all names, documentation etc. In the specification the key words \u201cMUST\u201d, \u201cMUST NOT\u201d, \u201cREQUIRED\u201d, \u201cSHALL\u201d, \u201cSHALL NOT\u201d, \u201cSHOULD\u201d, \u201cSHOULD NOT\u201d, \u201cRECOMMENDED\u201d, \u201cMAY\u201d, and \u201cOPTIONAL\u201d in this document are to be interpreted as described in RFC 2119 .","title":"Language"},{"location":"guidelines-api/#casing","text":"Unless otherwise stated the API works case sensitive. All names SHOULD be written in snake case, i.e. words are separated with one underscore character (_) and no spaces, with all letters lowercased. Example: hello_world . This applies particularly to endpoints and JSON property names. HTTP header fields follow their respective casing conventions, e.g. Content-Type or OpenEO-Costs , despite being case-insensitive according to RFC 7230 .","title":"Casing"},{"location":"guidelines-api/#technical-requirements","text":"","title":"Technical requirements"},{"location":"guidelines-api/#http","text":"The API developed by the openEO project uses HTTP REST Level 2 for communication between client and back-end server. Public APIs MUST be available via HTTPS only and all inbound calls MUST be HTTPS.","title":"HTTP"},{"location":"guidelines-api/#requests-and-response-formats","text":"","title":"Requests and response formats"},{"location":"guidelines-clients/","text":"Client library development guidelines \u00b6 This is a proposal for workflows that client libraries should support to make the experience with each library similar and users can easily adopt examples and workflows. For best experience libraries should still embrace best practices common in their environments. This means clients can... choose which kind of casing they use (see below). feel free to implement aliases for methods. Conventions \u00b6 Casing \u00b6 Clients can use snake_case , camelCase or any method used commonly in their environment. For example, the API request to get a list of collections can either be names get_collections or getCollections . This applies for all names, including scopes, method names and parameters. Scopes \u00b6 Each method belongs to a scope. To achieve this in object-oriented (OO) programming languages, methods would be part of a class. If programming languages don't support scopes, you may need to simulate it somehow to prevent name collisions, e.g. by adding a prefix to the method names (like in the \"procedural style\" example below). Best practices for this will likely evolve over time. Example for the clientVersion method in openEO : Procedural style: openeo_client_version() Object-oriented style: OpenEO obj = new OpenEO (); obj . clientVersion (); If you can't store scope data in an object, you may need to pass these information as argument(s) to the method. Example: Procedural style: $connection = openeo_connect(\"https://openeo.org\"); openeo_capabilities($connection); Object-oriented style: OpenEO obj = new OpenEO (); Connection con = obj . connect ( \"https://openeo.org\" ); con . capabilities (); Scope categories \u00b6 Each scope is assigned to a scope category, of which there are three: Root category: Contains only the scope openEO . API category: Mostly methods hiding API calls to the back-ends. Methods may be implemented asynchronously. Contains the scopes Connection , File , Job , ProcessGraph , Service . Content : Mostly methods hiding the complexity of response content. Methods are usually implemented synchronously. Currently contains only the scope Capabilities . Method names should be prefixed if name collisions are likely. Method names across ALL the scopes that belong to the root or API categories MUST be unique. This is the case because the parameter in hasFeature(method_name) must be unambiguous. Method names of scopes in the Content category may collide with method names of scopes in the root / API categories and names should be prefixed if collisions of names between different scope categories are to be expected. Parameters \u00b6 The parameters usually follow the request schemes in the openAPI specification. The parameters should follow their characteristics, for example regarding the default values. Some methods have a long list of (optional) parameters. This is easy to implement in languages that support named parameters such as R. For example, creating a job in R with a budget would lead to this method call: createJob ( process_graph = { ... }, budget = 123 ) Other languages that only support non-named parameters (i.e. the order of parameters is fixed) need to fill many parameters with default values, which is not convenient for a user. The example above in PHP would be: createJob({...}, null, null, null, null, null, 123) To avoid such method calls client developers should consider to pass either an instance of a class, which contains all parameters as member variables or the required parameters directly and the optional parameters as a dictionary (see example below). This basically emulates named parameters. The member variables / dictionary keys should use the same names as the parameters. The exemplary method call in PHP could be improved as follows: createJob({...}, [budget => 123]) Method mappings \u00b6 Note: Subscriptions and some scopes for response JSON objects are still missing. We are open for proposals. Parameters with a leading ? are optional. Scope: openEO (root category) \u00b6 Description Client method Connect to a back-end, including authentication. Returns Connection . connect(url, ?authType, ?authOptions) Get client library version. clientVersion() Parameters authType in connect : null , basic or oidc (non-exclusive). Defaults to null (no authentication). authOptions in connect : May hold additional data for authentication, for example a username and password for basic authentication. Scope: Connection (API category) \u00b6 Description API Request Client method Get the capabilities of the back-end. Returns Capabilities . GET / capabilities() List the supported output file formats. GET /output_formats listFileTypes() List the supported secondary service types. GET /service_types listServiceTypes() List all collections available on the back-end. GET /collections listCollections() Get information about a single collection. GET /collections/{collection_id} describeCollection(collection_id) List all processes available on the back-end. GET /processes listProcesses() Authenticate with OpenID Connect (if not specified in connect ). GET /credentials/oidc authenticateOIDC(?options) Authenticate with HTTP Basic (if not specified in connect ). GET /credentials/basic authenticateBasic(username, password) Get information about the authenticated user. GET /me describeAccount() Lists all files from a user. Returns a list of File . GET /files/{user_id} listFiles(?userId) Opens a (existing or non-existing) file without reading any information. Returns a File . None openFile(path, ?userId) Validates a process graph. POST /validate validateProcessGraph(processGraph) Lists all process graphs of the authenticated user. Returns a list of ProcessGraph . GET /process_graphs listProcessGraphs() Creates a new stored process graph. Returns a ProcessGraph . POST /process_graphs createProcessGraph(processGraph, ?title, ?description) Get all information about a stored process graph. Returns a ProcessGraph . GET /process_graphs/{process_graph_id} getJobById(id) Executes a process graph synchronously. POST /result computeResult(processGraph, ?outputFormat, ?outputParameters, ?budget) Lists all jobs of the authenticated user. Returns a list of Job . GET /jobs listJobs() Creates a new job. Returns a Job . POST /jobs createJob(processGraph, ?outputFormat, ?outputParameters, ?title, ?description, ?plan, ?budget, ?additional) Get all information about a job. Returns a Job . GET /jobs/{job_id} getJobById(id) Lists all secondary services of the authenticated user. Returns a list of Service . GET /services listServices() Creates a new secondary service. Returns a Service . POST /services createService(processGraph, type, ?title, ?description, ?enabled, ?parameters, ?plan, ?budget) Get all information about a service. Returns a Service . GET /services/{service_id} getServiceById(id) Parameters userId in listFiles and createFile : Defaults to the user id of the authenticated user. options in authenticateOIDC : May hold additional data required for OpenID connect authentication. Scope Capabilities (Content category) \u00b6 Should be prefixed with Capabilities if collisions of names between different scope categories are to be expected. Description Field Client method Get the implemented openEO version. api_version apiVersion() Get the back-end version. backend_version backendVersion() Get the name of the back-end. title title() Get the description of the back-end. description description() List all supported features / endpoints. endpoints listFeatures() Check whether a feature / endpoint is supported. endpoints > ... hasFeature(methodName) Get default billing currency. billing > currency currency() List all billing plans. billing > plans listPlans() Parameters methodName in hasFeature : The name of a client method in any of the scopes that are part of the API category. E.g. hasFeature(\"describeAccount\") checks whether the GET /me endpoint is contained in the capabilities response's endpoints object. Scope: File (API category) \u00b6 The File scope internally knows the user_id and the path . Description API Request Client method Download a user file. GET /files/{user_id}/{path} downloadFile(target) Upload a user file. PUT /files/{user_id}/{path} uploadFile(source) Delete a user file. DELETE /files/{user_id}/{path} deleteFile() Parameters target in downloadFile : Path to a local file or folder. Scope: Job (API category) \u00b6 The Job scope internally knows the job_id . Description API Request Client method Get (and update on client-side) all job information. GET /jobs/{job_id} describeJob() Modify a job at the back-end. PATCH /jobs/{job_id} updateJob(?processGraph, ?outputFormat, ?outputParameters, ?title, ?description, ?plan, ?budget, ?additional) Delete a job DELETE /jobs/{job_id} deleteJob() Calculate an time/cost estimate for a job. GET /jobs/{job_id}/estimate estimateJob() Start / queue a job for processing. POST /jobs/{job_id}/results startJob() Stop / cancel job processing. DELETE /jobs/{job_id}/results stopJob() Get document with download links. GET /jobs/{job_id}/results listResults(?type) Download job results. GET /jobs/{job_id}/results > ... downloadResults(target) Parameters type in listResult : Either json or metalink (non-exclusive). Defaults to json . target in downloadResults : Path to a local folder. Scope: ProcessGraph (API category) \u00b6 The ProcessGraph scope internally knows the process_graph_id . Description API Request Client method Get (and update on client-side) all information about a stored process graph. GET /process_graphs/{process_graph_id} describeProcessGraph() Modify a stored process graph at the back-end. PATCH /process_graphs/{process_graph_id} updateProcessGraph(?processGraph, ?title, ?description) Delete a stored process graph. DELETE /process_graphs/{process_graph_id} deleteProcessGraph() Scope: Service (API category) \u00b6 The Service scope internally knows the service_id . Description API Request Client method Get (and update on client-side) all information about a secondary web service. GET /services/{service_id} describeService() Modify a secondary web service at the back-end. PATCH /services/{service_id} updateService(?processGraph, ?title, ?description, ?enabled, ?parameters, ?plan, ?budget) Delete a secondary web service. DELETE /services/{service_id} deleteService() Processes \u00b6 The processes a back-end supports may be offered by the clients as methods in its own scope. The method names should follow the process names, but the conventions listed above can be applied here as well, e.g. converting filter_bands to filterBands . As parameters have no natural or technical ordering in the JSON objects, clients must come up with a reasonable ordering of parameters if required. This could be inspired by existing clients. The way of building a process graph from processes heavily depends on the technical capabilities of the programming language. Therefore it may differ between the client libraries. Follow the best practices of the programming language, e.g. support method chaining if possible. Workflow example \u00b6 Some simplified example workflows using different programming styles are listed below. The following steps are executed: Loading the client library. Connecting to a back-end and authenticating with username and password via OpenID Connect. Requesting the capabilities and showing the implemented openEO version of the back-end. Showing information about the \"Sentinel-2A\" collection. Showing information about all processes supported by the back-end. Building a simple process graph. Creating a job. Pushing the job to the processing queue. After a while, showing the job details, e.g. checking the job status. Once processing is finished, downloading the job results to the local directory /tmp/job_results/ . Please note that the examples below do not comply to the latest process specification. They are meant to show the differences in client development, but are no working examples! R (functional style) \u00b6 library ( openeo ) con = connect ( \"https://openeo.org\" , \"username\" , \"password\" ) cap = capabilities () cap %>% apiVersion () con %>% describeCollection ( \"Sentinel-2A\" ) con %>% listProcesses () processgraph = process ( \"load_collection\" , id = \"Sentinel-2A\" ) %>% process ( \"filter_bbox\" , west = 672000 , south = 5181000 , east = 652000 , north = 5161000 , crs = \"EPSG:32632\" ) %>% process ( \"filter_temporal\" , extent = c ( \"2017-01-01T00:00:00Z\" , \"2017-01-31T23:59:59Z\" )) %>% process ( \"ndvi\" , nir = \"B4\" , red = \"B8A\" ) %>% process ( \"min_time\" ) job = con %>% createJob ( processgraph ) job %>% startJob () job %>% describeJob () job %>% downloadResults ( \"/tmp/job_results/\" ) Python (mixed style) \u00b6 import openeo con = openeo . connect ( \"https://openeo.org\" , \"username\" , \"password\" ) cap = con . capabilities () print ( cap . api_version ()) print ( con . describe_collection ( \"Sentinel-2A\" )) print ( con . list_processes ()) processes = con . get_processes () pg = processes . load_collection ( id = \"Sentinel-2A\" ) pg = processes . filter_bbox ( pg , west = 672000 , south = 5181000 , east = 652000 , north = 5161000 , crs = \"EPSG:32632\" ) pg = processes . filter_temporal ( pg , extent = [ \"2017-01-01T00:00:00Z\" , \"2017-01-31T23:59:59Z\" ]) pg = processes . ndvi ( pg , nir = \"B4\" , red = \"B8A\" ) pg = processes . min_time ( pg ) job = con . create_job ( pg . graph ) job . start_job () print job . describe_job () job . download_results ( \"/tmp/job_results/\" ) Java (object oriented style) \u00b6 import org.openeo.* ; OpenEO obj = new OpenEO (); Connection con = obj . connect ( \"https://openeo.org\" , \"username\" , \"password\" ); Capabilities cap = con . capabilities (); System . out . println ( cap . apiVersion ()); System . out . println ( con . describeCollection ( \"Sentinel-2A\" )); System . out . println ( con . listProcesses ()); ProcessGraphBuilder pgb = con . getProcessGraphBuilder () // Chain processes... ProcessGraph processGraph = pgb . buildProcessGraph (); Job job = con . createJob ( processGraph ); job . startJob (); System . out . println ( job . describeJob ()); job . downloadResults ( \"/tmp/job_results/\" ); PHP (procedural style) \u00b6 require_once(\"/path/to/openeo.php\"); $connection = openeo_connect(\"http://openeo.org\", \"username\", \"password\"); $capabilities = openeo_capabilities($connection); echo openeo_api_version($capabilites); echo openeo_describe_collection($connection, \"Sentinel-2A\"); echo openeo_list_processes($connection); $pg = openeo_process($pg, \"load_collection\", [\"id\" => \"Sentinel-2A\"]); $pg = openeo_process($pg, \"filter_bbox\", [\"west\" => 672000, \"south\" => 5181000, \"east\" => 652000, \"north\" => 5161000, \"crs\" => \"EPSG:32632\"]); $pg = openeo_process($pg, \"filter_temporal\", [\"extent\" => [\"2017-01-01T00:00:00Z\", \"2017-01-31T23:59:59Z\"]]); $pg = openeo_process($pg, \"ndvi\", [\"red\" => \"B4\", \"nir\" => \"B8A\"]); $pg = openeo_process($pg, \"min_time\"); $job = openeo_create_job($connection, $pg); openeo_start_job($job); echo openeo_describe_job($job); openeo_download_results($job, \"/tmp/job_results/\");","title":"Client Library Development"},{"location":"guidelines-clients/#client-library-development-guidelines","text":"This is a proposal for workflows that client libraries should support to make the experience with each library similar and users can easily adopt examples and workflows. For best experience libraries should still embrace best practices common in their environments. This means clients can... choose which kind of casing they use (see below). feel free to implement aliases for methods.","title":"Client library development guidelines"},{"location":"guidelines-clients/#conventions","text":"","title":"Conventions"},{"location":"guidelines-clients/#casing","text":"Clients can use snake_case , camelCase or any method used commonly in their environment. For example, the API request to get a list of collections can either be names get_collections or getCollections . This applies for all names, including scopes, method names and parameters.","title":"Casing"},{"location":"guidelines-clients/#scopes","text":"Each method belongs to a scope. To achieve this in object-oriented (OO) programming languages, methods would be part of a class. If programming languages don't support scopes, you may need to simulate it somehow to prevent name collisions, e.g. by adding a prefix to the method names (like in the \"procedural style\" example below). Best practices for this will likely evolve over time. Example for the clientVersion method in openEO : Procedural style: openeo_client_version() Object-oriented style: OpenEO obj = new OpenEO (); obj . clientVersion (); If you can't store scope data in an object, you may need to pass these information as argument(s) to the method. Example: Procedural style: $connection = openeo_connect(\"https://openeo.org\"); openeo_capabilities($connection); Object-oriented style: OpenEO obj = new OpenEO (); Connection con = obj . connect ( \"https://openeo.org\" ); con . capabilities ();","title":"Scopes"},{"location":"guidelines-clients/#scope-categories","text":"Each scope is assigned to a scope category, of which there are three: Root category: Contains only the scope openEO . API category: Mostly methods hiding API calls to the back-ends. Methods may be implemented asynchronously. Contains the scopes Connection , File , Job , ProcessGraph , Service . Content : Mostly methods hiding the complexity of response content. Methods are usually implemented synchronously. Currently contains only the scope Capabilities . Method names should be prefixed if name collisions are likely. Method names across ALL the scopes that belong to the root or API categories MUST be unique. This is the case because the parameter in hasFeature(method_name) must be unambiguous. Method names of scopes in the Content category may collide with method names of scopes in the root / API categories and names should be prefixed if collisions of names between different scope categories are to be expected.","title":"Scope categories"},{"location":"guidelines-clients/#parameters","text":"The parameters usually follow the request schemes in the openAPI specification. The parameters should follow their characteristics, for example regarding the default values. Some methods have a long list of (optional) parameters. This is easy to implement in languages that support named parameters such as R. For example, creating a job in R with a budget would lead to this method call: createJob ( process_graph = { ... }, budget = 123 ) Other languages that only support non-named parameters (i.e. the order of parameters is fixed) need to fill many parameters with default values, which is not convenient for a user. The example above in PHP would be: createJob({...}, null, null, null, null, null, 123) To avoid such method calls client developers should consider to pass either an instance of a class, which contains all parameters as member variables or the required parameters directly and the optional parameters as a dictionary (see example below). This basically emulates named parameters. The member variables / dictionary keys should use the same names as the parameters. The exemplary method call in PHP could be improved as follows: createJob({...}, [budget => 123])","title":"Parameters"},{"location":"guidelines-clients/#method-mappings","text":"Note: Subscriptions and some scopes for response JSON objects are still missing. We are open for proposals. Parameters with a leading ? are optional.","title":"Method mappings"},{"location":"guidelines-clients/#scope-openeo-root-category","text":"Description Client method Connect to a back-end, including authentication. Returns Connection . connect(url, ?authType, ?authOptions) Get client library version. clientVersion()","title":"Scope: openEO (root category)"},{"location":"guidelines-clients/#scope-connection-api-category","text":"Description API Request Client method Get the capabilities of the back-end. Returns Capabilities . GET / capabilities() List the supported output file formats. GET /output_formats listFileTypes() List the supported secondary service types. GET /service_types listServiceTypes() List all collections available on the back-end. GET /collections listCollections() Get information about a single collection. GET /collections/{collection_id} describeCollection(collection_id) List all processes available on the back-end. GET /processes listProcesses() Authenticate with OpenID Connect (if not specified in connect ). GET /credentials/oidc authenticateOIDC(?options) Authenticate with HTTP Basic (if not specified in connect ). GET /credentials/basic authenticateBasic(username, password) Get information about the authenticated user. GET /me describeAccount() Lists all files from a user. Returns a list of File . GET /files/{user_id} listFiles(?userId) Opens a (existing or non-existing) file without reading any information. Returns a File . None openFile(path, ?userId) Validates a process graph. POST /validate validateProcessGraph(processGraph) Lists all process graphs of the authenticated user. Returns a list of ProcessGraph . GET /process_graphs listProcessGraphs() Creates a new stored process graph. Returns a ProcessGraph . POST /process_graphs createProcessGraph(processGraph, ?title, ?description) Get all information about a stored process graph. Returns a ProcessGraph . GET /process_graphs/{process_graph_id} getJobById(id) Executes a process graph synchronously. POST /result computeResult(processGraph, ?outputFormat, ?outputParameters, ?budget) Lists all jobs of the authenticated user. Returns a list of Job . GET /jobs listJobs() Creates a new job. Returns a Job . POST /jobs createJob(processGraph, ?outputFormat, ?outputParameters, ?title, ?description, ?plan, ?budget, ?additional) Get all information about a job. Returns a Job . GET /jobs/{job_id} getJobById(id) Lists all secondary services of the authenticated user. Returns a list of Service . GET /services listServices() Creates a new secondary service. Returns a Service . POST /services createService(processGraph, type, ?title, ?description, ?enabled, ?parameters, ?plan, ?budget) Get all information about a service. Returns a Service . GET /services/{service_id} getServiceById(id)","title":"Scope: Connection (API category)"},{"location":"guidelines-clients/#scope-capabilities-content-category","text":"Should be prefixed with Capabilities if collisions of names between different scope categories are to be expected. Description Field Client method Get the implemented openEO version. api_version apiVersion() Get the back-end version. backend_version backendVersion() Get the name of the back-end. title title() Get the description of the back-end. description description() List all supported features / endpoints. endpoints listFeatures() Check whether a feature / endpoint is supported. endpoints > ... hasFeature(methodName) Get default billing currency. billing > currency currency() List all billing plans. billing > plans listPlans()","title":"Scope Capabilities (Content category)"},{"location":"guidelines-clients/#scope-file-api-category","text":"The File scope internally knows the user_id and the path . Description API Request Client method Download a user file. GET /files/{user_id}/{path} downloadFile(target) Upload a user file. PUT /files/{user_id}/{path} uploadFile(source) Delete a user file. DELETE /files/{user_id}/{path} deleteFile()","title":"Scope: File (API category)"},{"location":"guidelines-clients/#scope-job-api-category","text":"The Job scope internally knows the job_id . Description API Request Client method Get (and update on client-side) all job information. GET /jobs/{job_id} describeJob() Modify a job at the back-end. PATCH /jobs/{job_id} updateJob(?processGraph, ?outputFormat, ?outputParameters, ?title, ?description, ?plan, ?budget, ?additional) Delete a job DELETE /jobs/{job_id} deleteJob() Calculate an time/cost estimate for a job. GET /jobs/{job_id}/estimate estimateJob() Start / queue a job for processing. POST /jobs/{job_id}/results startJob() Stop / cancel job processing. DELETE /jobs/{job_id}/results stopJob() Get document with download links. GET /jobs/{job_id}/results listResults(?type) Download job results. GET /jobs/{job_id}/results > ... downloadResults(target)","title":"Scope: Job (API category)"},{"location":"guidelines-clients/#scope-processgraph-api-category","text":"The ProcessGraph scope internally knows the process_graph_id . Description API Request Client method Get (and update on client-side) all information about a stored process graph. GET /process_graphs/{process_graph_id} describeProcessGraph() Modify a stored process graph at the back-end. PATCH /process_graphs/{process_graph_id} updateProcessGraph(?processGraph, ?title, ?description) Delete a stored process graph. DELETE /process_graphs/{process_graph_id} deleteProcessGraph()","title":"Scope: ProcessGraph (API category)"},{"location":"guidelines-clients/#scope-service-api-category","text":"The Service scope internally knows the service_id . Description API Request Client method Get (and update on client-side) all information about a secondary web service. GET /services/{service_id} describeService() Modify a secondary web service at the back-end. PATCH /services/{service_id} updateService(?processGraph, ?title, ?description, ?enabled, ?parameters, ?plan, ?budget) Delete a secondary web service. DELETE /services/{service_id} deleteService()","title":"Scope: Service (API category)"},{"location":"guidelines-clients/#processes","text":"The processes a back-end supports may be offered by the clients as methods in its own scope. The method names should follow the process names, but the conventions listed above can be applied here as well, e.g. converting filter_bands to filterBands . As parameters have no natural or technical ordering in the JSON objects, clients must come up with a reasonable ordering of parameters if required. This could be inspired by existing clients. The way of building a process graph from processes heavily depends on the technical capabilities of the programming language. Therefore it may differ between the client libraries. Follow the best practices of the programming language, e.g. support method chaining if possible.","title":"Processes"},{"location":"guidelines-clients/#workflow-example","text":"Some simplified example workflows using different programming styles are listed below. The following steps are executed: Loading the client library. Connecting to a back-end and authenticating with username and password via OpenID Connect. Requesting the capabilities and showing the implemented openEO version of the back-end. Showing information about the \"Sentinel-2A\" collection. Showing information about all processes supported by the back-end. Building a simple process graph. Creating a job. Pushing the job to the processing queue. After a while, showing the job details, e.g. checking the job status. Once processing is finished, downloading the job results to the local directory /tmp/job_results/ . Please note that the examples below do not comply to the latest process specification. They are meant to show the differences in client development, but are no working examples!","title":"Workflow example"},{"location":"guidelines-clients/#r-functional-style","text":"library ( openeo ) con = connect ( \"https://openeo.org\" , \"username\" , \"password\" ) cap = capabilities () cap %>% apiVersion () con %>% describeCollection ( \"Sentinel-2A\" ) con %>% listProcesses () processgraph = process ( \"load_collection\" , id = \"Sentinel-2A\" ) %>% process ( \"filter_bbox\" , west = 672000 , south = 5181000 , east = 652000 , north = 5161000 , crs = \"EPSG:32632\" ) %>% process ( \"filter_temporal\" , extent = c ( \"2017-01-01T00:00:00Z\" , \"2017-01-31T23:59:59Z\" )) %>% process ( \"ndvi\" , nir = \"B4\" , red = \"B8A\" ) %>% process ( \"min_time\" ) job = con %>% createJob ( processgraph ) job %>% startJob () job %>% describeJob () job %>% downloadResults ( \"/tmp/job_results/\" )","title":"R (functional style)"},{"location":"guidelines-clients/#python-mixed-style","text":"import openeo con = openeo . connect ( \"https://openeo.org\" , \"username\" , \"password\" ) cap = con . capabilities () print ( cap . api_version ()) print ( con . describe_collection ( \"Sentinel-2A\" )) print ( con . list_processes ()) processes = con . get_processes () pg = processes . load_collection ( id = \"Sentinel-2A\" ) pg = processes . filter_bbox ( pg , west = 672000 , south = 5181000 , east = 652000 , north = 5161000 , crs = \"EPSG:32632\" ) pg = processes . filter_temporal ( pg , extent = [ \"2017-01-01T00:00:00Z\" , \"2017-01-31T23:59:59Z\" ]) pg = processes . ndvi ( pg , nir = \"B4\" , red = \"B8A\" ) pg = processes . min_time ( pg ) job = con . create_job ( pg . graph ) job . start_job () print job . describe_job () job . download_results ( \"/tmp/job_results/\" )","title":"Python (mixed style)"},{"location":"guidelines-clients/#java-object-oriented-style","text":"import org.openeo.* ; OpenEO obj = new OpenEO (); Connection con = obj . connect ( \"https://openeo.org\" , \"username\" , \"password\" ); Capabilities cap = con . capabilities (); System . out . println ( cap . apiVersion ()); System . out . println ( con . describeCollection ( \"Sentinel-2A\" )); System . out . println ( con . listProcesses ()); ProcessGraphBuilder pgb = con . getProcessGraphBuilder () // Chain processes... ProcessGraph processGraph = pgb . buildProcessGraph (); Job job = con . createJob ( processGraph ); job . startJob (); System . out . println ( job . describeJob ()); job . downloadResults ( \"/tmp/job_results/\" );","title":"Java (object oriented style)"},{"location":"guidelines-clients/#php-procedural-style","text":"require_once(\"/path/to/openeo.php\"); $connection = openeo_connect(\"http://openeo.org\", \"username\", \"password\"); $capabilities = openeo_capabilities($connection); echo openeo_api_version($capabilites); echo openeo_describe_collection($connection, \"Sentinel-2A\"); echo openeo_list_processes($connection); $pg = openeo_process($pg, \"load_collection\", [\"id\" => \"Sentinel-2A\"]); $pg = openeo_process($pg, \"filter_bbox\", [\"west\" => 672000, \"south\" => 5181000, \"east\" => 652000, \"north\" => 5161000, \"crs\" => \"EPSG:32632\"]); $pg = openeo_process($pg, \"filter_temporal\", [\"extent\" => [\"2017-01-01T00:00:00Z\", \"2017-01-31T23:59:59Z\"]]); $pg = openeo_process($pg, \"ndvi\", [\"red\" => \"B4\", \"nir\" => \"B8A\"]); $pg = openeo_process($pg, \"min_time\"); $job = openeo_create_job($connection, $pg); openeo_start_job($job); echo openeo_describe_job($job); openeo_download_results($job, \"/tmp/job_results/\");","title":"PHP (procedural style)"},{"location":"guidelines-software/","text":"Software Development Guidelines \u00b6 This document describes guidelines for software developers, written for the openEO project. Since the openEO infrastructure will encompasses several programming languages and software environments, this document does not prescribe particular tools or platforms but rather focuses on general principles and methods behind them. License: all software developed in the openEO project and published on the openEO GitHub organisation shall be licensed under the Apache 2.0 license . If software repositories deviate from this, or contain code or other artifacts that deviates from this, this shall be described in the README.md file. Location: Official openEO software is developed under the openEO GitHub organisation . Proof-of-concept versus sustainable: each repository shall indicate its status: either proof-of-concept , or sustainable . Proof-of-concept code is meant to work but comes without quality assurance. Software repositories with proof-of-concept developments shall clearly say so in the first paragraph of the README.md file. Sustainable code should undergo standard quality checks , and point out its documentation . Sustainable code shall undergo code review ; no direct commits to master; any commit shall come in the form of a PR, commit after review. Sustainable code shall be written in a Test-driven manner , and repositories shall at the top of their README.md give indication of the degree to which code is covered by tests. Continuous integration shall be used to indicate code currently passes its test on CI platforms. A Code of conduct describes the rules and constraints to developers and contributors. Version numbers of sustainable software releases shall follow Semantic Versioning 2.0.0 . Software quality guidelines \u00b6 software shall be written in such a way that another person can understand its intention comment lines shall be used sparsely, but effectively reuse of unstable or esoteric libraries shall be avoided Software documentation guidelines \u00b6 Software documentation shall include: installation instructions usage instructions explain in detail the intention of the software pointers to reference documents explaining overarching concepts Each repository's README.md shall point to the documentation. Reference documentation shall be written using well-defined reference documentation language, such as RFC2119 or arc42 , and refer to the definitions used. Software review \u00b6 sustainable software development shall take place by always having two persons involved in a change to the master branch: individuals push to branches, pull request indicate readiness to be taken up in the master branch, a second developer reviews the pull request before merging it into the master branch. software review discussions shall be intelligible for external developers, and serve as implicit documentation of development decisions taken Test-driven development \u00b6 Software shall be developed in a test-driven fashion, meaning that while the code is written, tests are developed that verify, to a reasonable extent, the correctness of the code. Tools such as codecov.io to automatically indicate the amount of code covered by tests, and code that is not covered by tests shall be used in combination with a continuous integration framework. Continuous integration \u00b6 Repositories containing running software shall use an appropriate continuous integration platform, such as Travis CI or similar, to show whether the current build passes all checks. This helps understand contributors that the software passes tests on an independent platform, and may give insights in the way the software is compiled, deployed and tested. Additional guidelines \u00b6 There a specific guidelines for client library development and API development .","title":"Software Development"},{"location":"guidelines-software/#software-development-guidelines","text":"This document describes guidelines for software developers, written for the openEO project. Since the openEO infrastructure will encompasses several programming languages and software environments, this document does not prescribe particular tools or platforms but rather focuses on general principles and methods behind them. License: all software developed in the openEO project and published on the openEO GitHub organisation shall be licensed under the Apache 2.0 license . If software repositories deviate from this, or contain code or other artifacts that deviates from this, this shall be described in the README.md file. Location: Official openEO software is developed under the openEO GitHub organisation . Proof-of-concept versus sustainable: each repository shall indicate its status: either proof-of-concept , or sustainable . Proof-of-concept code is meant to work but comes without quality assurance. Software repositories with proof-of-concept developments shall clearly say so in the first paragraph of the README.md file. Sustainable code should undergo standard quality checks , and point out its documentation . Sustainable code shall undergo code review ; no direct commits to master; any commit shall come in the form of a PR, commit after review. Sustainable code shall be written in a Test-driven manner , and repositories shall at the top of their README.md give indication of the degree to which code is covered by tests. Continuous integration shall be used to indicate code currently passes its test on CI platforms. A Code of conduct describes the rules and constraints to developers and contributors. Version numbers of sustainable software releases shall follow Semantic Versioning 2.0.0 .","title":"Software Development Guidelines"},{"location":"guidelines-software/#software-quality-guidelines","text":"software shall be written in such a way that another person can understand its intention comment lines shall be used sparsely, but effectively reuse of unstable or esoteric libraries shall be avoided","title":"Software quality guidelines"},{"location":"guidelines-software/#software-documentation-guidelines","text":"Software documentation shall include: installation instructions usage instructions explain in detail the intention of the software pointers to reference documents explaining overarching concepts Each repository's README.md shall point to the documentation. Reference documentation shall be written using well-defined reference documentation language, such as RFC2119 or arc42 , and refer to the definitions used.","title":"Software documentation guidelines"},{"location":"guidelines-software/#software-review","text":"sustainable software development shall take place by always having two persons involved in a change to the master branch: individuals push to branches, pull request indicate readiness to be taken up in the master branch, a second developer reviews the pull request before merging it into the master branch. software review discussions shall be intelligible for external developers, and serve as implicit documentation of development decisions taken","title":"Software review"},{"location":"guidelines-software/#test-driven-development","text":"Software shall be developed in a test-driven fashion, meaning that while the code is written, tests are developed that verify, to a reasonable extent, the correctness of the code. Tools such as codecov.io to automatically indicate the amount of code covered by tests, and code that is not covered by tests shall be used in combination with a continuous integration framework.","title":"Test-driven development"},{"location":"guidelines-software/#continuous-integration","text":"Repositories containing running software shall use an appropriate continuous integration platform, such as Travis CI or similar, to show whether the current build passes all checks. This helps understand contributors that the software passes tests on an independent platform, and may give insights in the way the software is compiled, deployed and tested.","title":"Continuous integration"},{"location":"guidelines-software/#additional-guidelines","text":"There a specific guidelines for client library development and API development .","title":"Additional guidelines"},{"location":"processes/","text":"Processes \u00b6 A process is an operation that performs a specific task, see the glossary for a detailed definition. It consists of a name, a set of parameters, a return type and may throw errors or exceptions. In openEO, processes are used to build a chain of processes ( process graph ), which can be applied to EO data to derive your own findings from the data. Core processes \u00b6 There are some processes that we define to be core processes that are pre-defined and back-ends SHOULD follow these specifications to be interoperable. Not all processes need to be implemented by all back-ends. See the process reference for pre-defined processes. Defining processes \u00b6 Any back-end provider can either implement a set of pre-defined processes (STRONGLY RECOMMENDED) or define new processes for their domain. To define new processes, back-end providers MUST follow the process schema in the API specification. This includes: Choosing a intuitive and ideally unique process name consisting of only letters (a-z), numbers and underscores. Defining the parameters and their exacts (JSON) schemes. Specifying the return value of a process also with a (JSON) schema. Providing examples or compliance tests. Trying to make the process universially usable so that other back-end providers or openEO can adopt it. openEO specific formats \u00b6 In addition to the native data formats specified by JSON schema, openEO defines a set of specific formats that should be re-used in process schema definitions: Format Name Data type Description bounding-box object A bounding box with the required fields west , south , east , north and optionally base , height , crs . The crs is a EPSG code or PROJ definition. callback object An openEO process graph that is passed as an argument and is expected to be executed by the process. Callback parameters are specified in a parameters property (see chapter \"Callbacks\" below). collection-id string A collection id from the list of supported collections. Pattern: ^[A-Za-z0-9_\\-\\.~/]+$ date string Date only representation, as defined for full-date by RFC 3339 in section 5.6 . The time zone is UTC. date-time string Date and time representation, as defined for date-time by RFC 3339 in section 5.6 . epsg-code integer Specifies details about cartographic projections as EPSG code. geojson object GeoJSON as defined by RFC 7946 . JSON Schemes for validation are available. job-id string A batch job id, either one of the jobs a user has stored or a publicly available job. Pattern: ^[A-Za-z0-9_\\-\\.~]+$ kernel array Image kernel, a multi-dimensional array of numbers. output-format string An output format supported by the back-end. output-format-options object Key-value-pairs with arguments for the output format options supported by the back-end. process-graph-id string A process graph id, either one of the process graphs a user has stored or a publicly available process graph. Pattern: ^[A-Za-z0-9_\\-\\.~]+$ process-graph-variables object Key-value-pairs with values for variables that are defined by the process graph. The key of the pair is the variable_id for the value specified. proj-definition string Specifies details about cartographic projections as PROJ definition. raster-cube object A raster data cube, an image collection stored at the back-end. Different back-ends have different internal representations for this data structure. temporal-interval array A two-element array, which describes a left-closed temporal interval. The first element is the start of the date and/or time interval. The second element is the end of the date and/or time interval. The specified temporal strings follow the formats date-time , date (see above) and time (see below). temporal-intervals array An array of two-element arrays, each being an array with format temporal-interval (see above). time string Time only representation, as defined for full-time by RFC 3339 in section 5.6 . Although RFC 3339 prohibits the hour to be '24' , this definition allows the value '24' for the hour as end time in an interval in order to make it possible that left-closed time intervals can fully cover the day. vector-cube object A vector data cube, a vector collection stored at the back-end. Different back-ends have different internal representations for this data structure Callbacks \u00b6 A callback is defined by setting the type to object and the format to callback . Additionally, it must have a property parameters (a custom JSON Schema keyword). parameters must be an object with the keys being the callback parameter names and the values being a valid JSON Schema again. A schema for a callback with two parameters dimension (a string) and data (an array of numbers) could be defined as follows: { \"type\" : \"object\" , \"format\" : \"callback\" , \"parameters\" : { \"dimension\" : { \"description\" : \"Name of the dimension\" , \"type\" : \"string\" }, \"data\" : { \"description\" : \"Data for the dimension\" , \"type\" : \"array\" , \"items\" : { \"type\" : \"number\" } } } }","title":"Processes"},{"location":"processes/#processes","text":"A process is an operation that performs a specific task, see the glossary for a detailed definition. It consists of a name, a set of parameters, a return type and may throw errors or exceptions. In openEO, processes are used to build a chain of processes ( process graph ), which can be applied to EO data to derive your own findings from the data.","title":"Processes"},{"location":"processes/#core-processes","text":"There are some processes that we define to be core processes that are pre-defined and back-ends SHOULD follow these specifications to be interoperable. Not all processes need to be implemented by all back-ends. See the process reference for pre-defined processes.","title":"Core processes"},{"location":"processes/#defining-processes","text":"Any back-end provider can either implement a set of pre-defined processes (STRONGLY RECOMMENDED) or define new processes for their domain. To define new processes, back-end providers MUST follow the process schema in the API specification. This includes: Choosing a intuitive and ideally unique process name consisting of only letters (a-z), numbers and underscores. Defining the parameters and their exacts (JSON) schemes. Specifying the return value of a process also with a (JSON) schema. Providing examples or compliance tests. Trying to make the process universially usable so that other back-end providers or openEO can adopt it.","title":"Defining processes"},{"location":"processes/#openeo-specific-formats","text":"In addition to the native data formats specified by JSON schema, openEO defines a set of specific formats that should be re-used in process schema definitions: Format Name Data type Description bounding-box object A bounding box with the required fields west , south , east , north and optionally base , height , crs . The crs is a EPSG code or PROJ definition. callback object An openEO process graph that is passed as an argument and is expected to be executed by the process. Callback parameters are specified in a parameters property (see chapter \"Callbacks\" below). collection-id string A collection id from the list of supported collections. Pattern: ^[A-Za-z0-9_\\-\\.~/]+$ date string Date only representation, as defined for full-date by RFC 3339 in section 5.6 . The time zone is UTC. date-time string Date and time representation, as defined for date-time by RFC 3339 in section 5.6 . epsg-code integer Specifies details about cartographic projections as EPSG code. geojson object GeoJSON as defined by RFC 7946 . JSON Schemes for validation are available. job-id string A batch job id, either one of the jobs a user has stored or a publicly available job. Pattern: ^[A-Za-z0-9_\\-\\.~]+$ kernel array Image kernel, a multi-dimensional array of numbers. output-format string An output format supported by the back-end. output-format-options object Key-value-pairs with arguments for the output format options supported by the back-end. process-graph-id string A process graph id, either one of the process graphs a user has stored or a publicly available process graph. Pattern: ^[A-Za-z0-9_\\-\\.~]+$ process-graph-variables object Key-value-pairs with values for variables that are defined by the process graph. The key of the pair is the variable_id for the value specified. proj-definition string Specifies details about cartographic projections as PROJ definition. raster-cube object A raster data cube, an image collection stored at the back-end. Different back-ends have different internal representations for this data structure. temporal-interval array A two-element array, which describes a left-closed temporal interval. The first element is the start of the date and/or time interval. The second element is the end of the date and/or time interval. The specified temporal strings follow the formats date-time , date (see above) and time (see below). temporal-intervals array An array of two-element arrays, each being an array with format temporal-interval (see above). time string Time only representation, as defined for full-time by RFC 3339 in section 5.6 . Although RFC 3339 prohibits the hour to be '24' , this definition allows the value '24' for the hour as end time in an interval in order to make it possible that left-closed time intervals can fully cover the day. vector-cube object A vector data cube, a vector collection stored at the back-end. Different back-ends have different internal representations for this data structure","title":"openEO specific formats"},{"location":"processes/#callbacks","text":"A callback is defined by setting the type to object and the format to callback . Additionally, it must have a property parameters (a custom JSON Schema keyword). parameters must be an object with the keys being the callback parameter names and the values being a valid JSON Schema again. A schema for a callback with two parameters dimension (a string) and data (an array of numbers) could be defined as follows: { \"type\" : \"object\" , \"format\" : \"callback\" , \"parameters\" : { \"dimension\" : { \"description\" : \"Name of the dimension\" , \"type\" : \"string\" }, \"data\" : { \"description\" : \"Data for the dimension\" , \"type\" : \"array\" , \"items\" : { \"type\" : \"number\" } } } }","title":"Callbacks"},{"location":"processgraphs/","text":"Process graphs \u00b6 A process graph is a chain of specific processes . Similarly to scripts in the context of programming, process graphs organize and automate the execution of one or more processes that could alternatively be executed individually. In a process graph, processes need to be specific, which means that concrete values for input parameters need to be specified. These arguments can again be process graphs (callbacks), scalar values, arrays, objects or variables. Schematic definition \u00b6 A process graph is defined to be a map of connected processes with exactly one node returning the final result: <ProcessGraph> := { <ProcessNodeIdentifier>: <ProcessNode>, ... } <ProcessNodeIdentifier> is a unique key across the process graph that is used to reference (the return value of) this process in arguments of other processes. The identifier is unique only across the current map of processes, so excluding any parent and child process graphs. This means all identifier are strictly scoped and can not be used in child or parent process graphs. Please note that circular references are not allowed. Processes (Process Nodes) \u00b6 A single node in a process graph (i.e. a specific instance of a process) is defined as follows: <ProcessNode> := { \"process_id\": <string>, \"description\": <string>, \"arguments\": <Arguments>, \"result\": <boolean> } A process node MUST always contain key-value-pairs named process_id and arguments (see the next section). It MAY contain a description . One of the nodes in a map of processes (the final one) MUST have the result flag set to true , all the other nodes can omit it as the default value is false . This is important as multiple end nodes are possible, but in most use cases it is important to have exactly one end node, which can be referenced and the return value be used by other processes. Please note each callback also has a result node similar to the \"main\" process graph. process_id can contain any of the process names defined by a back-end, which are all listed at GET /processes , e.g. load_collection to retrieve data from a specific collection for processing. Arguments \u00b6 A process can in theory have an arbitrary number of arguments. The arguments including its names and values are specified by the process specification. Arguments are specified as an object and therefore is a simple map with key-value-pairs: <Arguments> := { <ParameterName>: <ArgumentValue> } The key <ArgumentValue> is RECOMMENDED to use snake case and MUST limit the characters to letters (a-z), numbers and underscores. A value is defined as follows: <ArgumentValue> := <string|number|boolean|null|array|object|Callback|CallbackParameter|Result|Variable> Note : The specified data types except Callback , CallbackParameter , Result and Variable are the native data types supported by JSON. Limitations apply as objects are not allowed to have keys with the following names: variable_id , except for objects of type Variable from_argument , except for objects of type CallbackParameter from_node , except for objects of type Result Important: Arrays and objects can also contain any of the data types defined above for <ArgumentValue> . So back-ends must fully traverse the process graphs, including all children. <Result> is simply an object with a key from_node with a <ProcessNodeIdentifier> as value, which tells the back-end that the process expects the result (i.e. the return value) from another node to be passed as argument: <Result> := { \"from_node\": <ProcessNodeIdentifier> } Please note that the <ProcessNodeIdentifier> is strictly scoped and can only referenced from within the same process graph, i.e. can not be referenced in child or parent process graphs. For Variable , Callback and CallbackParameter see the following sections. Callbacks \u00b6 Callbacks are simply specifying a process graph to be evaluated as part of another process. A callback object is a simple object with a single property callback that stores a process graph: <Callback> := { \"callback\": <ProcessGraph> } For example, you'd like to iterate over an array and want to apply another process abs (absolute value) on each value in the array. You can do so by executing apply in openEO (often also called map in other languages) and pass as callback the process abs , which is wrapped in a process graph. The values passed from apply to abs are the callback parameters, which you can also \"expect\", similar to return values of processes (see above). You can use an object of type CallbackParameter with with a key from_argument with the callback parameter name as value: <CallbackParameter> := { \"from_argument\": <CallbackParameterName> } Please note that the <CallbackParameterName> is also strictly scoped and can not be referenced in child or parent process graphs. The callback parameter names ( <CallbackParameterName> ) are defined by the processes. See the parameters property in the JSON schema of the parameter. Variables \u00b6 Process graphs can also hold a variable, which can be filled in later. For shared process graphs this can be useful to make them more portable, e.g in case a back-end specific product name would be stored with the process graph. Variables are defined as follows: <Variable> := { \"variable_id\": <string>, \"description\": <string>, \"type\": <string>, \"default\": <string|number|boolean|null|array|object> } The value for type is the expected data type for the content of the variable and MUST be one of string (default), number , boolean , array or object . The value for variable_id is the name of the variable and can be any valid JSON key, but it is RECOMMENDED to use snake case and limit the characters to a-z , 0-9 and _ . Whenever no value for the variable is defined the default value is used or the process graph is rejected if not default value has been specified. Example \u00b6 Deriving minimum EVI (Enhanced Vegetation Index) measurements over pixel time series of Sentinel 2 imagery. The main process chain in blue, callbacks in yellow: The process graph representing the algorithm: { \"dc\" : { \"process_id\" : \"load_collection\" , \"process_description\" : \"Loading the data; The order of the specified bands is important for the following reduce operation.\" , \"arguments\" : { \"id\" : \"Sentinel-2\" , \"spatial_extent\" : { \"west\" : 16.1 , \"east\" : 16.6 , \"north\" : 48.6 , \"south\" : 47.2 }, \"temporal_extent\" : [ \"2018-01-01\" , \"2018-02-01\" ], \"bands\" : [ \"B08\" , \"B04\" , \"B02\" ] } }, \"evi\" : { \"process_id\" : \"reduce\" , \"process_description\" : \"Compute the EVI. Formula: 2.5 * (NIR - RED) / (1 + NIR + 6*RED + -7.5*BLUE)\" , \"arguments\" : { \"data\" : { \"from_node\" : \"dc\" }, \"dimension\" : \"spectral\" , \"reducer\" : { \"callback\" : { \"nir\" : { \"process_id\" : \"array_element\" , \"arguments\" : { \"data\" : { \"from_argument\" : \"data\" }, \"index\" : 0 } }, \"red\" : { \"process_id\" : \"array_element\" , \"arguments\" : { \"data\" : { \"from_argument\" : \"data\" }, \"index\" : 1 } }, \"blue\" : { \"process_id\" : \"array_element\" , \"arguments\" : { \"data\" : { \"from_argument\" : \"data\" }, \"index\" : 2 } }, \"sub\" : { \"process_id\" : \"substract\" , \"arguments\" : { \"data\" : [{ \"from_node\" : \"nir\" }, { \"from_node\" : \"red\" }] } }, \"p1\" : { \"process_id\" : \"product\" , \"arguments\" : { \"data\" : [ 6 , { \"from_node\" : \"red\" }] } }, \"p2\" : { \"process_id\" : \"product\" , \"arguments\" : { \"data\" : [ -7.5 , { \"from_node\" : \"blue\" }] } }, \"sum\" : { \"process_id\" : \"sum\" , \"arguments\" : { \"data\" : [ 1 , { \"from_node\" : \"nir\" }, { \"from_node\" : \"p1\" }, { \"from_node\" : \"p2\" }] } }, \"div\" : { \"process_id\" : \"divide\" , \"arguments\" : { \"data\" : [{ \"from_node\" : \"sub\" }, { \"from_node\" : \"sum\" }] } }, \"p3\" : { \"process_id\" : \"product\" , \"arguments\" : { \"data\" : [ 2.5 , { \"from_node\" : \"div\" }] }, \"result\" : true } } } } }, \"mintime\" : { \"process_id\" : \"reduce\" , \"process_description\" : \"Compute a minimum time composite by reducing the temporal dimension\" , \"arguments\" : { \"data\" : { \"from_node\" : \"evi\" }, \"dimension\" : \"temporal\" , \"reducer\" : { \"callback\" : { \"min\" : { \"process_id\" : \"min\" , \"arguments\" : { \"data\" : { \"from_argument\" : \"data\" } }, \"result\" : true } } } } }, \"save\" : { \"process_id\" : \"save_result\" , \"arguments\" : { \"data\" : { \"from_node\" : \"mintime\" }, \"format\" : \"GTiff\" }, \"result\" : true } } Remarks for back-end processing \u00b6 To process the process graph on the back-end you need to go through all nodes/processes in the list and set for each node to which node it passes data and from which it expects data. In another iteration the back-end can find all start nodes for processing by checking for zero dependencies. You can now start and execute the start nodes (in parallel if possible). Results can be passed to the nodes that were identified beforehand. For each node that depends on multiple inputs you need to check whether all dependencies have already finished and only execute once the last dependency is ready. Please be aware that the result node ( result set to true ) is not necessarily the last node that is executed. The author of the process graph may choose to set a non-end node to the result node!","title":"Process Graphs"},{"location":"processgraphs/#process-graphs","text":"A process graph is a chain of specific processes . Similarly to scripts in the context of programming, process graphs organize and automate the execution of one or more processes that could alternatively be executed individually. In a process graph, processes need to be specific, which means that concrete values for input parameters need to be specified. These arguments can again be process graphs (callbacks), scalar values, arrays, objects or variables.","title":"Process graphs"},{"location":"processgraphs/#schematic-definition","text":"A process graph is defined to be a map of connected processes with exactly one node returning the final result: <ProcessGraph> := { <ProcessNodeIdentifier>: <ProcessNode>, ... } <ProcessNodeIdentifier> is a unique key across the process graph that is used to reference (the return value of) this process in arguments of other processes. The identifier is unique only across the current map of processes, so excluding any parent and child process graphs. This means all identifier are strictly scoped and can not be used in child or parent process graphs. Please note that circular references are not allowed.","title":"Schematic definition"},{"location":"processgraphs/#processes-process-nodes","text":"A single node in a process graph (i.e. a specific instance of a process) is defined as follows: <ProcessNode> := { \"process_id\": <string>, \"description\": <string>, \"arguments\": <Arguments>, \"result\": <boolean> } A process node MUST always contain key-value-pairs named process_id and arguments (see the next section). It MAY contain a description . One of the nodes in a map of processes (the final one) MUST have the result flag set to true , all the other nodes can omit it as the default value is false . This is important as multiple end nodes are possible, but in most use cases it is important to have exactly one end node, which can be referenced and the return value be used by other processes. Please note each callback also has a result node similar to the \"main\" process graph. process_id can contain any of the process names defined by a back-end, which are all listed at GET /processes , e.g. load_collection to retrieve data from a specific collection for processing.","title":"Processes (Process Nodes)"},{"location":"processgraphs/#arguments","text":"A process can in theory have an arbitrary number of arguments. The arguments including its names and values are specified by the process specification. Arguments are specified as an object and therefore is a simple map with key-value-pairs: <Arguments> := { <ParameterName>: <ArgumentValue> } The key <ArgumentValue> is RECOMMENDED to use snake case and MUST limit the characters to letters (a-z), numbers and underscores. A value is defined as follows: <ArgumentValue> := <string|number|boolean|null|array|object|Callback|CallbackParameter|Result|Variable> Note : The specified data types except Callback , CallbackParameter , Result and Variable are the native data types supported by JSON. Limitations apply as objects are not allowed to have keys with the following names: variable_id , except for objects of type Variable from_argument , except for objects of type CallbackParameter from_node , except for objects of type Result Important: Arrays and objects can also contain any of the data types defined above for <ArgumentValue> . So back-ends must fully traverse the process graphs, including all children. <Result> is simply an object with a key from_node with a <ProcessNodeIdentifier> as value, which tells the back-end that the process expects the result (i.e. the return value) from another node to be passed as argument: <Result> := { \"from_node\": <ProcessNodeIdentifier> } Please note that the <ProcessNodeIdentifier> is strictly scoped and can only referenced from within the same process graph, i.e. can not be referenced in child or parent process graphs. For Variable , Callback and CallbackParameter see the following sections.","title":"Arguments"},{"location":"processgraphs/#callbacks","text":"Callbacks are simply specifying a process graph to be evaluated as part of another process. A callback object is a simple object with a single property callback that stores a process graph: <Callback> := { \"callback\": <ProcessGraph> } For example, you'd like to iterate over an array and want to apply another process abs (absolute value) on each value in the array. You can do so by executing apply in openEO (often also called map in other languages) and pass as callback the process abs , which is wrapped in a process graph. The values passed from apply to abs are the callback parameters, which you can also \"expect\", similar to return values of processes (see above). You can use an object of type CallbackParameter with with a key from_argument with the callback parameter name as value: <CallbackParameter> := { \"from_argument\": <CallbackParameterName> } Please note that the <CallbackParameterName> is also strictly scoped and can not be referenced in child or parent process graphs. The callback parameter names ( <CallbackParameterName> ) are defined by the processes. See the parameters property in the JSON schema of the parameter.","title":"Callbacks"},{"location":"processgraphs/#variables","text":"Process graphs can also hold a variable, which can be filled in later. For shared process graphs this can be useful to make them more portable, e.g in case a back-end specific product name would be stored with the process graph. Variables are defined as follows: <Variable> := { \"variable_id\": <string>, \"description\": <string>, \"type\": <string>, \"default\": <string|number|boolean|null|array|object> } The value for type is the expected data type for the content of the variable and MUST be one of string (default), number , boolean , array or object . The value for variable_id is the name of the variable and can be any valid JSON key, but it is RECOMMENDED to use snake case and limit the characters to a-z , 0-9 and _ . Whenever no value for the variable is defined the default value is used or the process graph is rejected if not default value has been specified.","title":"Variables"},{"location":"processgraphs/#example","text":"Deriving minimum EVI (Enhanced Vegetation Index) measurements over pixel time series of Sentinel 2 imagery. The main process chain in blue, callbacks in yellow: The process graph representing the algorithm: { \"dc\" : { \"process_id\" : \"load_collection\" , \"process_description\" : \"Loading the data; The order of the specified bands is important for the following reduce operation.\" , \"arguments\" : { \"id\" : \"Sentinel-2\" , \"spatial_extent\" : { \"west\" : 16.1 , \"east\" : 16.6 , \"north\" : 48.6 , \"south\" : 47.2 }, \"temporal_extent\" : [ \"2018-01-01\" , \"2018-02-01\" ], \"bands\" : [ \"B08\" , \"B04\" , \"B02\" ] } }, \"evi\" : { \"process_id\" : \"reduce\" , \"process_description\" : \"Compute the EVI. Formula: 2.5 * (NIR - RED) / (1 + NIR + 6*RED + -7.5*BLUE)\" , \"arguments\" : { \"data\" : { \"from_node\" : \"dc\" }, \"dimension\" : \"spectral\" , \"reducer\" : { \"callback\" : { \"nir\" : { \"process_id\" : \"array_element\" , \"arguments\" : { \"data\" : { \"from_argument\" : \"data\" }, \"index\" : 0 } }, \"red\" : { \"process_id\" : \"array_element\" , \"arguments\" : { \"data\" : { \"from_argument\" : \"data\" }, \"index\" : 1 } }, \"blue\" : { \"process_id\" : \"array_element\" , \"arguments\" : { \"data\" : { \"from_argument\" : \"data\" }, \"index\" : 2 } }, \"sub\" : { \"process_id\" : \"substract\" , \"arguments\" : { \"data\" : [{ \"from_node\" : \"nir\" }, { \"from_node\" : \"red\" }] } }, \"p1\" : { \"process_id\" : \"product\" , \"arguments\" : { \"data\" : [ 6 , { \"from_node\" : \"red\" }] } }, \"p2\" : { \"process_id\" : \"product\" , \"arguments\" : { \"data\" : [ -7.5 , { \"from_node\" : \"blue\" }] } }, \"sum\" : { \"process_id\" : \"sum\" , \"arguments\" : { \"data\" : [ 1 , { \"from_node\" : \"nir\" }, { \"from_node\" : \"p1\" }, { \"from_node\" : \"p2\" }] } }, \"div\" : { \"process_id\" : \"divide\" , \"arguments\" : { \"data\" : [{ \"from_node\" : \"sub\" }, { \"from_node\" : \"sum\" }] } }, \"p3\" : { \"process_id\" : \"product\" , \"arguments\" : { \"data\" : [ 2.5 , { \"from_node\" : \"div\" }] }, \"result\" : true } } } } }, \"mintime\" : { \"process_id\" : \"reduce\" , \"process_description\" : \"Compute a minimum time composite by reducing the temporal dimension\" , \"arguments\" : { \"data\" : { \"from_node\" : \"evi\" }, \"dimension\" : \"temporal\" , \"reducer\" : { \"callback\" : { \"min\" : { \"process_id\" : \"min\" , \"arguments\" : { \"data\" : { \"from_argument\" : \"data\" } }, \"result\" : true } } } } }, \"save\" : { \"process_id\" : \"save_result\" , \"arguments\" : { \"data\" : { \"from_node\" : \"mintime\" }, \"format\" : \"GTiff\" }, \"result\" : true } }","title":"Example"},{"location":"processgraphs/#remarks-for-back-end-processing","text":"To process the process graph on the back-end you need to go through all nodes/processes in the list and set for each node to which node it passes data and from which it expects data. In another iteration the back-end can find all start nodes for processing by checking for zero dependencies. You can now start and execute the start nodes (in parallel if possible). Results can be passed to the nodes that were identified beforehand. For each node that depends on multiple inputs you need to check whether all dependencies have already finished and only execute once the last dependency is ready. Please be aware that the result node ( result set to true ) is not necessarily the last node that is executed. The author of the process graph may choose to set a non-end node to the result node!","title":"Remarks for back-end processing"},{"location":"processreference/","text":"Placeholder for generated process specifications.","title":"Process Reference"},{"location":"udfs/","text":"User-defined functions \u00b6 The abbreviation UDF stands for user-defined function . With this concept, users are able to upload custom code and have it executed e.g. for every pixel of a scene, allowing custom calculations on server-side data. UDFs are currently developed and evaluated outside of the core API. More information regarding the current draft for UDFs can be found in a separate repository . There is additional documentation available for the UDF Framework and the UDF API .","title":"UDFs"},{"location":"udfs/#user-defined-functions","text":"The abbreviation UDF stands for user-defined function . With this concept, users are able to upload custom code and have it executed e.g. for every pixel of a scene, allowing custom calculations on server-side data. UDFs are currently developed and evaluated outside of the core API. More information regarding the current draft for UDFs can be found in a separate repository . There is additional documentation available for the UDF Framework and the UDF API .","title":"User-defined functions"},{"location":"usermanagement/","text":"User Management and Accounting \u00b6 In general, the openEO API only defines a minimum subset of user management and accounting functionality. It allows to authenticate and authorize a user, which may include user registration with OpenID Connect , handle storage space limits (disk quota), manage billing, which includes to query the credit a user has available, estimate costs for certain operations (data processing and downloading), get information about produced costs, limit costs of certain operations. Therefore, the API leaves some aspects open that have to be handled by the back-ends separately, including credential recovery, e.g. retrieving a forgotten password user data management, e.g. changing the users payment details or email address payments, i.e. topping up credits for pre-paid services or paying for post-paid services other accounting related tasks, e.g. creating invoices, user registration (only specified when OpenID Connect is implemented).","title":"User Management and Accounting"},{"location":"usermanagement/#user-management-and-accounting","text":"In general, the openEO API only defines a minimum subset of user management and accounting functionality. It allows to authenticate and authorize a user, which may include user registration with OpenID Connect , handle storage space limits (disk quota), manage billing, which includes to query the credit a user has available, estimate costs for certain operations (data processing and downloading), get information about produced costs, limit costs of certain operations. Therefore, the API leaves some aspects open that have to be handled by the back-ends separately, including credential recovery, e.g. retrieving a forgotten password user data management, e.g. changing the users payment details or email address payments, i.e. topping up credits for pre-paid services or paying for post-paid services other accounting related tasks, e.g. creating invoices, user registration (only specified when OpenID Connect is implemented).","title":"User Management and Accounting"}]}